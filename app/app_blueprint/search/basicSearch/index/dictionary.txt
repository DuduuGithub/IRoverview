all_indexed_docIDs:104292008,108308108,12308095,13395552,13900194,14096841,14717992,15280949,173990671,174797767,17711681,182952687,18362887,1890353,189856873,20038688,203641746,203836948,204907203,208910151,209531816,210911499,211082893,211126562,211132391,211549689,213969759,219558760,219708206,219721074,219792420,220769181,221376626,221508448,222124972,222133372,222208633,222379753,223953610,226254532,227162606,228063930,228376209,229924317,231740484,231749906,231847109,231918454,231918471,231934149,232046055,232257793,232307359,234358843,235293845,235417023,235795764,235828929,236087352,237532682,238198466,238215654,238407774,238408001,238408308,238408406,238419359,238582721,238583252,238634584,238857129,239009452,239016615,239049633,240070335,243756979,244117789,244920632,245117682,245123905,245335427,245906072,246294808,246430476,246431014,246634167,246634506,246822414,247058853,247222761,247222973,247292326,247450846,247518687,247570285,247595263,247628166,247741267,247741535,247996981,248887351,248887644,249191952,249209856,249209990,249395483,249461537,249538336,249538415,250113584,251320393,251320459,251320513,251554821,252089864,252544861,252596087,252596252,252683543,252693109,252715596,252715693,252735281,252762275,252815807,252846202,252918439,253080708,253224274,253255190,253510295,253523474,253553242,254044338,254198988,254221022,254926490,255749563,256105083,256416103,256503523,256615681,256900870,257038864,257232422,257254919,257632050,257636556,257687205,257757426,257766959,257913782,258041281,258108073,258436870,258480011,258741298,258832670,258833272,258833682,258865444,258865597,258887582,258947377,258999763,259075246,259075723,259108646,259138821,259165244,259165262,259203115,259203325,259298238,259342096,259360601,259375870,259841489,259847777,259924554,260125817,260154786,260164542,260316137,260378901,260378993,260886874,260887189,261076339,261100891,261276856,261530996,261557296,261582259,261682321,262013288,262054014,262828485,262944419,263152829,263311025,263334074,263334567,263334587,263334596,263605851,263606194,263608672,263609164,263610128,263620293,263671662,263829270,263829348,263829780,263829872,263829977,263830433,263830786,263831485,263831492,263831863,263834884,263889455,263909212,263909278,263909429,263909549,264128166,264147017,264147054,264172174,264172668,264172845,264172935,264288700,264288929,264289064,264306002,264306022,264306078,264306111,264406064,264426013,264426077,264438904,264438909,264439509,264490587,264555202,264555382,264802494,264825357,264825424,264825556,27494814,3290366,33513311,3464416,3484654,3508638,3509777,3516266,3522489,3524184,3532296,3557557,43968607,44084312,44119895,47015748,4737664,48352800,49667762,51559,52890982,52893258,52898806,52909341,52909749,52920181,52944914,52947902,53208122,53327717,53729760,53951481,57825721,5834589,58554701,58981389,59317031,60441438,67855552,67856213,7167114,8394195,86393936,9725544,997870
doc_lengths:{"104292008": 377, "108308108": 297, "12308095": 215, "13395552": 161, "13900194": 257, "14096841": 941, "14717992": 814, "15280949": 221, "173990671": 386, "174797767": 375, "17711681": 225, "182952687": 266, "18362887": 195, "1890353": 235, "189856873": 697, "20038688": 389, "203641746": 554, "203836948": 470, "204907203": 304, "208910151": 154, "209531816": 265, "210911499": 219, "211082893": 277, "211126562": 383, "211132391": 371, "211549689": 178, "213969759": 243, "219558760": 296, "219708206": 266, "219721074": 782, "219792420": 623, "220769181": 239, "221376626": 408, "221508448": 642, "222124972": 351, "222133372": 235, "222208633": 217, "222379753": 211, "223953610": 336, "226254532": 360, "227162606": 280, "228063930": 388, "228376209": 416, "229924317": 309, "231740484": 644, "231749906": 797, "231847109": 439, "231918454": 215, "231918471": 267, "231934149": 184, "232046055": 391, "232257793": 281, "232307359": 257, "234358843": 570, "235293845": 241, "235417023": 359, "235795764": 640, "235828929": 305, "236087352": 259, "237532682": 425, "238198466": 364, "238215654": 385, "238407774": 1257, "238408001": 252, "238408308": 900, "238408406": 353, "238419359": 320, "238582721": 772, "238583252": 262, "238634584": 332, "238857129": 455, "239009452": 792, "239016615": 290, "239049633": 481, "240070335": 398, "243756979": 329, "244117789": 429, "244920632": 497, "245117682": 380, "245123905": 1037, "245335427": 733, "245906072": 277, "246294808": 352, "246430476": 489, "246431014": 556, "246634167": 247, "246634506": 372, "246822414": 213, "247058853": 322, "247222761": 427, "247222973": 223, "247292326": 370, "247450846": 358, "247518687": 322, "247570285": 352, "247595263": 229, "247628166": 405, "247741267": 298, "247741535": 328, "247996981": 687, "248887351": 338, "248887644": 158, "249191952": 364, "249209856": 267, "249209990": 979, "249395483": 585, "249461537": 167, "249538336": 337, "249538415": 324, "250113584": 221, "251320393": 736, "251320459": 189, "251320513": 158, "251554821": 1020, "252089864": 1063, "252544861": 700, "252596087": 425, "252596252": 250, "252683543": 468, "252693109": 1182, "252715596": 212, "252715693": 453, "252735281": 575, "252762275": 339, "252815807": 295, "252846202": 284, "252918439": 288, "253080708": 240, "253224274": 272, "253255190": 449, "253510295": 700, "253523474": 246, "253553242": 421, "254044338": 231, "254198988": 596, "254221022": 382, "254926490": 297, "255749563": 372, "256105083": 172, "256416103": 347, "256503523": 411, "256615681": 268, "256900870": 258, "257038864": 220, "257232422": 411, "257254919": 299, "257632050": 277, "257636556": 392, "257687205": 420, "257757426": 254, "257766959": 370, "257913782": 202, "258041281": 317, "258108073": 344, "258436870": 227, "258480011": 316, "258741298": 316, "258832670": 473, "258833272": 294, "258833682": 273, "258865444": 259, "258865597": 284, "258887582": 317, "258947377": 292, "258999763": 214, "259075246": 242, "259075723": 886, "259108646": 365, "259138821": 291, "259165244": 321, "259165262": 411, "259203115": 284, "259203325": 940, "259298238": 315, "259342096": 767, "259360601": 260, "259375870": 394, "259841489": 346, "259847777": 899, "259924554": 222, "260125817": 285, "260154786": 362, "260164542": 242, "260316137": 245, "260378901": 293, "260378993": 561, "260886874": 344, "260887189": 463, "261076339": 283, "261100891": 731, "261276856": 367, "261530996": 403, "261557296": 408, "261582259": 619, "261682321": 374, "262013288": 195, "262054014": 360, "262828485": 400, "262944419": 405, "263152829": 270, "263311025": 576, "263334074": 432, "263334567": 423, "263334587": 241, "263334596": 337, "263605851": 478, "263606194": 340, "263608672": 310, "263609164": 296, "263610128": 338, "263620293": 215, "263671662": 192, "263829270": 559, "263829348": 367, "263829780": 319, "263829872": 354, "263829977": 266, "263830433": 342, "263830786": 330, "263831485": 267, "263831492": 153, "263831863": 350, "263834884": 323, "263889455": 329, "263909212": 367, "263909278": 872, "263909429": 256, "263909549": 311, "264128166": 216, "264147017": 186, "264147054": 242, "264172174": 549, "264172668": 264, "264172845": 429, "264172935": 348, "264288700": 379, "264288929": 693, "264289064": 326, "264306002": 282, "264306022": 364, "264306078": 290, "264306111": 406, "264406064": 338, "264426013": 318, "264426077": 341, "264438904": 179, "264438909": 311, "264439509": 358, "264490587": 238, "264555202": 262, "264555382": 294, "264802494": 306, "264825357": 330, "264825424": 226, "264825556": 326, "27494814": 374, "3290366": 199, "33513311": 190, "3464416": 316, "3484654": 254, "3508638": 265, "3509777": 184, "3516266": 266, "3522489": 273, "3524184": 200, "3532296": 312, "3557557": 264, "43968607": 198, "44084312": 123, "44119895": 285, "47015748": 249, "4737664": 219, "48352800": 226, "49667762": 445, "51559": 236, "52890982": 224, "52893258": 289, "52898806": 565, "52909341": 219, "52909749": 201, "52920181": 369, "52944914": 372, "52947902": 491, "53208122": 379, "53327717": 387, "53729760": 381, "53951481": 507, "57825721": 214, "5834589": 378, "58554701": 218, "58981389": 207, "59317031": 471, "60441438": 320, "67855552": 455, "67856213": 226, "7167114": 132, "8394195": 232, "86393936": 208, "9725544": 313, "997870": 258}
doc_metadata:{"104292008": {"id": "104292008", "openalex": null, "doi": null, "title": "A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning", "abstract": "Due to the lack of enough training data and high computational cost to train a deep neural network from scratch, transfer learning has been extensively used in many deep-neural-network-based applications, such as face recognition, image classification, speech recognition, etc. A commonly-used transfer learning approach involves taking a part of a pre-trained model, adding a few layers at the end, and re-training the new layers with a small dataset. This approach, while efficient and widely used, imposes a security vulnerability because the pre-trained models used in transfer learning are usually available publicly to everyone, including potential attackers. In this paper, we show that without any additional knowledge other than the pre-trained model, an attacker can launch an effective and efficient brute force attack that can craft instances of input to trigger each target class with high confidence. Note that we assume that the attacker does not have access to any targetspecific information, including samples from target classes, re-trained model, and probabilities assigned by Softmax to each class, and thus called target-agnostic attack. These assumptions render all previous attacks impractical, to the best of our knowledge. To evaluate the proposed attack, we perform a set of experiments on face recognition and speech recognition tasks and show the effectiveness of the attack. Our work sheds light on a fundamental security challenge of transfer learning in deep neural networks.", "authors": [], "concepts": ["widely", "attacker", "each", "paper,", "classes,", "access", "tasks", "approach,", "recognition,", "used", "show", "available", "networks.", "speech", "efficient", "exploiting", "such", "note", "these", "does", "extensively", "been", "layers", "target-agnostic", "light", "sheds", "re-training", "launch", "from", "scratch,", "part", "deep", "attack,", "many", "while", "force", "applications,", "instances", "fundamental", "information,", "models", "attack.", "thus", "transfer", "because", "work", "learning", "usually", "additional", "probabilities", "target", "without", "render", "other", "everyone,", "experiments", "high", "impractical,", "assigned", "network", "commonly-used", "cost", "model,", "class", "image", "trigger", "potential", "data", "input", "training", "etc.", "neural", "with", "targetspecific", "including", "train", "approach", "assumptions", "that", "security", "adding", "knowledge", "previous", "end,", "softmax", "assume", "confidence.", "effective", "re-trained", "effectiveness", "called", "used,", "lack", "brute", "vulnerability", "challenge", "computational", "perform", "best", "recognition", "dataset.", "publicly", "than", "this", "taking", "enough", "samples", "face", "attack", "imposes", "deep-neural-network-based", "have", "knowledge.", "small", "proposed", "involves", "attacks", "pre-trained", "evaluate", "classification,", "vulnerabilities", "models:", "craft", "attackers.", "class,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "108308108": {"id": "108308108", "openalex": null, "doi": null, "title": "ENVIRONMENT PROBING INTERACTION POLICIES", "abstract": "A key challenge in reinforcement learning (RL) is environment generalization: a policy trained to solve a task in one environment often fails to solve the same task in a slightly different test environment. A common approach to improve interenvironment transfer is to learn policies that are invariant to the distribution of testing environments. However, we argue that instead of being invariant, the policy should identify the specific nuances of an environment and exploit them to achieve better performance. In this work, we propose the \"Environment-Probing\" Interaction (EPI) policy, a policy that probes a new environment to extract an implicit understanding of that environment's behavior. Once this environmentspecific information is obtained, it is used as an additional input to a task-specific policy that can now perform environment-conditioned actions to solve a task. To learn these EPI-policies, we present a reward function based on transition predictability. Specifically, a higher reward is given if the trajectory generated by the EPI-policy can be used to better predict transitions. We experimentally show that EPI-conditioned task-specific policies significantly outperform commonly used policy generalization methods on novel testing environments.", "authors": [], "concepts": ["actions", "learn", "outperform", "used", "generalization:", "them", "show", "policy,", "experimentally", "test", "information", "probing", "these", "specific", "performance.", "present", "however,", "predict", "interenvironment", "(epi)", "function", "should", "environment's", "task", "invariant", "propose", "reinforcement", "generalization", "once", "being", "solve", "transfer", "environment.", "environment-conditioned", "methods", "(rl)", "better", "epi-policy", "extract", "probes", "learning", "improve", "additional", "significantly", "interaction", "understanding", "environment", "argue", "specifically,", "novel", "nuances", "based", "transition", "policy", "generated", "input", "achieve", "\"environment-probing\"", "implicit", "identify", "environments.", "fails", "given", "reward", "trained", "common", "work,", "approach", "task.", "that", "instead", "distribution", "trajectory", "testing", "exploit", "different", "higher", "environmentspecific", "epi-conditioned", "behavior.", "challenge", "policies", "perform", "predictability.", "commonly", "same", "invariant,", "slightly", "task-specific", "this", "obtained,", "transitions.", "often", "epi-policies,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "12308095": {"id": "12308095", "openalex": null, "doi": null, "title": "COUNTERING ADVERSARIAL IMAGES USING INPUT TRANSFORMATIONS", "abstract": "This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.", "authors": [], "concepts": ["transformations", "total", "particular,", "compression,", "them", "show", "inherent", "non-differentiable", "such", "countering", "lies", "images", "strategies", "using", "convolutional", "before", "defense", "reduction,", "very", "practice,", "transforming", "minimization,", "nature", "randomness,", "adversary", "feeding", "gray-box", "jpeg", "methods.", "experiments", "inputs", "quilting", "network", "specifically,", "when", "image", "makes", "input", "major", "variance", "systems", "trained", "image-classification", "that", "defenses.", "their", "imagenet", "transformed", "strong", "minimization", "investigates", "eliminates", "those", "black-box", "effective", "strength", "defenses", "adversarial-example", "paper", "difficult", "images.", "best", "this", "which", "system.", "circumvent", "adversarial", "attack", "bit-depth", "attacks", "defend", "study", "variety", "applying", "classifier.", "against"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "13395552": {"id": "13395552", "openalex": null, "doi": null, "title": "Learning to Optimize", "abstract": "Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.", "authors": [], "concepts": ["learn", "demonstrate", "paper,", "problem", "particular", "believe", "guided", "method", "and/or", "resulting", "existing", "explore", "represent", "from", "present", "using", "many", "requires", "convergence", "reinforcement", "process", "algorithm.", "optimize", "final", "terms", "better", "learning", "automatically", "perspective", "laborious", "policy", "automating", "search", "design", "hand-engineered", "speed", "approach", "outperforms", "that", "algorithm,", "first", "validation.", "objective", "algorithms", "this", "which", "algorithm", "discover", "ideation", "often", "value.", "optimization", "policy.", "iterations"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "13900194": {"id": "13900194", "openalex": null, "doi": null, "title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.* Equal contribution 1 Note that z, u \u2208 R m , so trivially the dimensionality of the solution grows polynomially.2We assume that X is a subset of normed vector space.", "authors": [], "concepts": ["researchers", "wasserstein", "perturbations", "vulnerable", "robust", "problem", "perturbing", "model", "lens", "method", "augments", "minimization.", "vector", "lagrangian", "subset", "principled", "note", "updates", "procedure", "examples", "moderate", "empirical", "through", "performance", "ball,", "many", "imperceptible", "population", "defense", "perturbations,", "relative", "matches", "achieves", "networks", "normed", "some", "data.", "furthermore,", "mechanisms.", "statistical", "contribution", "losses,", "levels", "heuristic", "dimensionality", "cost", "data", "input", "training", "provably", "neural", "perturbations.", "with", "distributionally", "space.", "outperforms", "that", "distribution", "distributional", "solution", "certify", "robustness", "assume", "allow", "efficiently", "address", "provide", "parameter", "considering", "computational", "formulation", "optimization,", "this", "which", "worst-case", "loss.", "trivially", "risk", "grows", "penalty", "little", "adversarial", "attack", "have", "smooth", "proposed", "underlying", "approaches.*", "under", "certifying", "polynomially.2we", "guarantees", "equal"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "14096841": {"id": "14096841", "openalex": null, "doi": null, "title": "AN ACTOR-CRITIC ALGORITHM FOR SEQUENCE PREDICTION", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. * CIFAR Senior Fellow \u2020 CIFAR Fellow Published as a conference paper at ICLR 2017 from a sampled prediction lead to a high task-specific score, such as BLEU(Papineni et al., 2002)or ROUGE (Lin & Hovy, 2003).In this work, we propose and study an alternative procedure for training sequence prediction networks that aims to directly improve their test time metrics (which are typically not the log-likelihood). In particular, we train an additional network called the critic to output the value of each token, which we define as the expected task-specific score that the network will receive if it outputs the token and continues to sample outputs according to its probability distribution. Furthermore, we show how the predicted values can be used to train the main sequence prediction network, which we refer to as the actor. The theoretical foundation of our method is that, under the assumption that the critic computes exact values, the expression that we use to train the actor is an unbiased estimate of the gradient of the expected task-specific score.Our approach draws inspiration and borrows the terminology from the field of reinforcement learning (RL)(Sutton & Barto, 1998), in particular from the actor-critic approach(Sutton, 1984;Sutton et al., 1999;Barto et al., 1983). RL studies the problem of acting efficiently based only on weak supervision in the form of a reward given for some of the agent's actions. In our case, the reward is analogous to the task-specific score associated with a prediction. However, the tasks we consider are those of supervised learning, and we make use of this crucial difference by allowing the critic to use the ground-truth answer as an input. In other words, the critic has access to a sequence of expert actions that are known to lead to high (or even optimal) returns. To train the critic, we adapt the temporal difference methods from the RL literature(Sutton, 1988)to our setup. While RL methods with non-linear function approximators are not new(Tesauro, 1994;Miller et al., 1995), they have recently surged in popularity, giving rise to the field of 'deep RL'(Mnih et al., 2015). We show that some of the techniques recently developed in deep RL, such as having a target network, may also be beneficial for sequence prediction.The contributions of the paper can be summarized as follows: 1) we describe how RL methodology like the actor-critic approach can be applied to supervised learning problems with structured outputs; and 2) we investigate the performance and behavior of the new method on both a synthetic task and a real-world task of machine translation, demonstrating the improvements over maximum-likelihood and REINFORCE brought by the actor-critic training.", "authors": [], "concepts": ["values,", "improvements", "actions", "1994;miller", "maximum-likelihood", "aims", "known", "machine", "(which", "each", "must", "field", "typically", "generation,", "learning,", "access", "surged", "associated", "developed", "studies", "tasks", "particular,", "token,", "problem", "particular", "theoretical", "recently", "german-english", "used", "show", "method", "lead", "receive", "literature(sutton,", "they", "techniques", "tokens", "score", "test", "weak", "demonstrating", "such", "sequences", "will", "these", "only", "difference", "procedure", "crucially,", "setting,", "much", "actor-critic", "actor", "computes", "expected", "hovy,", "performance", "modes,", "from", "leverage", "present", "deep", "however,", "critic", "predict", "leads", "predicted", "using", "estimate", "2015).", "generation", "consider", "while", "new(tesauro,", "function", "conference", "task", "applied", "1999;barto", "propose", "brought", "reinforcement", "according", "summarized", "models", "analogous", "1995),", "networks", "task,", "outputs;", "setup.", "rather", "some", "current", "discrepancy", "both", "critic,", "furthermore,", "(rl)(sutton", "optimize", "caption", "methods", "unbiased", "draws", "improved", "learning", "that,", "improve", "foundation", "probability", "actions.", "contributions", "like", "problems", "gradient", "additional", "inspiration", "target", "translation,", "output", "non-linear", "outputs", "reinforce", "closer", "having", "continues", "other", "dialogue", "log-likelihood", "high", "popularity,", "network", "analysis", "traditional", "conditioned", "define", "based", "form", "acting", "cifar", "real-world", "modelling.", "1983).", "rise", "policy", "since", "training", "1998),", "(lin", "neural", "input.", "published", "with", "introducing", "allowing", "train", "score,", "senior", "behavior", "given", "reward", "condition", "network.", "refer", "score.our", "phase,", "exact", "trained", "log-likelihood).", "work,", "answer", "agent's", "approach", "that", "training.", "case,", "borrows", "between", "expert", "supervision", "their", "beneficial", "alternative", "giving", "testing", "1988)to", "token", "language", "previous", "assumption", "those", "terminology", "prediction", "structured", "barto,", "expression", "network,", "tokens.", "natural", "sequence", "make", "paves", "called", "efficiently", "rl'(mnih", "follows:", "ground-truth", "address", "value", "paper", "allows", "metrics", "temporal", "investigate", "approach(sutton,", "results", "output.", "rouge", "al.,", "synthetic", "prediction.", "also", "returns.", "words,", "'deep", "optimal)", "(rl).", "sample", "than", "over", "fellow", "tasks,", "task-specific", "this", "which", "adapt", "methodology", "algorithm", "guesses", "time", "prediction.the", "bleu.", "translation.", "have", "directly", "iclr", "main", "actor.", "2002)or", "limited", "under", "bleu(papineni", "describe", "2003).in", "even", "study", "sampled", "crucial", "generate", "supervised", "values", "1984;sutton", "setting", "approximators", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "14717992": {"id": "14717992", "openalex": null, "doi": null, "title": "REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS", "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-theart on Atari, averaging 880% expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10\u00d7 and averaging 87% expert human performance on Labyrinth.Natural and artificial agents live in a stream of sensorimotor data. At each time step t, the agent receives observations o t and executes actions a t . These actions influence the future course of the sensorimotor stream. In this paper we develop agents that learn to predict and control this stream, by solving a host of reinforcement learning problems, each focusing on a distinct feature of the sensorimotor stream. Our hypothesis is that an agent that can flexibly control its future experiences will also be able to achieve any goal with which it is presented, such as maximising its future rewards.The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. However, in many interesting domains, extrinsic rewards are only rarely observed. This raises questions of what and how to learn in their absence. Even if extrinsic rewards are frequent, the sensorimotor stream contains an abundance of other possible learning targets. Traditionally, unsupervised learning attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. It is typically used to accelerate the acquisition of a useful representation. In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudorewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent's long-term goals, potentially leading to more useful representations.Consider a baby that learns to maximise the cumulative amount of red that it observes. To correctly predict the optimal value, the baby must understand how to increase \"redness\" by various means, including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a red object); and communication (crying until the parents bring a red object). These behaviours are likely to recur for many other goals that the baby may subsequently encounter. No understanding of these behaviours is required to simply reconstruct the redness of current or subsequent images.Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards. It also makes other auxiliary predictions that serve to focus the agent on important aspects of the task. These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. To learn more efficiently, our agents use an experience replay mechanism to provide additional updates * Joint first authors. Ordered alphabetically by first name.", "authors": [], "concepts": ["environments", "state-of-the-art", "actions", "maximisation", "future", "challenging", "state-of-theart", "learn", "maximise", "what", "each", "must", "flexibly", "typically", "understand", "paper,", "targets.", "learning,", "focuses", "means,", "speedup", "stream", "sensorimotor", "baby", "most", "tasks", "upon", "short-term", "absence.", "behaviours", "approximate", "observed.", "used", "object", "them", "authors.", "achieved", "subsequently", "such", "joint", "will", "parents", "these", "only", "actual", "updates", "artificial", "efficiently,", "feature", "pseudo-rewards.", "first-person,", "much", "increase", "able", "rewards.", "more", "performance", "long-term", "deep", "however,", "targets,", "predict", "rapidly", "traditionally,", "distinct", "labyrinth.natural", "well", "many", "function", "auxiliary", "value,", "share", "agents", "pixels", "locomotion", "front", "optimal", "(moving", "reinforcement", "cumulative", "likely", "pseudo-reward", "averaging", "learning.", "suite", "until", "influence", "hypothesis", "current", "data.", "both", "domains,", "contain", "features", "object);", "abundance", "include", "learning", "goal", "that,", "absence", "\"redness\"", "like", "additional", "simultaneously", "solving", "significantly", "goals", "extrinsic", "goals,", "performance,", "closer", "presented,", "continues", "other", "understanding", "maximises", "uses", "alphabetically", "potentially", "signals.", "novel", "redness", "live", "labyrinth", "receives", "stream.", "step", "intuitively,", "control", "acquisition", "predicting", "interesting", "policy", "makes", "achieve", "training", "experiences", "mean", "classic", "treating", "learns", "focusing", "with", "including", "(bringing", "reward", "executes", "predictions", "questions", "maximising", "common", "paradigm", "attempts", "wider", "agent's", "outperforms", "(crying", "task.", "that", "functions", "expert", "their", "rewards,", "required", "communication", "frequent,", "rewards.the", "previous", "different", "representation.", "observes.", "atari,", "encounter.", "useful", "subsequent", "contains", "contrast,", "course", "880%", "leading", "name.", "pseudorewards", "simply", "first", "bring", "matched", "accelerate", "images.our", "value", "paper", "focus", "closely", "provide", "objective", "host", "amount", "unsupervised", "rewards", "possible", "observations", "experience", "results", "problems,", "also", "rarely", "introduce", "reconstruct", "various", "serve", "eyes);", "reward.", "correctly", "this", "which", "object).", "adapt", "ordered", "agent", "time", "raises", "recur", "develop", "have", "directly", "aspects", "replay", "manipulation", "relevant", "important", "three-dimensional", "architecture", "representation", "even", "stream,", "variety", "mechanism", "human", "representations.consider", "frame."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "15280949": {"id": "15280949", "openalex": null, "doi": null, "title": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING", "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks. * This work has been done during the 1st author's internship with IBM Watson.", "authors": [], "concepts": ["what", "each", "term", "watson.", "model", "effect,", "side", "show", "comes", "attending", "proposes", "been", "embedding", "embedding,", "specific", "represent", "classification", "interpretable", "self-attention.", "performance", "into", "part", "matrix", "using", "compared", "propose", "sentence.", "self-attention", "significant", "author", "methods", "work", "profiling,", "parts", "gain", "done", "textual", "other", "sentence", "regularization", "easy", "visualizing", "tasks.", "with", "introducing", "self-attentive", "entailment.", "that", "instead", "internship", "extracting", "tasks:", "different", "structured", "model.", "vector,", "yields", "encoded", "paper", "sentiment", "special", "results", "also", "author's", "this", "during", "embedding.", "evaluate", "mechanism"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "173990671": {"id": "173990671", "openalex": null, "doi": null, "title": "SpikeGrad: An ANN-equivalent Computation Model for Implementing Backpropagation with Spikes", "abstract": "Event-based neuromorphic systems promise to reduce the energy consumption of deep learning tasks by replacing expensive floating point operations on dense matrices by low power sparse and asynchronous operations on spike events. While these systems can be trained increasingly well using approximations of the backpropagation algorithm, these implementations usually require high precision errors for training and are therefore incompatible with the typical communication infrastructure of neuromorphic circuits. In this work, we analyze how the gradient can be discretized into spike events when training a spiking neural network. To accelerate our simulation, we show that using a special implementation of the integrate-andfire neuron allows us to describe the accumulated activations and errors of the spiking neural network in terms of an equivalent artificial neural network, allowing us to largely speed up training compared to an explicit simulation of all spike events. This way we are able to demonstrate that even for deep networks, the gradients can be discretized sufficiently well with spikes if the gradient is properly rescaled. This form of spike-based backpropagation enables us to achieve equivalent or better accuracies on the MNIST and CIFAR10 dataset than comparable state-of-the-art spiking neural networks trained with full precision gradients. The algorithm, which we call SpikeGrad, is based on accumulation and comparison operations and can naturally exploit sparsity in the gradient computation, which makes it an interesting choice for a spiking neuromorphic systems with on-chip learning capacities.Preprint. Under review.", "authors": [], "concepts": ["state-of-the-art", "demonstrate", "backpropagation", "comparable", "tasks", "computation,", "expensive", "spikegrad:", "model", "activations", "show", "typical", "dataset", "these", "accumulated", "artificial", "accumulation", "able", "into", "deep", "gradients", "using", "integrate-andfire", "well", "explicit", "while", "compared", "ann-equivalent", "asynchronous", "increasingly", "networks", "full", "networks,", "mnist", "terms", "better", "sufficiently", "learning", "usually", "naturally", "spiking", "gradient", "equivalent", "sparse", "errors", "gradients.", "high", "network", "energy", "events.", "when", "based", "form", "consumption", "spikes", "interesting", "makes", "achieve", "training", "neural", "with", "allowing", "network.", "circuits.", "point", "systems", "comparison", "dense", "trained", "cifar10", "call", "neuromorphic", "speed", "spike-based", "work,", "that", "on-chip", "analyze", "communication", "algorithm,", "exploit", "floating", "implementation", "network,", "implementations", "replacing", "sparsity", "choice", "reduce", "approximations", "accuracies", "accelerate", "special", "allows", "simulation", "event-based", "promise", "events", "discretized", "incompatible", "capacities.preprint.", "review.", "simulation,", "properly", "than", "infrastructure", "power", "this", "which", "operations", "enables", "implementing", "largely", "computation", "precision", "therefore", "under", "describe", "rescaled.", "even", "require", "matrices", "spike", "spikegrad,", "neuron"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "174797767": {"id": "174797767", "openalex": null, "doi": null, "title": "Disentangling neural mechanisms for perceptual grouping", "abstract": "Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations of these connections on two synthetic visual tasks, which stress low-level \"gestalt\" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up processing. Horizontal connections resolve this limitation on tasks with gestalt cues by supporting incremental spatial propagation of activities, whereas top-down connections rescue learning on tasks featuring object cues by propagating coarse predictions about the position of the target object. Our findings disassociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.Extant theory suggests that there are two distinct types of feedback strategies: A low-level strategy of grouping visual features with neighboring features according to Gestalt laws including similarity, good continuation, etc.[13][14][15][16][17][18][19]. In contrast, an object-based strategy is mediated by expectations \u2020 These authors contributed equally to this work.", "authors": [], "concepts": ["gestalt", "grouping.", "essential", "learn", "demonstrate", "strategy", "flexibly", "poorly", "tasks", "equally", "question", "model", "object", "show", "disassociate", "etc.[13][14][15][16][17][18][19].", "computations", "either", "brain", "understood.", "spatial", "these", "forming", "about", "strategies:", "object-based", "low-level", "featuring", "good", "more", "from", "interactions", "however,", "suggests", "cues", "distinct", "groups", "systematically", "towards", "task", "objects", "horizontal,", "relative", "according", "neurons.", "there", "networks", "coarse", "combinations", "groups.extant", "position", "propagating", "features", "strains", "learning", "contributions", "intelligence.", "high-level", "rescue", "target", "work.", "ability", "rely", "network", "grouping", "types", "activities,", "form", "connectivity,", "thought", "step", "disentangling", "contributed", "difficulty", "mechanisms", "neural", "with", "including", "neighboring", "whereas", "\"gestalt\"", "evaluating", "predictions", "supporting", "propagation", "that", "solely", "between", "object.", "stress", "expectations", "similarity,", "findings", "implemented", "scenes", "top-down", "processing.", "contrast,", "horizontal", "continuation,", "connections", "address", "architectures", "feedback", "computational", "theory", "increasing", "synthetic", "resolve", "laws", "tasks,", "this", "which", "mediated", "authors", "bottom-up", "bottom-up,", "limitation", "individuating", "visual", "roles", "perceptual", "incremental", "arise"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "17711681": {"id": "17711681", "openalex": null, "doi": null, "title": "MAXIMUM ENTROPY FLOW NETWORKS", "abstract": "Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, rather than the traditional method of optimizing over the continuous density directly, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. By exploiting recent developments in normalizing flow networks, we cast the maximum entropy problem into a finite-dimensional constrained optimization, and solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks. * These authors contributed equally.", "authors": [], "concepts": ["learn", "computer", "demonstrate", "nontrivial", "paper,", "(entropy)", "problem", "continuous", "show", "method", "directly,", "recent", "lagrangian", "finite-dimensional", "networks.", "modeling", "exploiting", "maximized", "these", "normalizing", "itself.", "into", "optimizing", "using", "flexible", "function", "accuracy", "simple", "framework", "models", "method,", "being", "networks", "rather", "solve", "doing", "networks,", "popular", "statistical", "desired", "constrained", "applications", "invertible", "traditional", "combining", "vision", "method.", "contributed", "with", "given", "flexibility", "developments", "maps", "finance", "that", "distribution", "cast", "transformation", "maximum", "augmented", "formulating", "effectiveness", "partial", "flow", "simulation", "objective", "results", "equally.", "optimization,", "entropy", "than", "over", "this", "authors", "density", "knowledge.", "smooth", "optimization", "stochastic", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "182952687": {"id": "182952687", "openalex": null, "doi": null, "title": "WATCH, TRY, LEARN: META-LEARNING FROM DEMONSTRATIONS AND REWARDS", "abstract": "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-anderror experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.", "authors": [], "concepts": ["learn", "feedback.", "ambiguity", "must", "learning,", "demonstration", "exploration.", "effectively", "behaviors", "meta-learning", "show", "method", "alone", "scale", "prior", "meta-reinforcement", "number", "learn:", "from", "broader", "however,", "presence", "impractical", "towards", "task", "agents", "propose", "demonstrations.", "successfully", "reduces", "information;", "data.", "meta-imitation,", "both", "demonstrations", "learning", "improve", "substantially", "significantly", "sparse", "experiments", "vision-based", "promising", "challenging,", "burden", "control", "unobserved", "tasks.", "with", "reward", "comparison", "beyond", "work,", "approach", "outperforms", "that", "distributions", "itself", "approaches", "try,", "efficiently", "enabling", "leveraging", "allows", "provide", "rewards", "experience", "imitation", "also", "infer", "autonomously", "watch,", "tasks,", "this", "enough", "trial-anderror", "enables", "agent", "policy.", "similar", "dynamics,", "require", "complex", "meta-imitation"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "18362887": {"id": "18362887", "openalex": null, "doi": null, "title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "abstract": "Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network. As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice. This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.", "authors": [], "concepts": ["inference", "respected", "tractable.", "leaves", "must", "typically", "technique.", "continuous", "model", "recently", "used", "accompanying", "techniques", "view", "specify", "probabilistic", "from", "deep", "clear", "follow", "attractive", "structure", "dual", "decomposability)", "networks", "some", "semantics", "leaves.", "conditions", "hand", "learning", "online", "gaussian", "completeness", "valid", "network", "practice.", "easy", "emerged", "(i.e.,", "neural", "with", "network.", "that", "their", "graphical", "those", "technique", "always", "first", "sum-product", "result,", "paper", "special", "parameter", "also", "introduce", "describes", "type", "this", "which", "spns", "have", "therefore", "representation", "properties"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "1890353": {"id": "1890353", "openalex": null, "doi": null, "title": "INTROSPECTION:ACCELERATING NEURAL NETWORK TRAINING BY LEARNING WEIGHT EVOLUTION", "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks.We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks. * This work was done as part of an internship at Adobe Systems, Noida", "authors": [], "concepts": ["state-of-the-art", "learn", "machine", "paper,", "numerous", "used", "them", "method", "idea", "classification.", "networks.", "achieved", "faster", "cifar-10", "large", "footprint", "general", "explore", "classification", "from", "part", "function", "accuracy", "evolution", "optimizers", "simple", "convergence.", "networks", "utilize", "work", "mnist", "terms", "great", "learning", "noida", "computationally", "done", "memory", "other", "network", "success", "novel", "pattern", "indicate", "makes", "accuracy,", "training", "tasks.", "neural", "with", "systems,", "introspection:accelerating", "adobe", "networks.we", "that", "internship", "their", "imagenet", "weight", "efficient.", "accelerate", "spite", "trend", "difficult", "results", "accelerating", "also", "various", "this", "during", "time", "give", "have", "approximators"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "189856873": {"id": "189856873", "openalex": null, "doi": null, "title": "Towards Stable and Efficient Training of Verifiably Robust Neural Networks", "abstract": "Training neural networks with verifiable robustness guarantees is challenging. Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures. Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training. In this paper, we propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based bound, CROWN, in a backward bounding pass. CROWN-IBP is computationally efficient and consistently outperforms IBP baselines on training verifiably robust neural networks. We conduct large scale experiments on MNIST and CIFAR datasets, and outperform all previous linear relaxation and bound propagation based certified defenses in \u221e robustness. Notably, we achieve 7.02% verified test error on MNIST at = 0.3, and 66.94% on CIFAR-10 with = 8/255. * Work partially done during an internship at DeepMind. . On the effectiveness of interval bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018. Conv 4 5 \u00d7 5+1, Conv 8 5 \u00d7 5+1, Conv 8 5 \u00d7 5+4, FC 64 F Conv 8 5 \u00d7 5+1, Conv 16 5 \u00d7 5+1, Conv 16 5 \u00d7 5+4, FC 128 G Conv 4 3 \u00d7 3+1, Conv 4 4 \u00d7 4+2, Conv 8 3 \u00d7 3+1, Conv 8 4 \u00d7 4+2, FC 256, FC 256 H Conv 8 3 \u00d7 3+1, Conv 8 4 \u00d7 4+2, Conv 16 3 \u00d7 3+1, Conv 16 4 \u00d7 4+2, FC 256, FC 256 I Conv 4 3 \u00d7 3+1, Conv 4 4 \u00d7 4+2, Conv 8 3 \u00d7 3+1, Conv 8 4 \u00d7 4+2, FC 512, FC 512 J Conv 8 3 \u00d7 3+1, Conv 8 4 \u00d7 4+2, Conv 16 3 \u00d7 3+1, Conv 16 4 \u00d7 4+2, FC 512, FC 512 K Conv 16 3 \u00d7 3+1, Conv 16 4 \u00d7 4+2, Conv 32 3 \u00d7 3+1, Conv 32 4 \u00d7 4+2, FC 256, FC 256 L Conv 16 3 \u00d7 3+1, Conv 16 4 \u00d7 4+2, Conv 32 3 \u00d7 3+1, Conv 32 4 \u00d7 4+2, FC 512, FC 512 M Conv 32 3 \u00d7 3+1, Conv 32 4 \u00d7 4+2, Conv 64 3 \u00d7 3+1, Conv 64 4 \u00d7 4+2, FC 512, FC 512 N Conv 64 3 \u00d7 3+1, Conv 64 4 \u00d7 4+2, Conv 128 3 \u00d7 3+1, Conv 128 4 \u00d7 4+2, FC 512, FC 512 O(MNIST Only) Conv 64 5 \u00d7 5+1, Conv 128 5 \u00d7 5+1, Conv 128 4 \u00d7 4+4, FC 512 P(MNIST Only) Conv 32 5 \u00d7 5+1, Conv 64 5 \u00d7 5+1, Conv 64 4 \u00d7 4+4, FC 512 Q Conv 16 5 \u00d7 5+1, Conv 32 5 \u00d7 5+1, Conv 32 5 \u00d7 5+4, FC 512 R Conv 32 3 \u00d7 3+1, Conv 64 3 \u00d7 3+1, Conv 64 3 \u00d7 3+4, FC 512 S(CIFAR-10 Only) Conv 32 4 \u00d7 4+2, Conv 64 4 \u00d7 4+2, FC 128 T(CIFAR-10 Only) Conv 64 4 \u00d7 4+2, Conv 128 4 \u00d7 4+2, FC 256", "authors": [], "concepts": ["partially", "bound", "paper,", "outperform", "error", "robust", "they", "test", "only)", "networks.", "efficient", "pass", "cifar-10", "3+1,", "existing", "large", "scale", "verifiable", "fast", "much", "forward", "arxiv", "from", "verified", "many", "s(cifar-10", "o(mnist", "66.94%", "towards", "2018.", "propose", "method,", "networks", "conduct", "utilize", "interval", "methods", "work", "mnist", "hundreds", "t(cifar-10", "linear", "deepmind.", "conv", "significantly", "output", "especially", "computationally", "done", "architectures.", "experiments", "stable", "7.02%", "network", "challenging.", "stability", "5+4,", "crown,", "slow", "based", "cifar", "combining", "robustness.", "verifiably", "achieve", "since", "training", "neural", "looser", "notably,", "with", "relaxation", "meanwhile,", "several", "5+1,", "p(mnist", "pass.", "propagation", "outperforms", "internship", "training.", "preprint", "down", "previous", "robustness", "8/255.", "suffer", "(ibp)", "effectiveness", "defenses", "approaches", "backward", "bound,", "4+4,", "datasets,", "baselines", "tight", "arxiv:1810.12715,", "depending", "crown-ibp,", "certified", "bounding", "3+4,", "4+2,", "models.", "0.3,", "consistently", "tasks,", "this", "during", "issues", "factor", "256,", "bounds", "adversarial", "crown-ibp", "perturbation,", "underlying", "under", "beginning", "512,", "guarantees"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "20038688": {"id": "20038688", "openalex": null, "doi": null, "title": "META-LEARNING AND UNIVERSALITY: DEEP REPRESENTATIONS AND GRADIENT DESCENT CAN APPROXIMATE ANY LEARNING ALGORITHM", "abstract": "Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models. . Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. . One-shot visual imitation learning via meta-learning. Conference on Robot Learning (CoRL), 2017b.Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks. Neural networks, 1989.", "authors": [], "concepts": ["aims", "learn", "widely", "rl2:", "parameters", "paper,", "aforementioned", "particular,", "problem", "effectively", "approximate", "indeed", "continuous", "model", "meta-learning", "recent", "dataset", "test", "recurrent", "networks.", "meta-learner.", "does", "fast", "more", "arxiv", "from", "mappings", "into", "deep", "strategies", "comparing", "leads", "consider", "conference", "fine-tuned,", "compared", "reinforcement", "models", "learning.", "acquire", "networks,", "popular", "question:", "inputs.", "universality,", "expressive", "learning", "one-shot", "gradient", "arxiv:1611.02779,", "meta-learning.", "output", "realization", "perspective", "abbeel,", "model,", "slow", "alternatively,", "powerful", "1989.", "2017b.ken-ichi", "embed", "(corl),", "data", "input", "training", "tasks.", "model-agnostic", "neural", "with", "train", "sufficient", "sergey", "standard", "levine.", "predictions", "find,", "pieter", "paradigm", "adaptation", "capacity", "answer", "approach", "that", "preprint", "algorithm?", "those", "learned", "approximation", "approaches", "representations", "generalize", "enabling", "funahashi.", "true,", "gradient-based", "experiments,", "combined", "robot", "find", "following", "descent,", "imitation", "finn,", "models.", "power", "consistently", "this", "seek", "notion", "algorithm", "efficiently.", "have", "visual", "universality:", "represented", "representation", "formalizing", "read", "further", "2016.chelsea", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "203641746": {"id": "203641746", "openalex": null, "doi": null, "title": "Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks", "abstract": "Recent theoretical work has established connections between over-parametrized neural networks and linearized models governed by the Neural Tangent Kernels (NTKs). NTK theory leads to concrete convergence and generalization results, yet the empirical performance of neural networks are observed to exceed their linearized models, suggesting insufficiency of this theory.Towards closing this gap, we investigate the training of over-parametrized neural networks that are beyond the NTK regime yet still governed by the Taylor expansion of the network. We bring forward the idea of randomizing the neural networks, which allows them to escape their NTK and couple with quadratic models. We show that the optimization landscape of randomized two-layer networks are nice and amenable to escaping-saddle algorithms. We prove concrete generalization and expressivity results on these randomized networks, which leads to sample complexity bounds (of learning certain simple functions) that match the NTK and can in addition be better by a dimension factor when mild distributional assumptions are present. We demonstrate that our randomization technique can be generalized systematically beyond the quadratic case, by using it to find networks that are coupled with higher-order terms in their Taylor series. * Salesforce Research. yu.bai@salesforce.com \u2020 Princeton University. jasonlee@princeton.edu design. arXiv preprint arXiv:1711.00501, 2017. . Characterizing implicit bias in terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. In Advances in Neural Information Processing Systems, pages 9461-9471, 2018b.Benjamin D Haeffele and Ren\u00e9 Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016. . Kernel and deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. arXiv preprint arXiv:1904.00687, 2019. gradient descent optimizes over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.", "authors": [], "concepts": ["soudry,", "demonstrate", "2015.moritz", "higher-order", "beyond.", "nati", "learning,", "expressivity", "lee,", "haeffele", "amenable", "characterizing", "linearized", "daniel", "theoretical", "functions)", "them", "addition", "show", "observed", "factorization,", "recent", "idea", "algorithms.", "networks.", "information", "certain", "these", "matters", "limitations", "kernels", "empirical", "forward", "performance", "arxiv", "deep", "suggesting", "leads", "using", "theory.towards", "convolutional", "relu", "concrete", "systematically", "closing", "tensor", "arxiv:1811.08888,", "2018.", "simple", "jason", "global", "convergence", "two-layer", "models", "generalization", "yu.bai@salesforce.com", "escape", "networks", "wide", "learning.", "advances", "arxiv:1906.05827,", "complexity", "mild", "tangent", "networks,", "random", "work", "2019.gilad", "features", "terms", "better", "expansion", "generalized", "linear", "pages", "learning", "srebro.", "linearization:", "arxiv:1711.00501,", "gradient", "ren\u00e9", "randomized", "2018a.suriya", "university.", "governed", "prove", "arxiv:1802.08246,", "understanding", "shamir.", "overparametrized", "ohad", "taylor", "2018b.benjamin", "over-parametrized", "when", "randomization", "regime", "training", "salesforce", "2016.", "neural", "present.", "with", "systems,", "identity", "(ntks).", "implicit", "network.", "over-parameterized", "escaping-saddle", "bias", "couple", "design.", "beyond", "dimension", "assumptions", "that", "gunasekar,", "case,", "geometry.", "preprint", "between", "arxiv:1611.04231,", "their", "landscape", "distributional", "yehudai", "2019.", "2017.", "processing", "arxiv:1904.00687,", "approximation", "quadratic", "technique", "hardt", "bring", "nice", "connections", "optimality", "allows", "kernel", "regimes", "match", "optimizes", "find", "jasonlee@princeton.edu", "investigate", "theory", "results", "coupled", "established", "randomizing", "tengyu", "series.", "exceed", "sample", "princeton", "research.", "models.", "power", "this", "which", "still", "gap,", "vidal.", "9461-9471,", "results,", "factor", "bounds", "optimization", "insufficiency", "arxiv:1506.07540,", "models,", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "203836948": {"id": "203836948", "openalex": null, "doi": null, "title": "INDUCTIVE MATRIX COMPLETION BASED ON GRAPH NEURAL NETWORKS", "abstract": "We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user's age or movie's genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive -it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems.", "authors": [], "concepts": ["state-of-the-art", "learn", "achieving", "unseen", "high-quality", "complete,", "methods;", "(users)", "paper,", "trains", "most", "recommender", "(given", "model", "used", "user", "gives", "side", "show", "majority", "surprisingly", "available", "model?", "dataset", "(igmc)", "systems.", "extreme", "predictions.", "item;", "information", "modeling", "such", "matrices.", "these", "existing", "dependencies", "purely", "highly", "(user,", "performance.", "good", "performance", "from", "interactions", "into", "problem.", "however,", "matrix", "predict", "using", "subgraphs", "information),", "rating", "while", "might", "propose", "1-hop", "models", "achieves", "networks", "transfer", "rows", "methods", "(items),", "graph", "work", "information.", "better", "movie's", "pair", "learning", "without", "item)", "low-dimensional", "local", "completion", "latent", "other", "experiments", "network", "inductive", "competitive", "embeddings", "based", "necessary", "corresponding", "content", "movie", "addition,", "generated", "ratings", "ratings.", "since", "training", "tasks.", "rows/columns", "neural", "works", "with", "train", "transductive,", "factorizing", "trained", "maps", "extract.", "that", "their", "graph-based", "previous", "long-range", "learned", "douban", "demonstrates", "effective", "patterns", "make", "around", "generalize", "always", "predictors", "exist),", "address", "(side", "(gnn)", "possible", "performances", "where", "movielens", "than", "this", "still", "product", "during", "(rating)", "users/items", "cannot", "pairs", "baselines.", "available,", "directly", "columns", "similar", "inductive,", "under", "transductive", "that:", "even", "hard", "igmc", "genre,", "setting", "user's"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "204907203": {"id": "204907203", "openalex": null, "doi": null, "title": "THIEVES ON SESAME STREET! MODEL EXTRACTION OF BERT-BASED APIS", "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al.,  2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction-membership classification and API watermarking-which while successful against naive adversaries, are ineffective against more sophisticated ones.", "authors": [], "concepts": ["inference", "meaningful", "attacker", "fine-tune", "access", "performs", "tasks", "problem", "heuristics", "dollars,", "question", "model", "assuming", "show", "within", "adversaries,", "such", "victim", "sequences", "large", "only", "does", "classification", "more", "strategies", "while", "defense", "diverse", "towards", "grammatical", "adversary", "sesame", "successfully", "attack.", "thus", "transfer", "both", "query", "random", "methods", "work", "made", "finally,", "extract", "apis", "learning", "ineffective", "budget", "processing,", "successful", "answering.", "local", "semantically", "ones.", "2019),", "naive", "pretrained", "bert-based", "form", "queries:", "data", "training", "community:", "with", "including", "extraction", "attempts", "highlights", "extraction-membership", "shift", "that", "mount", "real", "fact,", "feasible", "language", "exploit", "model.", "effective", "natural", "need", "hundred", "copy", "thieves", "watermarking-which", "street!", "al.,", "coupled", "reconstruct", "slightly", "than", "task-specific", "which", "words", "(devlin", "sophisticated", "bert", "worse", "even", "study", "queries", "against"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "208910151": {"id": "208910151", "openalex": null, "doi": null, "title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation", "abstract": "We design a new provably efficient algorithm for episodic reinforcement learning with generalized linear function approximation. We analyze the algorithm under a new expressivity assumption that we call \"optimistic closure,\" which is strictly weaker than assumptions from prior analyses for the linear setting. With optimistic closure, we prove that our algorithm enjoys a regret bound of\u00d5(where d is the dimensionality of the state-action features and T is the number of episodes. This is the first statistically and computationally efficient algorithm for reinforcement learning with generalized linear functions.", "authors": [], "concepts": ["setting.", "of\u00f5(where", "bound", "strictly", "expressivity", "regret", "efficient", "prior", "number", "approximation.", "from", "function", "reinforcement", "\"optimistic", "features", "closure,\"", "enjoys", "generalized", "linear", "learning", "functions.", "prove", "state-action", "computationally", "dimensionality", "statistically", "analyses", "provably", "with", "design", "call", "assumptions", "that", "episodic", "analyze", "optimistic", "assumption", "closure,", "approximation", "first", "than", "this", "which", "algorithm", "optimism", "weaker", "episodes.", "under"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "209531816": {"id": "209531816", "openalex": null, "doi": null, "title": "THE GAMBLER'S PROBLEM AND BEYOND", "abstract": "We analyze the Gambler's problem, a simple reinforcement learning problem where the gambler has the chance to double or lose their bets until the target is reached. This is an early example introduced in the reinforcement learning textbook bySutton & Barto (2018), where they mention an interesting pattern of the optimal value function with high-frequency components and repeating nonsmooth points. It is however without further investigation. We provide the exact formula for the optimal value function for both the discrete and the continuous cases. Though simple as it might seem, the value function is pathological: fractal, self-similar, derivative taking either zero or infinity, not smooth on any interval, and not written as elementary functions. It is in fact one of the generalized Cantor functions, where it holds a complexity that has been uncharted thus far. Our analyses could lead insights into improving value function approximation, gradientbased algorithms, and Q-learning, in real applications and implementations.", "authors": [], "concepts": ["holds", "problem", "continuous", "lead", "they", "q-learning,", "reached.", "either", "written", "(2018),", "lose", "insights", "been", "into", "gradientbased", "barto", "function", "might", "simple", "algorithms,", "optimal", "reinforcement", "thus", "until", "complexity", "pathological:", "both", "bets", "cases.", "textbook", "problem,", "generalized", "learning", "fact", "functions.", "self-similar,", "example", "double", "functions,", "target", "without", "could", "investigation.", "applications", "approximation,", "elementary", "repeating", "gambler's", "pattern", "analyses", "cantor", "bysutton", "early", "implementations.", "interesting", "zero", "nonsmooth", "chance", "with", "discrete", "components", "exact", "seem,", "beyond", "introduced", "gambler", "that", "derivative", "real", "their", "points.", "analyze", "fractal,", "value", "provide", "where", "this", "taking", "improving", "however", "far.", "smooth", "high-frequency", "formula", "mention", "uncharted", "infinity,", "interval,", "further", "though"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "210911499": {"id": "210911499", "openalex": null, "doi": null, "title": "GRAPH CONSTRAINED REINFORCEMENT LEARNING FOR NATURAL LANGUAGE ACTION SPACES", "abstract": "Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. We present KG-A2C 1 , an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. We contend that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language actions. Results across a wide variety of IF games show that KG-A2C outperforms current IF agents despite the exponential increase in action space size.", "authors": [], "concepts": ["environments", "actions", "size.", "action", "show", "they", "interactive", "exponential", "combinatorially-large", "about", "large", "despite", "game", "purely", "through", "increase", "present", "using", "generation", "while", "reason", "games", "agents", "dual", "reinforcement", "wide", "current", "keys", "graph", "understanding,", "exploring", "constrain", "learning", "interacts", "actions.", "constrained", "space", "uses", "challenges", "template-based", "exploration", "ideal", "kg-a2c", "world", "across", "with", "observability,", "fiction", "space.", "dynamic", "spaces", "studying", "contend", "outperforms", "that", "knowledge", "language", "scalable", "extend", "natural", "builds", "partial", "simulations", "spaces.", "results", "combinatorially", "which", "state", "agent", "meet", "text-based", "variety", "generates", "language."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "211082893": {"id": "211082893", "openalex": null, "doi": null, "title": "VIDEOFLOW: A CONDITIONAL FLOW-BASED MODEL FOR STOCHASTIC VIDEO GENERATION", "abstract": "Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally as in the case of pixel-level autoregressive models, or do not directly optimize the likelihood of the data. To our knowledge, our work is the first to propose multi-frame video prediction with normalizing flows, which allows for direct optimization of the data likelihood, and produces high-quality stochastic predictions. We describe an approach for modeling the latent space dynamics, and demonstrate that flow-based generative models offer a viable and competitive approach to generative modeling of video. * A majority of this work was done as part of the Google AI Residency Program.", "authors": [], "concepts": ["case", "future", "learn", "demonstrate", "high-quality", "flow-based", "expensive", "flows,", "model", "majority", "recent", "interactions.", "predictions.", "central", "modeling", "either", "such", "sequences", "pixel-level", "normalizing", "number", "highly", "represent", "probabilistic", "imply", "part", "however,", "predict", "extremely", "generation", "many", "generative", "physical", "propose", "capture", "models", "conditional", "data.", "produces", "optimize", "work", "video", "viable", "computationally", "done", "latent", "video.", "space", "google", "competitive", "real-world", "past", "data", "principle,", "although", "offer", "works", "with", "direct", "approach", "that", "futures.", "phenomena,", "uncertain:", "can,", "prediction", "autoregressive", "sequence", "residency", "likelihood,", "first", "allows", "videoflow:", "challenge", "events", "possible", "observations", "multi-frame", "uncertain", "this", "which", "futures,", "have", "directly", "optimization", "likelihood", "studied", "program.", "describe", "dynamics,", "models,", "knowledge,", "complex", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "211126562": {"id": "211126562", "openalex": null, "doi": null, "title": "FREQUENCY-BASED SEARCH-CONTROL IN DYNA", "abstract": "Model-based reinforcement learning has been empirically demonstrated as a successful strategy to improve sample efficiency. In particular, Dyna is an elegant model-based architecture integrating learning and planning that provides huge flexibility of using a model. One of the most important components in Dyna is called search-control, which refers to the process of generating state or state-action pairs from which we query the model to acquire simulated experiences. Searchcontrol is critical in improving learning efficiency. In this work, we propose a simple and novel search-control strategy by searching high frequency regions of the value function. Our main intuition is built on Shannon sampling theorem from signal processing, which indicates that a high frequency signal requires more samples to reconstruct. We empirically show that a high frequency function is more difficult to approximate. This suggests a search-control strategy: we should use states from high frequency regions of the value function to query the model to acquire more samples. We develop a simple strategy to locally measure the frequency of a function by gradient and hessian norms, and provide theoretical justification for this approach. We then apply our strategy to search-control in Dyna, and conduct experiments to show its property and effectiveness on benchmark domains. * Equal contribution.Published as a conference paper at ICLR 2020 model errors too, which causes some performance deterioration(Talvitie, 2014;2017). Without an elegant search-control mechanism, we are not likely to benefit from the flexibility given by a model.", "authors": [], "concepts": ["strategy:", "strategy", "most", "particular,", "theoretical", "model", "show", "samples.", "locally", "simulated", "elegant", "been", "theorem", "domains.", "more", "sampling", "performance", "from", "suggests", "using", "hessian", "function", "dyna", "conference", "mechanism,", "should", "demonstrated", "built", "measure", "simple", "requires", "frequency", "propose", "reinforcement", "frequency-based", "likely", "critical", "process", "some", "conduct", "query", "acquire", "causes", "generating", "learning", "processing,", "improve", "successful", "signal", "gradient", "without", "errors", "dyna,", "state-action", "search-control,", "experiments", "high", "intuition", "contribution.published", "novel", "norms,", "given", "searching", "states", "components", "approach.", "flexibility", "benchmark", "work,", "deterioration(talvitie,", "that", "function.", "benefit", "integrating", "refers", "model.", "effectiveness", "called", "apply", "empirically", "indicates", "value", "paper", "2014;2017).", "search-control", "provide", "difficult", "efficiency.", "too,", "then", "sample", "model-based", "approximate.", "this", "which", "state", "property", "provides", "samples", "improving", "searchcontrol", "reconstruct.", "pairs", "develop", "regions", "iclr", "main", "important", "architecture", "planning", "huge", "shannon", "justification", "equal", "experiences."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "211132391": {"id": "211132391", "openalex": null, "doi": null, "title": "UNDERSTANDING THE LIMITATIONS OF CONDITIONAL GENERATIVE MODELS", "abstract": "Class-conditional generative models hold promise to overcome the shortcomings of their discriminative counterparts. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based generative models from a theoretical and practical perspective to investigate if they can deliver on their promises. Our analysis focuses on a spectrum of robustness properties:(1)Detection of worst-case outliers in the form of adversarial examples; (2) Detection of average-case outliers in the form of ambiguous inputs and (3) Detection of incorrectly labeled in-distribution inputs.Our theoretical result reveals that it is impossible to guarantee detectability of adversarially-perturbed inputs even for near-optimal generative classifiers. Experimentally, we find that while we are able to train robust models for MNIST, robustness completely breaks down on CIFAR10. We relate this failure to various undesirable model properties that can be traced to the maximum likelihood training objective. Despite being a common choice in the literature, our results indicate that likelihood-based conditional generative models may are surprisingly ineffective for robust classification.Published as a conference paper at ICLR 2020 jointly modeling the input and target distribution should make it easy to detect out-of-distribution inputs. These traits lend hope to the belief that good class-conditional generative models can overcome important problems faced by discriminative models.", "authors": [], "concepts": ["experimentally,", "shortcomings", "accurate", "focuses", "robust", "tasks", "failure", "theoretical", "model", "surprisingly", "they", "likelihood-based", "modeling", "deliver", "incorrectly", "reveals", "these", "hold", "ambiguous", "limitations", "out-of-distribution", "inputs.our", "despite", "classification", "able", "good", "performance", "from", "generative", "while", "conference", "predictive", "should", "models", "labeled", "outliers", "being", "conditional", "solve", "average-case", "manner", "optimize", "impossible", "inputs.", "ineffective", "problems", "target", "discriminative", "relate", "overcome", "completely", "perspective", "understanding", "inputs", "belief", "classifiers.", "analysis", "traced", "examples;", "class-conditional", "in-distribution", "form", "indicate", "promises.", "easy", "input", "training", "with", "train", "traits", "jointly", "common", "detection", "work,", "that", "distribution", "near-optimal", "their", "down", "robustness", "maximum", "counterparts.", "natural", "make", "choice", "faced", "mnist,", "result", "hope", "literature,", "paper", "promise", "practical", "find", "investigate", "breaks", "results", "detect", "various", "models.", "this", "worst-case", "undesirable", "adversarial", "detectability", "classification.published", "adversarially-perturbed", "objective.", "iclr", "likelihood", "cifar10.", "important", "properties:(1)detection", "spectrum", "even", "properties", "guarantee", "lend", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "211549689": {"id": "211549689", "openalex": null, "doi": null, "title": "CLASSIFICATION-BASED ANOMALY DETECTION FOR GENERAL DATA", "abstract": "Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains.", "authors": [], "concepts": ["state-of-the-art", "non-image", "broad", "method", "transformation-based", "view", "extensively", "artificial", "were", "multiple", "affine", "general", "seen", "shown", "datasets", "domains.", "performance", "from", "present", "using", "accuracy", "assumptions.", "fundamental", "propose", "transformations.", "generalization", "method,", "current", "furthermore,", "unifying", "random", "methods", "superior", "relax", "intelligence.", "problems", "classification-based", "recently,", "substantially", "validated", "data", "achieve", "deviate", "open-set", "applicability", "detection", "work,", "previously,", "task.", "that", "finding", "strong", "different", "those", "extend", "patterns", "results", "detection,", "this", "anomaly", "goad,", "types.", "applicable", "obtain"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "213969759": {"id": "213969759", "openalex": null, "doi": null, "title": "MUTUAL INFORMATION GRADIENT ESTIMATION FOR REPRESENTATION LEARNING", "abstract": "Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.", "authors": [], "concepts": ["accurate", "most", "continuous", "recent", "score", "estimator", "estimators", "information", "indicated", "existing", "itself.", "more", "performance", "large.", "deep", "however,", "gradients", "infomax", "propose", "learning.", "advances", "bottleneck", "significant", "both", "high-dimensional", "methods", "mige", "(mige)", "capable", "large-mi", "learning", "experimental", "gradient", "applications", "estimation", "argue", "when", "providing", "based", "establish", "method.", "with", "implicit", "improvement", "settings.", "that", "unfortunately", "exhibits", "representation.", "end,", "scalable", "useful", "plays", "distributions.", "representations", "tight", "unsupervised", "(mi)", "tractable", "results", "appealing", "expand", "than", "intractable", "this", "discover", "have", "smooth", "directly", "role", "estimating", "low-variance", "important", "representation", "mutual"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "219558760": {"id": "219558760", "openalex": null, "doi": null, "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications", "abstract": "Graph neural networks (GNNs) were shown to effectively learn from highly structured data containing elements (nodes) with relationships (edges) between them. GNN variants differ in how each node in the graph absorbs the information flowing from its neighbor nodes. In this paper, we highlight an inherent problem in GNNs: the mechanism of propagating information between neighbors creates a bottleneck when every node aggregates messages from its neighbors. This bottleneck causes the over-squashing of exponentially-growing information into fixed-size vectors. As a result, the graph fails to propagate messages flowing from distant nodes and performs poorly when the prediction task depends on long-range information. We demonstrate that the bottleneck hinders popular GNNs from fitting the training data. We show that GNNs that absorb incoming edges equally, like GCN and GIN, are more susceptible to over-squashing than other GNN types. We further show that existing, extensively-tuned, GNN-based models suffer from over-squashing and that breaking the bottleneck improves state-of-the-art results without any hyperparameter tuning or additional weights.Preprint. Under review.", "authors": [], "concepts": ["state-of-the-art", "learn", "nodes", "demonstrate", "each", "poorly", "paper,", "implications", "edges", "performs", "problem", "effectively", "hinders", "show", "inherent", "messages", "gnn-based", "information", "exponentially-growing", "weights.preprint.", "gnns", "were", "gnns:", "variants", "highly", "shown", "more", "from", "into", "breaking", "them.", "task", "(edges)", "models", "differ", "nodes.", "networks", "bottleneck", "data.", "propagating", "popular", "graph", "causes", "information.", "neighbors", "flowing", "relationships", "neighbors.", "like", "additional", "over-squashing", "without", "gin,", "other", "vectors.", "creates", "equally,", "distant", "absorb", "when", "improves", "highlight", "data", "training", "neural", "hyperparameter", "with", "fails", "tuning", "existing,", "that", "between", "absorbs", "extensively-tuned,", "prediction", "structured", "containing", "long-range", "suffer", "incoming", "result,", "susceptible", "practical", "propagate", "review.", "results", "fixed-size", "neighbor", "depends", "aggregates", "(nodes)", "than", "this", "node", "elements", "types.", "under", "fitting", "mechanism", "(gnns)", "further", "every"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "219708206": {"id": "219708206", "openalex": null, "doi": null, "title": "Learning continuous-time PDEs from sparse data with graph neural networks", "abstract": "The behavior of many dynamical systems follow complex, yet still unknown partial differential equations (PDEs). While several machine learning methods have been proposed to learn PDEs directly from data, previous methods are limited to discretetime approximations or make the limiting assumption of the observations arriving at regular grids. We propose a general continuous-time differential model for dynamical systems whose governing equations are parameterized by message passing graph neural networks. The model admits arbitrary space and time discretizations, which removes constraints on the locations of observation points and time intervals between the observations. The model is trained with continuous-time adjoint method enabling efficient neural PDE inference. We demonstrate the model's ability to work with unstructured grids, arbitrary time steps, and noisy observations. We compare our method with existing approaches on several well-known physical systems that involve first and higher-order PDEs with state-of-the-art predictive performance.Preprint. Under review.", "authors": [], "concepts": ["state-of-the-art", "learn", "unstructured", "demonstrate", "well-known", "machine", "higher-order", "data,", "pdes", "observation", "model", "method", "networks.", "efficient", "discretetime", "existing", "been", "general", "regular", "from", "follow", "differential", "many", "while", "discretizations,", "predictive", "inference.", "physical", "propose", "parameterized", "networks", "intervals", "limiting", "methods", "graph", "work", "learning", "model's", "(pdes).", "ability", "sparse", "governing", "space", "constraints", "admits", "points", "data", "neural", "with", "noisy", "behavior", "complex,", "several", "equations", "systems", "trained", "message", "performance.preprint.", "that", "between", "arbitrary", "involve", "previous", "locations", "continuous-time", "assumption", "dynamical", "make", "approaches", "partial", "enabling", "first", "approximations", "arriving", "adjoint", "observations.", "unknown", "grids,", "observations", "review.", "which", "still", "steps,", "time", "have", "directly", "proposed", "limited", "passing", "under", "compare", "removes", "whose", "grids."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "219721074": {"id": "219721074", "openalex": null, "doi": null, "title": "Categorical Normalizing Flows via Continuous Transformations", "abstract": "Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate Categorical Normalizing Flows, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. To maintain unique decoding, we learn a partitioning of the latent space by factorizing the posterior. Meanwhile, the complex relations between the categorical variables are learned by the ensuing normalizing flow, thus maintaining a close-to exact likelihood estimate and making it possible to scale up to a large number of categories. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs, outperforming both one-shot and autoregressive flow-based state-of-the-art on molecule generation. Preprint. Under review. arXiv:2006.09790v1 [cs.LG] 17 Jun 2020flow. As no information should be lost when mapping the data into continuous space, we limit the encoding distributions to ones whose (approximate) posterior is independent over discrete variables. This leads to a learned partitioning of the latent space with an almost unique decoding. We call this approach Categorical Normalizing Flows and experiment with encoding distributions of increasing flexibility, but find that a simple mixture model is sufficient for encoding categorical data well.Categorical Normalizing Flows can be applied to any task involving categorical variables. Examples, which we visit experimentally in this work, include words as categorical (one-hot vector) variables, sets and graphs[48,50]. We put particular emphasis on graphs, as current approaches are mostly autoregressive[18,36,49]and view graphs as sequences, although there exists no intrinsic order of the nodes. Normalizing flows, however, can perform generation in parallel making a definition of order unnecessary. By treating both nodes and edges as categorical variables, we employ our variational inference encoding and propose GraphCNF. GraphCNF is a novel permutation-invariant normalizing flow on graph generation which assigns equal likelihood to any ordering of nodes. Meanwhile, GraphCNF encodes the node attributes, edge attributes and graph structure in three consecutive steps. As shown in the experiments, the improved encoding and flow architecture allows GraphCNF to outperform significantly both the autoregressive and parallel flow-based state-of-the-art.Overall, our contributions are summarized as follows:\u2022 We propose Categorical Normalizing Flows, which apply a novel encoding method for categorical data in normalizing flows. By using variational inference with a factorized posterior, we still support an close-to exact likelihood estimate and scale up to large number of categories. \u2022 Starting from the Categorical Normalizing Flows, we propose GraphCNF, a permutationinvariant normalizing flow on graph generation. On molecule generation, GraphCNF sets a new state-of-the-art for flow-based methods outperforming one-shot and autoregressive baselines. \u2022 We experiment with encoding distributions of increasing flexibility on various tasks including sets, language and graphs, and show that a simple mixture model is sufficient for modeling discrete, categorical distribution accurately.", "authors": [], "concepts": ["state-of-the-art", "inference", "learn", "nodes", "ordering", "transformations", "(one-hot", "flow-based", "must", "paper,", "outperform", "generation,", "graphcnf,", "exists", "variational", "edges", "tasks", "dequantization", "particular", "preprint.", "continuous", "flows,", "model", "attributes,", "flows", "show", "method", "limit", "experimentally", "well.categorical", "information", "modeling", "view", "casting", "2020flow.", "large", "scale", "variables,", "normalizing", "categories.", "despite", "number", "shown", "generation.", "intrinsic", "from", "into", "visit", "however,", "order", "leads", "using", "estimate", "generation", "generative", "structure", "graphs,", "should", "graphcnf", "definition", "task", "applied", "simple", "attributes", "propose", "inapplicable", "summarized", "there", "nodes.", "flow,", "date,", "thus", "factorized", "current", "data.", "encoding", "parallel", "outperforming", "encodes", "both", "optimize", "relations", "ensuing", "methods", "posterior.", "graph", "unique", "state-of-the-art.overall,", "graphs[48,50].", "flexibility,", "problem,", "improved", "support", "include", "mapping", "one-shot", "application", "contributions", "like", "significantly", "limited.", "examples,", "permutation-invariant", "synonymy", "latent", "space", "popularity,", "follows:\u2022", "novel", "when", "based", "maintain", "posterior,", "unnecessary.", "data", "employ", "although", "treating", "making", "with", "vector)", "including", "sufficient", "discrete,", "meanwhile,", "space,", "discrete", "jointly", "factorizing", "lost", "accurately.", "decoding,", "exact", "flexibility", "call", "independent", "(approximate)", "work,", "maintaining", "inferred,", "approach", "order.", "that", "sets,", "distribution", "between", "their", "flows.", "involving", "molecule", "language", "assigns", "steps.", "permutationinvariant", "autoregressive", "sets", "learned", "distributions", "graphs", "approaches", "categorical", "apply", "flow", "autoregressive[18,36,49]and", "allows", "experiments,", "three", "find", "close-to", "investigate", "possible", "perform", "starting", "review.", "[cs.lg]", "increasing", "consecutive", "various", "variables.", "partitioning", "almost", "over", "mostly", "this", "which", "still", "words", "node", "sequences,", "arxiv:2006.09790v1", "instead,", "ones", "baselines.", "mixture", "decoding.", "have", "likelihood", "variables", "practice", "experiment", "under", "architecture", "posterior", "representation", "words.", "whose", "likelihood.", "stays", "graphcnf.", "equal", "complex", "emphasis", "edge"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "219792420": {"id": "219792420", "openalex": null, "doi": null, "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.Learning control from images is important for many real world applications. While deep reinforcement learning (RL) has enjoyed many successes in simulated tasks, learning control from real vision is more complex, especially outdoors, where images reveal detailed scenes of a complex and unstructured world. Furthermore, while many RL algorithms can eventually learn control from real images given unlimited data, data-efficiency is often a necessity in real trials which are expensive and constrained to real-time. Prior methods for data-efficient learning of simulated visual tasks typically use representation learning. Representation learning summarizes images by encoding them into smaller vectored representations better suited for RL. For example, sequential autoencoders aim to learn lossless representations of streaming observations-sufficient to reconstruct current observations and predict future observations-from which various RL algorithms can be trained[11,19,34]. However, such methods are task-agnostic: the models represent all dynamic elements they observe in the world, whether they are relevant to the task or not. We argue such representations can easily \"distract\" RL algorithms with irrelevant information in the case of real images. The issues of distraction is less evident in popular simulation MuJoCo and Atari tasks, since any change in observation space is likely task-relevant, and thus, worth representing. By contrast, visual images that autonomous cars observe contain predominately task-irrelevant information, like cloud shapes and architectural details, illustrated inFigure 1. * Equal contribution.", "authors": [], "concepts": ["case", "smaller", "future", "learn", "sota", "unstructured", "demonstrate", "achieving", "moving", "day.", "typically", "trains", "real-time.", "data,", "robust", "tasks", "cars", "expensive", "observation", "videos,", "illustrated", "continuous", "distractors", "predominately", "them", "invariance", "downstream", "method", "they", "test", "streaming", "information", "either", "such", "simulated", "replaced", "only", "lossless", "prior", "mdps,", "represent", "performance.", "more", "images", "atari", "from", "mujoco", "into", "deep", "world.", "however,", "predict", "behavioral", "encoders", "using", "example,", "many", "while", "inference.learning", "task", "invariant", "autonomous", "highway", "propose", "information,", "reinforcement", "models", "likely", "generalization", "quantify", "enjoyed", "learning.", "bisimulation", "current", "encoding", "domain", "both", "furthermore,", "contain", "popular", "methods", "shapes", "task-agnostic:", "details.", "sequential", "(rl)", "finally,", "better", "architectural", "learning", "goal", "observations-sufficient", "thus,", "like", "reconstruction", "infigure", "worth", "constrained", "without", "reveal", "not.", "distraction", "summarizes", "especially", "latent", "space", "trials", "argue", "detailed", "representing.", "relying", "observations-from", "modified", "evident", "control", "observe", "task-relevant,", "background", "vision", "since", "cloud", "world", "learns", "with", "pixel-reconstruction.", "metrics,", "given", "complex,", "less", "states", "first-person", "space.", "dynamic", "whether", "vectored", "details,", "that", "between", "unlimited", "real", "task-relevant", "\"distract\"", "trained[11,19,34].", "easily", "knowledge", "weather,", "driving", "successes", "scenes", "necessity", "world,", "autoencoders", "data-efficiency", "observations,", "rich", "effective", "natural", "contribution.", "contrast,", "effectiveness", "images,", "representations", "data-efficient", "accelerate", "causal", "distances", "simulation", "provide", "observations.", "change", "algorithms", "metrics", "observations", "results", "also", "images.", "reconstruct", "various", "where", "encode", "tasks,", "applications.", "which", "state", "suited", "issues", "elements", "task-irrelevant", "drawn", "time", "links", "disregarding", "often", "irrelevant", "visual", "relevant", "important", "representation", "study", "eventually", "properties", "clouds,", "outdoors,", "similarity", "equal", "complex"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "220769181": {"id": "220769181", "openalex": null, "doi": null, "title": "DOP: Off-Policy Multi-Agent Decomposed Policy Gradients", "abstract": "Recently, multi-agent policy gradient (MAPG) methods witness vigorous progress. However, there is a discrepancy between the performance of MAPG methods and state-of-the-art multi-agent value-based approaches. In this paper, we investigate the causes that hinder the performance of MAPG algorithms and present a multiagent decomposed policy gradient method (DOP). This method introduces the idea of value function decomposition into the multi-agent actor-critic framework. Based on this idea, DOP supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. We formally show that DOP critics have sufficient representational capability to guarantee convergence. In addition, empirical evaluations on the StarCraft II micromanagement benchmark and multi-agent particle environments demonstrate that our method significantly outperforms state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at https", "authors": [], "concepts": ["environments", "state-of-the-art", "demonstrate", "off-policy", "paper,", "witness", "action", "continuous", "show", "method", "available", "idea", "dop:", "algorithms.", "critics", "videos", "addresses", "efficient", "empirical", "decomposed", "actor-critic", "performance", "into", "present", "however,", "gradients", "function", "mapg", "issue", "micromanagement", "reinforcement", "convergence.", "there", "discrepancy", "both", "particle", "methods", "causes", "(dop).", "learning", "vigorous", "gradient", "recently,", "significantly", "demonstrative", "starcraft", "value-based", "based", "hinder", "supports", "representational", "policy", "addition,", "decomposition", "introduces", "evaluations", "sufficient", "discrete", "progress.", "benchmark", "outperforms", "that", "between", "https", "idea,", "formally", "policy-based", "centralized-decentralized", "value", "multi-agent", "spaces.", "algorithms", "investigate", "multiagent", "this", "(mapg)", "mismatch", "have", "framework.", "approaches.", "assignment", "capability", "credit", "guarantee"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "221376626": {"id": "221376626", "openalex": null, "doi": null, "title": "SOLAR: Sparse Orthogonal Learned and Random Embeddings", "abstract": "Dense embedding models are commonly deployed in commercial search engines, wherein all the document vectors are pre-computed, and near-neighbor search (NNS) is performed with the query vector to find relevant documents. However, the bottleneck of indexing a large number of dense vectors and performing an NNS hurts the query time and accuracy of these models. In this paper, we argue that highdimensional and ultra-sparse embedding is a significantly superior alternative to dense low-dimensional embedding for both query efficiency and accuracy. Extreme sparsity eliminates the need for NNS by replacing them with simple lookups, while its high dimensionality ensures that the embeddings are informative even when sparse. However, learning extremely high dimensional embeddings leads to blow up in the model size. To make the training feasible, we propose a partitioning algorithm that learns such high dimensional embeddings across multiple GPUs without any communication. This is facilitated by our novel asymmetric mixture of Sparse, Orthogonal, Learned and Random (SOLAR) Embeddings. The label vectors are random, sparse, and near-orthogonal by design, while the query vectors are learned and sparse. We theoretically prove that our way of one-sided learning is equivalent to learning both query and label embeddings. With these unique properties, we can successfully train 500K dimensional SOLAR embeddings for the tasks of searching through 1.6M books and multi-label classification on the three largest public datasets. We achieve superior precision and recall compared to the respective state-of-the-art baselines for each task with up to 10\u00d7 faster speed.Preprint. Under review.", "authors": [], "concepts": ["state-of-the-art", "size.", "each", "gpus", "paper,", "informative", "tasks", "hurts", "document", "model", "them", "respective", "vector", "accuracy.", "extreme", "recall", "solar", "near-neighbor", "faster", "such", "these", "large", "ultra-sparse", "embedding", "multiple", "number", "ensures", "random,", "properties,", "through", "classification", "sparse.", "however,", "leads", "extremely", "while", "embeddings.", "accuracy", "feasible,", "compared", "task", "wherein", "simple", "propose", "deployed", "models", "largest", "successfully", "design,", "bottleneck", "orthogonal", "both", "query", "random", "unique", "datasets.", "superior", "learning", "solar:", "books", "equivalent", "significantly", "without", "sparse", "low-dimensional", "prove", "high", "argue", "dimensionality", "500k", "novel", "embeddings", "when", "documents.", "dimensional", "label", "engines,", "commercial", "achieve", "training", "search", "theoretically", "learns", "across", "with", "train", "searching", "one-sided", "blow", "performing", "dense", "efficiency", "asymmetric", "that", "highdimensional", "alternative", "eliminates", "learned", "speed.preprint.", "replacing", "sparsity", "make", "lookups,", "need", "facilitated", "orthogonal,", "baselines", "performed", "communication.", "three", "find", "review.", "commonly", "partitioning", "models.", "vectors", "this", "near-orthogonal", "(nns)", "1.6m", "algorithm", "time", "mixture", "(solar)", "sparse,", "precision", "pre-computed,", "relevant", "under", "multi-label", "even", "indexing", "public"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "221508448": {"id": "221508448", "openalex": null, "doi": null, "title": "MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES", "abstract": "Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1 1 Implementation available at : https://github.com/reml-lab/mTAN 1 arXiv:2101.10318v2 [cs.LG] 7 Jun 2021Published as a conference paper at ICLR 2021The main contributions of the mTAN model framework are: (1) It provides a flexible approach to modeling multivariate, sparse and irregularly sampled time series data (including irregularly sampled time series of partially observed vectors) by leveraging a time attention mechanism to learn temporal similarity from data instead of using fixed kernels. (2) It uses a temporally distributed latent representation to better capture local structure in time series data. (3) It provides interpolation and classification performance that is as good as current state-of-the-art methods or better, while providing significantly reduced training times.RELATED WORKAn irregularly sampled time series is a time series with irregular time intervals between observations. In the multivariate setting, there can also be a lack of alignment across different variables within the same multivariate time series. Finally, when gaps between observation times are large, the time series is also considered to be sparse. Such data occur in electronic health records (Marlin et al.,  astronomy (Scargle, 1982). It is well understood that such data cause significant issues for standard supervised machine learning models that typically assume fully observed, fixed-size feature representations(Marlin et al., 2012).A basic approach to dealing with irregular sampling is fixed temporal discretization. For example,Marlin et al. (2012)and Lipton et al. (2016) discretize continuous-time observations into hour-long bins. This has the advantage of simplicity, but requires ad-hoc handling of bins with more than one observation and results in missing data when bins are empty.", "authors": [], "concepts": ["fixed-length", "state-of-the-art", "interpolation", "learn", "partially", "machine", "fully", "gaps", "basic", "typically", "paper,", "performs", "tasks", "cause", "records,", "observation", "2021the", "continuous", "model", "recently", "understood", "show", "observed", "available", "mtan", "ad-hoc", "within", "networks.", "modeling", "(2016)", "faster", "missing", "such", "feature", "setting,", "embedding", "multiple", "number", "reduced", "classification", "sparse.", "good", "more", "sampling", "performance", "times", "from", "variable", "into", "deep", "using", "flexible", "are:", "well", "many", "structure", "while", "conference", "baseline", "1982).", "requires", "framework", "propose", "capture", "models", "there", "networks", "intervals", "significant", "current", "data.", "health", "advantage", "arxiv:2101.10318v2", "methods", "work", "finally,", "better", "datasets.", "learning", "observed,", "contributions", "motivated", "significantly", "sparse", "applications", "methods.", "produce", "(2012)and", "local", "latent", "multi-time", "dealing", "uses", "representations(marlin", "astronomy", "(scargle,", "analysis", "when", "providing", "discretization.", "multivariate", "irregularly", "temporally", "data", "series", "training", "better,", "across", "example,marlin", "with", "occurs", "standard", "considered", "multivariate.", "call", "physiological", "(marlin", "electronic", "approach", "irregular", "that", "instead", "2012).a", "bins", "between", "kernels.", "fixed", "occur", "different", "continuous-time", "implementation", "containing", "assume", "hour-long", "multivariate,", "times.related", "presents", "attention", "sampled,", "range", "leveraging", "2021published", "bins.", "paper", "lack", "observations.", "lipton", "challenge", "temporal", "investigate", "empty.", "observations", "vectors)", "(including", "results", "[cs.lg]", "al.,", "also", "fixed-size", "simplicity,", "same", "where", "series.", "than", "models.", "discretize", "this", "which", "provides", "issues", "time", "alignment", "sparse,", "proposed", "workan", "iclr", "main", "large,", "variables", "https://github.com/reml-lab/mtan", "distributed", "representation", "records", "sampled", "supervised", "mechanism", "offering", "handling", "values", "similarity", "setting"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "222124972": {"id": "222124972", "openalex": null, "doi": null, "title": "FASTER BINARY EMBEDDINGS FOR PRESERVING EUCLIDEAN DISTANCES", "abstract": "We propose a fast, distance-preserving, binary embedding algorithm to transform a high-dimensional dataset T \u2286 R n into binary sequences in the cube {\u00b11} m . When T consists of well-spread (i.e., non-sparse) vectors, our embedding method applies a stable noise-shaping quantization scheme to Ax where A \u2208 R m\u00d7n is a sparse Gaussian random matrix. This contrasts with most binary embedding methods, which usually use x \u2192 sign(Ax) for the embedding. Moreover, we show that Euclidean distances among the elements of T are approximated by the 1 norm on the images of {\u00b11} m under a fast linear transformation. This again contrasts with standard methods, where the Hamming distance is used instead. Our method is both fast and memory efficient, with time complexity O(m) and space complexity O(m). Further, we prove that the method is accurate and its associated error is comparable to that of a continuous valued Johnson-Lindenstrauss embedding plus a quantization error that admits a polynomial decay as the embedding dimension m increases. Thus the length of the binary codes required to achieve a desired accuracy is quite small, and we show it can even be compressed further without compromising the accuracy. To illustrate our results, we test the proposed method on natural images and show that it achieves strong performance.", "authors": [], "concepts": ["increases.", "applies", "johnson-lindenstrauss", "accurate", "comparable", "error", "associated", "most", "continuous", "approximated", "used", "show", "method", "norm", "dataset", "accuracy.", "test", "non-sparse)", "methods,", "faster", "consists", "sequences", "embedding", "fast", "performance.", "images", "fast,", "into", "vectors,", "again", "decay", "accuracy", "{\u00b11}", "plus", "propose", "codes", "achieves", "compressed", "thus", "complexity", "hamming", "noise-shaping", "both", "instead.", "high-dimensional", "random", "among", "scheme", "linear", "usually", "desired", "gaussian", "without", "cube", "sparse", "prove", "euclidean", "memory", "space", "stable", "distance", "quantization", "embeddings", "when", "admits", "distance-preserving,", "contrasts", "compromising", "(i.e.,", "achieve", "preserving", "with", "standard", "illustrate", "dimension", "that", "required", "polynomial", "strong", "sign(ax)", "natural", "matrix.", "distances", "o(m)", "quite", "moreover,", "binary", "well-spread", "where", "this", "which", "results,", "elements", "algorithm", "time", "efficient,", "embedding.", "transform", "length", "proposed", "o(m).", "valued", "under", "small,", "even", "transformation.", "further", "further,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "222133372": {"id": "222133372", "openalex": null, "doi": null, "title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors", "abstract": "Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new asynchronous RED (ASYNC-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of ASYNC-RED is further reduced by using a random subset of measurements at every iteration. We present complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate ASYNC-RED on image recovery using pre-trained deep denoisers as priors.", "authors": [], "concepts": ["state-of-the-art", "developed", "denoising", "data,", "problems.", "theoretical", "recently", "method", "recent", "denoisers", "systems.", "complete", "convergent", "subset", "faster", "denoiser.", "shown", "reduced", "performance", "present", "deep", "however,", "using", "multicore", "proposing", "explicit", "asynchronous", "priors.", "framework", "issue", "large-scale", "convergence", "current", "parallel", "complexity", "block", "random", "inadequate", "work", "counterparts", "problems", "solving", "significantly", "(red)", "serial", "priors", "data-fidelity", "analysis", "when", "recovery", "regularization", "image", "validate", "provably", "making", "with", "assumptions", "that", "establishing", "integrating", "processing", "measurements", "address", "combined", "inverse", "computational", "algorithms", "denoisers.", "than", "this", "enables", "iteration.", "algorithm", "async-red:", "(async-red)", "advanced", "pre-trained", "under", "async-red", "further", "every", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "222208633": {"id": "222208633", "openalex": null, "doi": null, "title": "DEFORMABLE DETR: DEFORMABLE TRANSFORMERS FOR END-TO-END OBJECT DETECTION", "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10\u00d7 less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released. * Equal contribution. \u2020 This work is done when Weijie Su is an intern at SenseTime Research.", "authors": [], "concepts": ["demonstrate", "recently", "object", "maps.", "demonstrating", "reference.", "spatial", "these", "only", "feature", "been", "performance.", "good", "sampling", "performance", "from", "deformable", "however,", "epochs.", "many", "released.", "while", "mitigate", "hand-designed", "convergence", "weijie", "work", "better", "suffers", "end-to-end", "done", "experiments", "issues,", "detr:", "slow", "when", "code", "image", "points", "objects)", "achieve", "training", "resolution,", "modules", "with", "attend", "less", "components", "approach.", "detection", "benchmark", "transformers", "intern", "transformer", "processing", "contribution.", "detr", "effectiveness", "need", "eliminate", "attention", "around", "coco", "sensetime", "research.", "than", "this", "(especially", "small", "proposed", "limitation", "limited", "detr,", "whose", "shall", "equal", "extensive"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "222379753": {"id": "222379753", "openalex": null, "doi": null, "title": "SELF-TRAINING FOR FEW-SHOT TRANSFER ACROSS EXTREME TASK DIFFERENCES", "abstract": "All few-shot learning techniques must be pre-trained on a large, labeled \"base dataset\". In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray images), one must resort to pre-training in a different \"source\" problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on a challenging benchmark with multiple domains.", "authors": [], "concepts": ["challenging", "average", "must", "paper,", "problem", "tackle", "domain.", "show", "available", "techniques", "extreme", "imagenet),", "such", "x-ray", "large", "multiple", "pre-training", "datasets", "domains.", "performance", "images),", "from", "present", "presence", "task", "very", "simple", "gap:", "labeled", "domain", "transfer", "learning", "self-training", "one-shot", "desired", "target", "source", "traditional", "improves", "points", "data", "tasks.", "across", "with", "few-shot", "benchmark", "domains", "task.", "that", "between", "solution", "different", "resort", "effective", "unlabeled", "\"source\"", "dataset\".", "fail", "(e.g.,", "where", "\"base", "this", "which", "pre-trained", "large,", "representation", "differences"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "223953610": {"id": "223953610", "openalex": null, "doi": null, "title": "What Can You Learn from Your Muscles? Learning Visual Representation from Human Interactions", "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our self-supervised representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al., 2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance).Figure 1: We propose to use human's interactions with their visual surrounding as a training signal for self-supervised representation learning. We record first person observations as well as the movements and gaze of people living their daily routines and use these cues to learn a visual embedding. We use the learned representation on a variety of diverse tasks and show consistent improvements compared to state-of-the-art self-supervised vision-only techniques.", "authors": [], "concepts": ["improvements", "state-of-the-art", "learn", "computer", "what", "paper,", "vision-only", "visual-only", "action", "most", "tasks", "2020),", "approach,", "surrounding", "show", "downstream", "method", "dataset", "(semantic),", "videos.", "consistent", "such", "these", "person", "(geometric),", "been", "vision.", "explore", "classification", "images", "(physics)", "from", "interactions", "part", "routines", "(temporal),", "cues", "movements", "well", "diverse", "representations.", "compared", "gaze", "propose", "scene", "learning.", "encodes", "better", "learning", "collect", "signal", "surface", "target", "study,", "long", "rely", "interaction", "lives.", "people", "experiments", "estimation", "capturing", "daily", "your", "novel", "self-supervised", "muscles?", "data", "training", "with", "depth", "walkable", "whether", "outperforms", "(affordance).figure", "that", "solely", "body", "their", "tasks:", "prediction", "human's", "learned", "effective", "approaches", "representations", "attention", "generalize", "record", "first", "investigate", "observations", "living", "al.,", "where", "recognition", "dynamics", "this", "quest", "techniques.", "embedding.", "visual", "representation", "variety", "human", "moco"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "226254532": {"id": "226254532", "openalex": null, "doi": null, "title": "LEARNING AND EVALUATING REPRESENTATIONS FOR DEEP ONE-CLASS CLASSIFICATION", "abstract": "We present a two-stage framework for deep one-class classification. We first learn self-supervised representations from one-class data, and then build one-class classifiers on learned representations. The framework not only allows to learn better representations, but also permits building one-class classifiers that are faithful to the target task. In particular, we present a novel distribution-augmented contrastive learning that extends training distributions via data augmentation to obstruct the uniformity of contrastive representations. Moreover, we argue that classifiers inspired by the statistical perspective in generative or discriminative models are more effective than existing approaches, such as an average of normality scores from a surrogate classifier. In experiments, we demonstrate state-of-the-art performance on visual domain one-class classification benchmarks. Finally, we present visual explanations, confirming that the decision-making process of our deep one-class classifier is intuitive to humans. The code is available at: https://github.com/google-research/google-research/ tree/master/deep_representation_one_class. * Equal contribution. den Oord. Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905arXiv: .09272, 2019 Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018. 2, 3, 8, 16Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve model robustness and uncertainty.", "authors": [], "concepts": ["state-of-the-art", "learn", "demonstrate", "average", "arxiv:1905arxiv:", "representations,", "data,", "particular,", "obstruct", "model", "normality", "available", "one-class", "inspired", "classification.", "such", "existing", "only", "hendrycks,", "kadavath,", "classification", "more", "performance", "arxiv", "from", "present", "deep", "thomas", "using", "generative", "contrastive", "classifier", "predictive", "representations.", "tree/master/deep_representation_one_class.", "2018.", "explanations,", "framework", "arxiv:1812.04606,", "models", "scores", "coding.", "exposure.", "process", "extends", "domain", "finally,", "better", "oord.", "intuitive", "statistical", "learning", "improve", "humans.", "target", "discriminative", "approaches,", "perspective", "https://github.com/google-research/google-research/", "argue", "augmentation", "two-stage", "confirming", "novel", "dietterich.", "code", "self-supervised", "permits", "image", "dawn", "data", "training", "song.", "with", "uniformity", "evaluating", "uncertainty.", "detection", ".09272,", "task.", "that", "16dan", "preprint", "robustness", "learned", "distributions", "effective", "contribution.", "faithful", "representations", "data-efficient", "build", "first", "decision-making", "allows", "experiments,", "building", "moreover,", "distribution-augmented", "also", "mantas", "mazeika,", "then", "recognition", "saurav", "than", "anomaly", "classifiers", "benchmarks.", "visual", "outlier", "surrogate", "classifier.", "equal"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "227162606": {"id": "227162606", "openalex": null, "doi": null, "title": "GENERALIZED VARIATIONAL CONTINUAL LEARNING", "abstract": "Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.", "authors": [], "concepts": ["interpolation", "modification", "term", "learning,", "variational", "tasks", "ewc)", "used", "addition", "show", "observed", "strongly", "settings", "larger", "existing", "been", "layers", "general", "datasets", "probabilistic", "performance", "from", "order", "consolidation", "leads", "gains,", "inference,", "mitigate", "specifically", "regime,", "models", "being", "networks", "limiting", "significant", "likelihood-tempering.", "gvcl", "whilst", "(vcl).", "terms", "better", "generalized", "improved", "learning", "(online", "online", "inspiration", "significantly", "methods.", "(gvcl).", "other", "calibration.", "overpruning", "competitive", "elastic", "providing", "regularization", "deals", "accuracy,", "training", "neural", "with", "allowing", "strand", "common", "outperforms", "that", "case,", "small-data", "recovers", "between", "weight", "take", "layers,", "approaches", "multi-task", "empirically", "datasets,", "baselines", "research", "find", "fashion.", "also", "task-specific", "this", "which", "algorithm", "employs", "architecture,", "baselines.", "main", "approaches.", "film", "effect", "applying", "vein", "continual"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "228063930": {"id": "228063930", "openalex": null, "doi": null, "title": "YOU ONLY NEED ADVERSARIAL SUPERVISION FOR SEMANTIC IMAGE SYNTHESIS", "abstract": "Despite their recent successes, GAN models for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Historically, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limiting the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially-and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity with better alignment to their input label maps, making the use of the perceptual loss superfluous. Moreover, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image change. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve an average improvement of 6 FID and 5 mIoU points over the state of the art across different datasets using only adversarial supervision. Semantic SPADE (Park et al., 2019) Our model (OASIS), sampled with different noise label map with VGG w/o VGG w/o VGG", "authors": [], "concepts": ["helped", "high-quality", "average", "semantically-aware", "stronger", "model", "show", "recent", "complete", "spatially-and", "successes,", "only", "feedback,", "multi-modal", "despite", "discriminator", "datasets", "through", "able", "more", "sampling", "images", "from", "into", "spade", "segmentation", "follow", "using", "synthesis", "well", "quality,", "diverse", "issue,", "tensor", "global", "propose", "2019)", "models", "superfluous.", "limiting", "enable", "employing", "better", "texture", "significantly", "local", "overcome", "high", "vgg-based", "change.", "poor", "model,", "when", "providing", "label", "synthesize", "image", "fidelity", "points", "input", "achieve", "re-design", "making", "across", "with", "novel,", "given", "closely.", "ground", "trained", "miou", "work,", "maps", "improvement", "results.", "that", "training.", "supervision.", "supervision", "real", "their", "noise", "semantic", "truth", "different", "higher", "needs", "color", "suffer", "synthesis.", "network,", "distributions", "need", "additionally", "partial", "allows", "(park", "injected", "historically,", "progress", "moreover,", "al.,", "same", "(oasis),", "quality", "over", "this", "which", "state", "still", "improving", "adversarial", "loss", "time", "generator", "alignment", "directly", "generator,", "perceptual", "sampled", "simplified", "synthesized", "maps,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "228376209": {"id": "228376209", "openalex": null, "doi": null, "title": "CONVEX POTENTIAL FLOWS: UNIVERSAL PROBABILITY DISTRIBUTIONS WITH OPTIMAL TRANSPORT AND CONVEX OPTIMIZATION", "abstract": "Flow-based models are powerful tools for designing probabilistic models with tractable density. This paper introduces Convex Potential Flows (CP-Flow), a natural and efficient parameterization of invertible models inspired by the optimal transport (OT) theory. CP-Flows are the gradient map of a strongly convex neural potential function. The convexity implies invertibility and allows us to resort to convex optimization to solve the convex conjugate for efficient inversion. To enable maximum likelihood training, we derive a new gradient estimator of the log-determinant of the Jacobian, which involves solving an inverse-Hessian vector product using the conjugate gradient method. The gradient estimator has constantmemory cost, and can be made effectively unbiased by reducing the error tolerance level of the convex optimization routine. Theoretically, we prove that CP-Flows are universal density approximators and are optimal in the OT sense. Our empirical results show that CP-Flow performs competitively on standard benchmarks of density estimation and variational inference. arXiv:2012.05942v1 [cs.LG] 10 Dec 2020 for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018. . Ot-flow: Fast and accurate continuous normalizing flows via optimal transport. arXiv preprint arXiv:2006.00104, 2020. George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. Ludger R\u00fcschendorf and Svetlozar T Rachev. A characterization of random variables with minimum l2-distance. Journal of multivariate analysis, 32(1):48-54, 1990. Filippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58-63):94, 2015.", "authors": [], "concepts": ["r\u00fcschendorf", "competitively", "log-determinant", "implies", "flow-based", "55(58-63):94,", "transport.", "accurate", "variational", "error", "performs", "george", "inverse-hessian", "effectively", "continuous", "flows", "show", "vector", "strongly", "(cp-flow),", "inspired", "estimator", "networks.", "efficient", "parameterization", "normalizing", "characterization", "murray.", "fast", "empirical", "probabilistic", "invertibility", "arxiv", "convexity", "ludger", "using", "theo", "generative", "derive", "inference.", "birk\u00e4user,", "applied", "2018.", "inversion.", "optimal", "constantmemory", "models", "pavlakou,", "convex", "rachev.", "benchmarks", "cp-flows", "solve", "enable", "random", "unbiased", "made", "1990.", "arxiv:1802.05957,", "32(1):48-54,", "probability", "papamakarios,", "analysis,", "designing", "gradient", "solving", "theoretically,", "prove", "estimation", "conjugate", "invertible", "svetlozar", "multivariate", "powerful", "potential", "introduces", "method.", "cp-flow", "neural", "masked", "with", "flows:", "standard", "cost,", "training,", "santambrogio.", "that", "estimation.", "iain", "preprint", "function.", "tools", "arxiv:2006.00104,", "mathematicians.", "routine.", "autoregressive", "maximum", "resort", "tolerance", "distributions", "natural", "reducing", "flow", "paper", "(ot)", "allows", "filippo", "universal", "ot-flow:", "tractable", "results", "[cs.lg]", "2020.", "density.", "this", "which", "product", "transport", "adversarial", "arxiv:2012.05942v1", "density", "jacobian,", "involves", "sense.", "optimization", "likelihood", "variables", "level", "l2-distance.", "minimum", "journal", "theory.", "2015.", "approximators"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "229924317": {"id": "229924317", "openalex": null, "doi": null, "title": "EMERGENT SYMBOLS THROUGH BINDING IN EXTERNAL MEMORY", "abstract": "A key aspect of human intelligence is the ability to infer abstract rules directly from high-dimensional sensory data, and to do so given only a limited amount of training experience. Deep neural network algorithms have proven to be a powerful tool for learning directly from high-dimensional data, but currently lack this capacity for data-efficient induction of abstract rules, leading some to argue that symbol-processing mechanisms will be necessary to account for this capacity. In this work, we take a step toward bridging this gap by introducing the Emergent Symbol Binding Network (ESBN), a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. Across a series of tasks, we show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples, and outperforms a number of other competitive neural network architectures.", "authors": [], "concepts": ["learn", "binding", "emergent", "rules,", "data,", "particular", "variable-binding", "experience.", "esbn", "show", "abstracted", "recurrent", "machinery,", "will", "only", "nearly", "proven", "tool", "number", "intelligence", "through", "from", "deep", "explicitly", "account", "generalization", "abstract", "process", "some", "away", "emerge", "manner", "high-dimensional", "aspect", "displays", "bridging", "learning", "examples,", "without", "ability", "currently", "toward", "architectures.", "memory", "other", "argue", "network", "indirection.", "competitive", "novel", "form", "necessary", "step", "powerful", "series", "training", "mechanisms", "neural", "across", "with", "introducing", "given", "capacity", "work,", "(esbn),", "symbol-processing", "outperforms", "that", "symbols", "symbol-like", "those", "take", "learned", "entities", "augmented", "rules", "need", "representations", "sensory", "leading", "data-efficient", "enabling", "lack", "symbol", "allows", "amount", "algorithms", "external", "infer", "tasks,", "this", "which", "enables", "have", "apply.", "induction", "directly", "incorporate", "perfect", "limited", "architecture", "mechanism", "human", "capacity."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "231740484": {"id": "231740484", "openalex": null, "doi": null, "title": "META-LEARNING WITH NEGATIVE LEARNING RATES", "abstract": "Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or learning to learn a distribution of tasks, where learning is represented by an outer loop, and to learn by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative values. These results help clarify under what circumstances meta-learning performs best.arXiv:2102.00940v1 [cs.LG] 1 Feb 2021Published as a conference paper at ICLR 2021 function of the inner loop learning rate during meta-training. Setting this learning rate to zero is equivalent to removing the inner loop, as advocated by recent work(Chen et al. (2020a), Tian et al. (2020), Dhillon et al. (2020), Chen et al. (2020b, Raghu et al. (2020)). Surprisingly, we find that the optimal learning rate is negative, thus performance can be increased by reducing the learning rate below zero. In particular, we find the following:\u2022 In the problem of mixed linear regression, we prove that the optimal learning rate is always negative in overparameterized models. The same result holds in underparameterized models provided that the optimal learning rate is small in absolute value. We validate the theory by running extensive experiments. \u2022 We extend these results to the case of nonlinear regression and wide neural networks, in which the output of can be approximated by a linear function of the parameters (Jacot et al.(2018), Lee et al.(2019)). While in this case we cannot prove that the optimal learning rate is always negative, preliminary experiments suggest that the result holds in this case as well.", "authors": [], "concepts": ["case", "raghu", "learn", "scarce", "implies", "what", "holds", "parameters", "performs", "studies", "tasks", "particular,", "solutions", "problem", "equally", "approximated", "meta-learning", "provided", "regression,", "rate", "work(chen", "recent", "test", "advocated", "dhillon", "following:\u2022", "al.(2019)).", "(2020a),", "these", "circumstances", "large", "only", "does", "been", "number", "empirical", "best.arxiv:2102.00940v1", "increase", "more", "calculate", "performance", "clarify", "zero,", "before.", "deep", "however,", "matrix", "using", "well", "maml", "while", "surprisingly,", "meta-learning,", "function", "conference", "regression", "applied", "rates", "simple", "optimal", "(2020b,", "models", "there", "wide", "task,", "thus", "meta-training.", "transfer", "networks,", "random", "work", "chen", "linear", "learning", "(jacot", "quickly", "inner", "successful", "gradient", "(2020)).", "equivalent", "mixed", "target", "output", "prove", "absolute", "outer", "(2020),", "experiments", "argue", "below", "positive,", "better.", "when", "negative,", "target.", "loop.", "zero", "data", "training", "negative", "validate", "experiments.", "neural", "gained", "with", "al.(2018),", "descent.", "considered", "exact", "tian", "adaptation", "zero.", "work,", "approach", "that", "distribution", "loop", "unnecessary", "knowledge", "preliminary", "increased", "running", "expression", "extend", "suggested", "result", "always", "reducing", "2021published", "paper", "nonlinear", "amount", "help", "find", "theory", "values.", "perform", "algebraic", "results", "[cs.lg]", "same", "where", "loop,", "overparameterized", "models.", "tasks,", "this", "which", "during", "well.", "loss", "cannot", "removing", "small", "value.", "iclr", "suggest", "represented", "similar", "under", "even", "underparameterized", "study", "require", "models,", "further", "extensive", "never", "setting", "decreasing", "therefore,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "231749906": {"id": "231749906", "openalex": null, "doi": null, "title": "GRAPH COARSENING WITH NEURAL NETWORKS", "abstract": "As large-scale graphs become increasingly more prevalent, it poses significant computational challenges to process, extract and analyze large graph data. Graph coarsening is one popular technique to reduce the size of a graph while maintaining essential properties. Despite rich graph coarsening literature, there is only limited exploration of data-driven methods in the field. In this work, we leverage the recent progress of deep learning on graphs for graph coarsening. We first propose a framework for measuring the quality of coarsening algorithm and show that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. Motivated by the observation that the current choice of edge weight for the coarse graph may be suboptimal, we parametrize the weight assignment map with graph neural networks and train it to improve the coarsening quality in an unsupervised way. Through extensive experiments on both synthetic and real networks, we demonstrate that our method significantly improves common graph coarsening methods under various metrics, reduction ratios, graph sizes, and graph types. It generalizes to graphs of larger size (25\u00d7 of training graphs), is adaptive to different losses (differentiable and non-differentiable), and scales to much larger graphs than previous work.Published as a conference paper at ICLR 2021 graph. This means the weights of the coarse graph is determined by the coarsening algorithm (of the vertex set), leaving no room for adjustment.With the two observations above, we aim to develop a data-driven approach to better assigning weights for the coarse graph depending on specific goals at hand. We will leverage the recent progress of deep learning on graphs to develop a framework to learn to assign edge weights in an unsupervised manner from a collection of input (small) graphs. This learned weight-assignment map can then be applied to new graphs (of potentially much larger sizes). In particular, our contributions are threefold.\u2022 First, depending on the quantity of interest F (such as the quadratic form w.r.t. Laplace operator), one has to carefully choose projection/lift operator to relate quantities defined on graphs of different sizes. We formulate this as the invariance of F under lift map, and provide three cases of projection/lift map as well as the corresponding operators on the coarse graph. Interestingly, those operators all can be seen as the special cases of doubly-weighted Laplace operators on coarse graphs (Horak & Jost, 2013). \u2022 Second, we are the first to propose and develop a framework to learn the edge weights of the coarse graphs via graph neural networks (GNN) in an unsupervised manner. We show convincing results both theoretically and empirically that changing the weights is crucial to improve the quality of coarse graphs. \u2022 Third, through extensive experiments on both synthetic graphs and real networks, we demonstrate that our method GOREN significantly improves common graph coarsening methods under different evaluation metrics, reduction ratios, graph sizes, and graph types. It generalizes to graphs of larger size (than the training graphs), adapts to different losses (so as to preserve different properties of original graphs), and scales to much larger graphs than what previous work can handle. Even for losses that are not differentiable w.r.t the weights of the coarse graph, we show training networks with a differentiable auxiliary loss still improves the result.", "authors": [], "concepts": ["essential", "learn", "data-driven", "demonstrate", "(than", "what", "laplace", "scales", "adjustment.with", "associated", "losses", "particular,", "suboptimal,", "third,", "observation", "means", "invariance", "show", "method", "operator),", "recent", "reduction", "cases", "goal,", "will", "larger", "large", "only", "despite", "field.", "assign", "specific", "seen", "much", "through", "carefully", "adaptive", "more", "from", "leverage", "deep", "coarsening", "threefold.\u2022", "sizes.", "quantity", "well", "interest", "lift", "while", "projection/lift", "conference", "auxiliary", "doubly-weighted", "sizes).", "applied", "jost,", "2013).", "framework", "propose", "increasingly", "large-scale", "graphs.", "there", "properties.", "networks", "coarse", "significant", "current", "data.", "both", "manner", "networks,", "differentiable", "popular", "methods", "graph", "work", "better", "extract", "hand.", "learning", "improve", "measuring", "contributions", "motivated", "significantly", "goals", "first,", "set),", "relate", "experiments", "work.published", "challenges", "potentially", "w.r.t.", "form", "improves", "corresponding", "way.", "exploration", "input", "sizes,", "above,", "second,", "training", "theoretically", "neural", "non-differentiable),", "operators", "with", "metrics,", "train", "assigning", "choose", "ratios,", "map,", "common", "(such", "leaving", "work,", "maintaining", "approach", "that", "generalizes", "determined", "coarsening.", "(25\u00d7", "real", "weights", "w.r.t", "analyze", "previous", "different", "weight", "those", "operator", "convincing", "interestingly,", "learned", "rich", "graphs", "quadratic", "choice", "need", "technique", "parametrize", "reduce", "(differentiable", "empirically", "literature,", "first", "size", "paper", "special", "provide", "vertex", "(gnn)", "preserve", "unsupervised", "three", "computational", "weight-assignment", "progress", "observations", "depending", "changing", "results", "(small)", "synthetic", "(horak", "various", "then", "collection", "formulate", "evaluation", "prevalent,", "operators.", "than", "quality", "this", "still", "adapts", "algorithm", "handle.", "loss", "graph.", "defined", "develop", "process,", "result.", "graphs),", "iclr", "types.", "graph,", "original", "limited", "assignment", "under", "goren", "even", "poses", "crucial", "quantities", "room", "properties", "manner.", "extensive", "become", "edge"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "231847109": {"id": "231847109", "openalex": null, "doi": null, "title": "OPEN-WORLD SEMI-SUPERVISED LEARNING", "abstract": "A fundamental limitation of applying semi-supervised learning in real-world settings is the assumption that unlabeled test data contains only classes previously encountered in the labeled training data. However, this assumption rarely holds for data in-the-wild, where instances belonging to novel classes may appear at testing time. Here, we introduce a novel open-world semi-supervised learning setting that formalizes the notion that novel classes may appear in the unlabeled test data. In this novel setting, the goal is to solve the class distribution mismatch between labeled and unlabeled data, where at the test time every input instance either needs to be classified into one of the existing classes or a new unseen class needs to be initialized. To tackle this challenging problem, we propose ORCA, an end-to-end deep learning approach that introduces uncertainty adaptive margin mechanism to circumvent the bias towards seen classes caused by learning discriminative features for seen classes faster than for the novel classes. In this way, ORCA reduces the gap between intra-class variance of seen with respect to novel classes. Experiments on image classification datasets and a single-cell annotation dataset demonstrate that ORCA consistently outperforms alternative baselines, achieving 25% improvement on seen and 96% improvement on novel classes of the ImageNet dataset. * The two first authors made equal contributions.Published as a conference paper at ICLR 2022 Stuart Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2): [129][130][131][132][133][134][135][136][137] 1982.David J Miller and John Browning. A mixture model and EM-based algorithm for class discovery, robust classification, and outlier rejection in mixed labeled/unlabeled data sets.", "authors": [], "concepts": ["way,", "challenging", "demonstrate", "achieving", "unseen", "ieee", "holds", "lloyd.", "margin", "uncertainty", "here,", "data,", "robust", "tackle", "model", "dataset", "settings", "classified", "test", "open-world", "information", "either", "baselines,", "faster", "existing", "only", "setting,", "belonging", "seen", "rejection", "datasets", "classification", "adaptive", "into", "deep", "however,", "browning.", "contributions.published", "instances", "conference", "towards", "fundamental", "propose", "discovery,", "labeled", "initialized.", "reduces", "data.", "solve", "in-the-wild,", "features", "made", "[129][130][131][132][133][134][135][136][137]", "problem,", "miller", "learning", "goal", "encountered", "mixed", "formalizes", "end-to-end", "discriminative", "pcm.", "classes", "previously", "experiments", "orca", "classes.", "quantization", "novel", "orca,", "class", "real-world", "image", "data", "input", "training", "introduces", "semi-supervised", "em-based", "with", "least", "variance", "bias", "28(2):", "john", "improvement", "approach", "1982.david", "outperforms", "that", "distribution", "sets.", "between", "single-cell", "stuart", "imagenet", "squares", "alternative", "testing", "assumption", "appear", "needs", "unlabeled", "contains", "transactions", "first", "paper", "labeled/unlabeled", "time.", "rarely", "introduce", "where", "dataset.", "instance", "annotation", "than", "consistently", "this", "respect", "circumvent", "notion", "algorithm", "mismatch", "time", "mixture", "authors", "iclr", "limitation", "theory,", "intra-class", "classification,", "caused", "outlier", "mechanism", "applying", "every", "equal", "setting"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "231918454": {"id": "231918454", "openalex": null, "doi": null, "title": "BAYESIAN NEURAL NETWORK PRIORS REVISITED", "abstract": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets. * Equal contribution. \u2020 Equal contribution.", "authors": [], "concepts": ["unclear", "fully", "resnets.", "summary", "show", "lead", "spatial", "these", "about", "(fcnns)", "classification", "performance.", "increase", "performance", "into", "cold", "however,", "(sgd).", "using", "convolutional", "while", "surprisingly,", "inference.", "mitigate", "optimal", "isotropic", "networks", "better", "datasets.", "improved", "(cnn)", "bayesian", "gradient", "revisited", "gaussian", "resnet", "priors", "network", "fcnns,", "image", "display", "reflect", "neural", "correlations,", "standard", "facto", "trained", "whether", "that", "heavy-tailed", "weights", "strong", "weight", "connected", "distributions", "contribution.", "distributions.", "statistics", "building", "find", "observations", "accurately", "true", "slightly", "beliefs", "give", "posterior", "effect", "study", "variety", "priors,", "modern", "equal", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "231918471": {"id": "231918471", "openalex": null, "doi": null, "title": "SCALABLE BAYESIAN INVERSE REINFORCEMENT LEARNING", "abstract": "Bayesian inference over the reward presents an ideal solution to the ill-posed nature of the inverse reinforcement learning problem. Unfortunately current methods generally do not scale well beyond the small tabular setting due to the need for an inner-loop MDP solver, and even non-Bayesian methods that do themselves scale often require extensive interaction with the environment to perform well, being inappropriate for high stakes or costly applications such as healthcare. In this paper we introduce our method, Approximate Variational Reward Imitation Learning (AVRIL), that addresses both of these issues by jointly learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward. Applying our method to real medical data alongside classic control simulations, we demonstrate Bayesian reward inference in environments beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms.", "authors": [], "concepts": ["environments", "inference", "demonstrate", "variational", "scales", "approximate", "(avril),", "method", "algorithms.", "addresses", "methods,", "such", "these", "scale", "generally", "well,", "through", "performance", "problem.", "ill-posed", "alongside", "well", "said", "task", "nature", "reinforcement", "method,", "being", "current", "both", "manner", "methods", "learning", "bayesian", "applications", "interaction", "focused", "completely", "latent", "offline", "healthcare.", "high", "environment", "non-bayesian", "competitive", "control", "policy", "inappropriate", "complicated", "data", "ideal", "classic", "with", "appropriate", "reward", "jointly", "spaces", "beyond", "approach", "that", "distribution", "real", "scope", "themselves", "unfortunately", "solution", "scalable", "presents", "need", "stakes", "inner-loop", "paper", "arbitrarily", "inverse", "perform", "imitation", "introduce", "simulations,", "over", "reward.", "this", "state", "issues", "tabular", "medical", "costly", "small", "often", "posterior", "even", "require", "solver,", "applying", "extensive", "setting"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "231934149": {"id": "231934149", "openalex": null, "doi": null, "title": "TOPOLOGICAL GRAPH NEURAL NETWORKS", "abstract": "Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler-Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.", "authors": [], "concepts": ["integrated", "strictly", "humans", "classified", "isomorphism", "information", "such", "gnns", "topology", "homology.", "been", "shown", "classification", "more", "performance", "into", "present", "substructures", "leads", "using", "gnns,", "predictive", "global", "networks", "data.", "both", "graph", "message-passing", "terms", "improved", "expressive", "learning", "topological", "oblivious", "weisfeiler-lehman", "gnns.", "novel", "real-world", "powerful", "ordinary", "data", "neural", "with", "eminent", "cycles.", "that", "sets,", "their", "easily", "togl,", "persistent", "augmenting", "togl", "synthetic", "layer", "than", "type", "tasks,", "which", "node", "tackling", "incorporates", "have", "architecture", "test)", "(gnns)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "232046055": {"id": "232046055", "openalex": null, "doi": null, "title": "TASK-AGNOSTIC MORPHOLOGY EVOLUTION", "abstract": "Deep reinforcement learning primarily focuses on learning behavior, usually overlooking the fact that an agent's function is largely determined by form. So, how should one go about finding a morphology fit for solving tasks in a given environment? Current approaches that co-adapt morphology and behavior use a specific task's reward as a signal for morphology optimization. However, this often requires expensive policy optimization and results in task-dependent morphologies that are not built to generalize. In this work, we propose a new approach, Task-Agnostic Morphology Evolution (TAME), to alleviate both of these issues. Without any task or reward specification, TAME evolves morphologies by only applying randomly sampled action primitives on a population of agents. This is accomplished using an information-theoretic objective that efficiently ranks agents by their ability to reach diverse states in the environment and the causality of their actions. Finally, we empirically demonstrate that across 2D, 3D, and manipulation environments TAME can evolve morphologies that match the multi-task performance of those learned with task supervised algorithms. Our code and videos can be found at . Quality and diversity in evolutionary modular robotics. arXiv preprint arXiv:2008.02116, 2020. 2 Tonnes F Nygaard, David Howard, and Kyrre Glette. Real world morphological evolution is feasible. arXiv preprint arXiv:2005.09288, 2020. 2Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:", "authors": [], "concepts": ["tonnes", "environments", "tame", "demonstrate", "arxiv:2008.02116,", "yazhe", "focuses", "action", "primitives", "tasks", "expensive", "approach,", "2aaron", "reach", "evolves", "behavior,", "algorithms.", "kyrre", "videos", "these", "agents.", "about", "only", "task's", "specific", "task-dependent", "oriol", "performance", "arxiv", "deep", "however,", "using", "co-adapt", "contrastive", "population", "function", "diverse", "predictive", "should", "evolution", "built", "evolve", "environment?", "task", "agents", "requires", "propose", "reinforcement", "robotics.", "coding.", "found", "(tame),", "current", "both", "form.", "arxiv:2005.09288,", "finally,", "issues.", "learning", "fact", "usually", "actions.", "signal", "solving", "without", "ability", "causality", "nygaard,", "environment", "david", "code", "policy", "generalize.", "world", "across", "with", "behavior", "given", "reward", "states", "morphology", "work,", "agent's", "that", "optimization.", "primarily", "evolutionary", "preprint", "determined", "randomly", "finding", "real", "their", "alleviate", "those", "learned", "accomplished", "approaches", "multi-task", "efficiently", "empirically", "feasible.", "objective", "morphologies", "match", "morphological", "information-theoretic", "results", "oord,", "vinyals.", "overlooking", "2020.", "howard,", "quality", "this", "task-agnostic", "diversity", "largely", "often", "optimization", "glette.", "ranks", "manipulation", "representation", "sampled", "supervised", "arxiv:", "applying", "specification,", "modular"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "232257793": {"id": "232257793", "openalex": null, "doi": null, "title": "HYPERDYNAMICS: META-LEARNING OBJECT AND AGENT DYNAMICS WITH HYPERNETWORKS", "abstract": "We propose HyperDynamics, a dynamics meta-learning framework that conditions on an agent's interactions with the environment and optionally its visual observations, and generates the parameters of neural dynamics models based on inferred properties of the dynamical system. Physical and visual properties of the environment that are not part of the low-dimensional state yet affect its temporal dynamics are inferred from the interaction history and visual observations, and are implicitly captured in the generated parameters. We test HyperDynamics on a set of object pushing and locomotion tasks. It outperforms existing dynamics models in the literature that adapt to environment variations by learning dynamics over high dimensional visual observations, capturing the interactions of the agent in recurrent state representations, or using gradient-based meta-optimization. We also show our method matches the performance of an ensemble of separately trained experts, while also being able to generalize well to unseen environment variations at test time. We attribute its good performance to the multiplicative interactions between the inferred system properties-captured in the generated parametersand the low-dimensional state representation of the dynamical system.", "authors": [], "concepts": ["unseen", "parameters", "representations,", "meta-learning", "object", "show", "method", "literature", "pushing", "test", "recurrent", "existing", "able", "good", "performance", "from", "hyperdynamics:", "interactions", "part", "using", "parametersand", "well", "optionally", "while", "locomotion", "physical", "framework", "propose", "models", "matches", "being", "system", "affect", "conditions", "meta-optimization.", "properties-captured", "attribute", "learning", "history", "low-dimensional", "interaction", "hypernetworks", "high", "environment", "capturing", "parameters.", "multiplicative", "based", "dimensional", "hyperdynamics", "separately", "generated", "experts,", "tasks.", "neural", "with", "trained", "variations", "agent's", "outperforms", "that", "ensemble", "between", "captured", "inferred", "dynamical", "observations,", "hyperdynamics,", "generalize", "gradient-based", "time.", "temporal", "also", "dynamics", "over", "state", "system.", "adapt", "implicitly", "agent", "visual", "representation", "properties", "generates"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "232307359": {"id": "232307359", "openalex": null, "doi": null, "title": "LANGUAGE-AGNOSTIC REPRESENTATION LEARNING OF SOURCE CODE FROM STRUCTURE AND CONTEXT", "abstract": "Source code (Context) and its parsed abstract syntax tree (AST; Structure) are two complementary representations of the same computer program. Traditionally, designers of machine learning models have relied predominantly either on Structure or Context. We propose a new model, which jointly learns on Context and Structure of source code. In contrast to previous approaches, our model uses only language-agnostic features, i.e., source code and features that can be computed directly from the AST. Besides obtaining state-of-the-art on monolingual code summarization on all five programming languages considered in this work, we propose the first multilingual code summarization model. We show that jointly training on non-parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low-resource languages. Remarkably, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code.", "authors": [], "concepts": ["state-of-the-art", "computer", "machine", "non-parallel", "low-resource", "designers", "model", "show", "lead", "code.", "(ast;", "predominantly", "either", "besides", "features,", "only", "does", "multiple", "contrast", "from", "languages", "structure)", "traditionally,", "improvements,", "structure", "remarkably,", "tree", "monolingual", "propose", "models", "abstract", "syntax", "features", "highlighting", "learning", "i.e.,", "approaches,", "source", "strongest", "uses", "model,", "code", "improves", "combining", "individual", "data", "multilingual", "training", "context", "learns", "obtaining", "jointly", "considered", "work,", "that", "programming", "complementary", "previous", "model.", "ast.", "representations", "first", "five", "results", "same", "where", "parsed", "this", "which", "benefits", "language-agnostic", "have", "directly", "languages,", "program.", "representation", "relied", "context.", "gains", "computed", "languages.", "(context)", "summarization"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "234358843": {"id": "234358843", "openalex": null, "doi": null, "title": "LAYER-ADAPTIVE SPARSITY FOR THE MAGNITUDE-BASED PRUNING", "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on \"how to choose,\" the layerwise sparsities are mostly selected algorithm-byalgorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level 2 distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation. Under various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection. Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity Recent discoveries (Gale et al., 2019; Evci et al., 2020) demonstrate that, given an appropriate choice of layerwise sparsity, simply pruning on the basis of weight magnitude yields a surprisingly powerful unstructured pruning scheme. For instance, Gale et al. (2019) evaluates the performance of magnitudebased pruning (MP; Han et al. (2015); Zhu & Gupta(2018)) with an extensive hyperparameter tuning, and shows that MP achieves comparable or better performance than state-of-the-art pruning algorithms that use more complicated importance scores. To arrive at such a performance level, the authors introduce the following handcrafted heuristic: Leave the first convolutional layer fully dense, and prune up to only 80% of weights from the last fully-connected layer; the heuristic is motivated by the sparsity pattern from other state-of-the-art algorithms(Molchanov et al., 2017)and additional experimental/architectural observations. Unfortunately, there is an apparent lack of consensus on \"how to choose the layerwise sparsity\" for the magnitude-based pruning. Instead, the layerwise sparsity is selected mostly on an algorithm-byalgorithm basis. One common method is the global MP criteria (see, e.g., Morcos et al.(2019)), * Work done at KAIST 1 i.e., simultaneously training and pruning arXiv:2010.07611v2 [cs.LG]", "authors": [], "concepts": ["state-of-the-art", "scores.", "unstructured", "(gale", "demonstrate", "gupta(2018))", "fully", "pruning,", "choose,\"", "shows", "outperform", "dense,", "comparable", "performs", "setups,", "heavy", "selection.", "heuristics", "sparsity\"", "version", "heuristic:", "method", "surprisingly", "recent", "kaist", "score", "such", "existing", "only", "(2019)", "does", "fully-connected", "morcos", "2020)", "magnitude-based", "classification", "basis", "performance.", "carefully", "more", "performance", "from", "weight-rewinding", "clear", "however,", "(the", "convolutional", "magnitude", "schemes", "case.", "while", "baseline", "apparent", "consensus", "e.g.,", "(mp;", "simple", "distortion", "global", "propose", "sparsities", "tradeoff", "there", "unfortunately,", "achieves", "experimental/architectural", "furthermore,", "discoveries", "selected", "popular", "work", "better", "arrive", "that,", "layer-adaptive", "handcrafted", "additional", "motivated", "simultaneously", "i.e.,", "without", "2017)and", "reveal", "done", "continues", "other", "strongest", "network", "layerwise", "heuristic", "importance", "novel", "algorithm-byalgorithm", "pattern", "overall)", "incurred", "https://github.com/jaeho-lee/layer-adaptive-sparsity", "image", "powerful", "(see,", "observe", "complicated", "coined", "training", "criteria", "chosen", "arxiv:2010.07611v2", "neural", "hyperparameter", "with", "appropriate", "algorithms(molchanov", "last", "given", "2019;", "score;", "choose", "tuning", "common", "outperforms", "that", "between", "lamp", "weights", "fill", "level,", "weight", "tuning,", "layer;", "model-level", "code:", "rescaled", "sparsity", "choice", "basis.", "evci", "simply", "instance,", "yields", "first", "(lamp)", "baselines", "lack", "resorting", "\"how", "observations.", "search.", "algorithms", "following", "computation.", "[cs.lg]", "al.,", "leave", "layer", "al.(2019)),", "introduce", "various", "than", "mostly", "pruning", "consistently", "this", "scheme.", "gap,", "magnitudebased", "connectivity-oriented", "instead,", "incorporates", "algorithm-byalgorithm,", "gale", "authors", "(2015);", "often", "prune", "under", "worse", "even", "sparsity,", "require", "extensive", "evaluates", "pruning."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "235293845": {"id": "235293845", "openalex": null, "doi": null, "title": "DIALOGRAPH: INCORPORATING INTERPRETABLE STRATEGY-GRAPH NETWORKS INTO NEGOTIATION DIALOGUES", "abstract": "To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DIALOGRAPH, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DIALOGRAPH explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues. 1", "authors": [], "concepts": ["state-of-the-art", "sentences,", "dialogues.", "fluent", "show", "downstream", "method", "associations", "they", "networks.", "sequences", "dependencies", "prior", "interpretable", "generation.", "into", "present", "deal,", "strategies", "using", "explicitly", "explicit", "while", "accuracy", "reason", "agents", "optimal", "models", "successfully", "networks", "system", "enable", "both", "dialograph", "negotiation", "graph", "incorporating", "generating", "improved", "response", "persuasive", "strategy/dialogue", "dialogue", "dialogue,", "providing", "strategically.", "neural", "given", "dialogues", "strategies,", "strategy-graph", "communicate", "outperforms", "that", "between", "graph-based", "grounding", "prediction", "negotiate", "dialograph,", "essential.", "learned", "effective", "dialograph:", "course", "leading", "next", "lack", "pragmatic", "strategy-graphs", "quality", "over", "enough", "still", "fluently:", "benefits", "cannot", "incorporates", "strategic", "qualitatively", "planning", "context.", "excel", "further", "modern"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "235417023": {"id": "235417023", "openalex": null, "doi": null, "title": "Adversarial Robustness through the Lens of Causality", "abstract": "The adversarial vulnerability of deep neural networks has attracted significant attention in machine learning. From a causal viewpoint, adversarial attacks can be considered as a specific type of distribution change on natural data. As causal reasoning has an instinct for modeling distribution change, we propose to incorporate causality into mitigating adversarial vulnerability. However, causal formulations of the intuition of adversarial attack and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of adversarial attacks. From a causal perspective, we find that the label is spuriously correlated with the style (content-independent) information when an instance is given. The spurious correlation implies that the adversarial distribution is constructed via making the statistical conditional association between style information and labels drastically different from that in natural distribution. Thus, DNNs that fit the spurious correlation are vulnerable to the adversarial distribution. Inspired by the observation, we propose the adversarial distribution alignment method to eliminate the difference between the natural distribution and the adversarial distribution. Extensive experiments demonstrate the efficacy of the proposed method. Our method can be seen as the first attempt to leverage causality for mitigating adversarial vulnerability. * Work done during an internship at Hong Kong Baptist University.", "authors": [], "concepts": ["demonstrate", "machine", "implies", "vulnerable", "robust", "spurious", "change,", "model", "lens", "method", "inspired", "information", "modeling", "difference", "attracted", "observation,", "examples", "specific", "seen", "through", "literature.", "from", "leverage", "into", "deep", "however,", "viewpoint,", "generation", "correlated", "propose", "lacking", "networks", "conditional", "learning.", "significant", "process", "data.", "formulations", "graph", "work", "statistical", "thus,", "dnns", "university.", "done", "causality", "experiments", "intuition", "constructed", "when", "define", "label", "perspective,", "vulnerability.", "method.", "neural", "bridge", "making", "reasoning", "with", "considered", "hong", "mitigating", "that", "distribution", "internship", "between", "(content-independent)", "different", "robustness", "formalize", "natural", "eliminate", "attention", "first", "causal", "vulnerability", "labels", "change", "kong", "find", "given.", "spuriously", "style", "instance", "type", "this", "still", "during", "gap,", "efficacy", "adversarial", "attack", "alignment", "development", "instinct", "incorporate", "proposed", "attacks", "correlation", "drastically", "attacks.", "association", "extensive", "construct", "baptist", "attempt", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "235795764": {"id": "235795764", "openalex": null, "doi": null, "title": "SOURCE-FREE ADAPTATION TO MEASUREMENT SHIFT VIA BOTTOM-UP FEATURE RESTORATION", "abstract": "Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and flexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottomup training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain.Recently, there has been increasing interest in methods to address this setting of source-free domain adaptation (SFDA, Kundu et al. 2020; Liang et al. 2020; Li et al. 2020; Morerio et al. 2020)  where the source dataset is unavailable during adaptation in the deployment phase. However, to adapt to the target domain, most of these methods employ entropy-minimization techniques which: (i) apply only to classification (discrete labels); (ii) destroy model calibration-minimizing prediction-entropy causes every sample to be classified (correctly or incorrectly) with extreme confidence; and (iii) assume that, in the target domain, the feature space of the unadapted source model contains reasonably well-separated data clusters, where samples within a cluster tend to share the same class label. As . Gradient-based learning applied to document recognition. unifying view on dataset shift in classification. Pattern Recognition, 45:521-530, 2012.Pietro Morerio, Riccardo Volpi, Ruggero Ragonesi, and Vittorio Murino. Generative pseudo-label refinement for unsupervised domain adaptation. In Tangent prop-a formalism for specifying selected invariances in an adaptive network. In Advances in Neural Information Processing Systems, pp. 895-903, 1991.Serban Stan and Mohammad Rostami. Unsupervised model adaptation for continual semantic segmentation. In Towards robust CNN-based object detection through augmentation with synthetic rain variations.", "authors": [], "concepts": ["morerio", "aims", "demonstrate", "rostami.", "achieving", "(sfda,", "bottomup", "access", "source-free", "(bufr).", "riccardo", "data,", "kundu", "most", "(discrete", "robust", "particular,", "approximate", "resolved", "recognition,", "document", "model", "object", "domain.", "prop-a", "dataset", "techniques", "within", "extreme", "classified", "labels);", "classification.", "information", "domain.recently,", "saved", "view", "such", "these", "murino.", "existing", "only", "feature", "been", "(iii)", "calibration-minimizing", "(ii)", "2020)", "through", "classification", "adaptive", "good", "performance", "cnn-based", "leverage", "however,", "lightweight", "formalism", "895-903,", "flexible", "interest", "segmentation.", "generative", "while", "feature-extractor", "calibration;", "share", "class-separation", "towards", "wherein", "applied", "propose", "there", "tend", "being", "advances", "rather", "ruggero", "domain", "bufr", "tangent", "unifying", "1991.serban", "selected", "methods", "causes", "features", "terms", "scheme", "stan", "domain,", "vittorio", "restoring", "restoration", "learning", "that,", "well-separated", "target", "without", "performance,", "variations.", "ragonesi,", "realigns", "rely", "sfda", "clusters,", "reasonably", "source", "(correctly", "ones.", "which:", "pervasive", "space", "(sfda)", "measurement", "augmentation", "refinement", "pattern", "class", "pseudo-label", "volpi,", "data", "accuracy,", "training", "employ", "classification;", "neural", "calibration,", "source-domain", "destroy", "with", "systems,", "confidence;", "network.", "less", "source.", "trained", "call", "incorrectly)", "detection", "adaptation", "reliant", "entropy-minimization", "shift", "outperforms", "cluster", "that", "distribution", "boosts", "45:521-530,", "real", "semantic", "extracting", "phase.", "unlabelled", "labelled", "unavailable", "2020;", "specifying", "assume", "processing", "approximation", "contains", "adaptation.", "2012.pietro", "efficiency,", "additionally", "called", "apply", "morerio,", "label.", "address", "gradient-based", "unsupervised", "increasing", "synthetic", "feature-space", "same", "where", "particularly", "(fr)", "sample", "data;", "than", "type", "this", "which", "samples", "during", "adapt", "issues", "mohammad", "bottom-up", "recognition.", "liang", "under", "unadapted", "level", "deployment", "invariances", "rain", "every", "prediction-entropy", "setting", "continual", "store"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "235828929": {"id": "235828929", "openalex": null, "doi": null, "title": "HIDDEN CONVEXITY OF WASSERSTEIN GANS: INTERPRETABLE GENERATIVE MODELS WITH CLOSED-FORM SOLUTIONS", "abstract": "Generative Adversarial Networks (GANs) are commonly used for modeling complex distributions of data. Both the generators and discriminators of GANs are often modeled by neural networks, posing a non-transparent optimization problem which is non-convex and non-concave over the generator and discriminator, respectively. Such networks are often heuristically optimized with gradient descent-ascent (GDA), but it is unclear whether the optimization problem contains any saddle points, or whether heuristic methods can find them in practice. In this work, we analyze the training of Wasserstein GANs with two-layer neural network discriminators through the lens of convex duality, and for a variety of generators expose the conditions under which Wasserstein GANs can be solved exactly with convex optimization approaches, or can be represented as convex-concave games. Using this convex duality interpretation, we further demonstrate the impact of different activation functions of the discriminator. Our observations are verified with numerical results demonstrating the power of the convex interpretation, with applications in progressive training of convex architectures corresponding to linear generators and quadratic-activation discriminators for CelebA image generation. The code for our experiments is available at", "authors": [], "concepts": ["wasserstein", "unclear", "duality", "solved", "demonstrate", "discriminator.", "solutions", "problem", "discriminator,", "non-convex", "used", "lens", "them", "available", "gans:", "modeling", "demonstrating", "such", "generators", "duality,", "through", "interpretable", "generation.", "convexity", "verified", "using", "exactly", "points,", "generative", "heuristically", "saddle", "two-layer", "models", "convex", "networks", "data.", "both", "networks,", "conditions", "celeba", "methods", "respectively.", "convex-concave", "interpretation,", "linear", "gradient", "activation", "applications", "approaches,", "experiments", "network", "heuristic", "numerical", "practice.", "code", "image", "corresponding", "training", "neural", "with", "progressive", "gans", "modeled", "whether", "work,", "non-concave", "functions", "closed-form", "non-transparent", "posing", "analyze", "discriminators", "optimized", "different", "impact", "distributions", "contains", "(gda),", "architectures", "find", "hidden", "observations", "results", "quadratic-activation", "commonly", "descent-ascent", "over", "power", "this", "which", "adversarial", "generator", "often", "games.", "(gans)", "optimization", "represented", "under", "expose", "variety", "further", "complex"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "236087352": {"id": "236087352", "openalex": null, "doi": null, "title": "HIERARCHICAL FEW-SHOT IMITATION WITH SKILL TRANSITION MODELS", "abstract": "A desirable property of autonomous agents is the ability to both solve long-horizon problems and generalize to unseen tasks. Recent advances in data-driven skill learning have shown that extracting behavioral priors from offline data can enable agents to solve challenging long-horizon tasks with reinforcement learning. However, generalization to tasks unseen during behavioral prior training remains an outstanding challenge. To this end, we present Few-shot Imitation with Skill Transition Models (FIST), an algorithm that extracts skills from offline data and utilizes them to generalize to unseen tasks given a few downstream demonstrations. FIST learns an inverse skill dynamics model, a distance function, and utilizes a semi-parametric approach for imitation. We show that FIST is capable of generalizing to new tasks and substantially outperforms prior baselines in navigation experiments requiring traversing unseen parts of a large maze and 7-DoF robotic arm experiments requiring manipulating previously unseen objects in a kitchen. arXiv:2107.08981v2 [cs.LG]", "authors": [], "concepts": ["outstanding", "challenging", "data-driven", "unseen", "arxiv:2107.08981v2", "tasks", "them", "show", "downstream", "recent", "large", "prior", "requiring", "shown", "from", "present", "traversing", "however,", "behavioral", "desirable", "agents", "objects", "kitchen.", "autonomous", "demonstrations.", "reinforcement", "models", "generalization", "learning.", "advances", "generalizing", "solve", "enable", "both", "maze", "capable", "hierarchical", "learning", "semi-parametric", "problems", "substantially", "ability", "parts", "offline", "priors", "previously", "experiments", "imitation.", "distance", "challenge.", "robotic", "model,", "fist", "transition", "data", "7-dof", "training", "tasks.", "learns", "with", "few-shot", "given", "navigation", "skill", "approach", "outperforms", "that", "function,", "extracting", "end,", "generalize", "manipulating", "baselines", "inverse", "[cs.lg]", "(fist),", "imitation", "dynamics", "this", "property", "during", "algorithm", "long-horizon", "extracts", "have", "utilizes", "skills", "remains"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "237532682": {"id": "237532682", "openalex": null, "doi": null, "title": "Scaling Laws for Neural Machine Translation", "abstract": "We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study.Preprint. Under review.", "authors": [], "concepts": ["threshold.", "increases.", "law.", "machine", "size,", "parameters", "accurate", "train/test", "total", "exponents", "model", "used", "(iv)", "gives", "translations.", "side", "show", "reduction", "translation", "composition", "score", "observation.", "report", "test", "alone", "such", "certain", "follows", "scaling,", "were", "(iii)", "encoder/decoder", "(ii)", "number", "empirical", "from", "present", "function", "specifically", "relationship", "improves,", "propose", "optimal", "nature", "models", "language,", "data.", "stops", "both", "scaling", "finally,", "enjoys", "deviation", "originally", "naturally", "improve", "successful", "decoder", "target", "source", "manifests", "text", "when", "define", "based", "observe", "(either", "generated", "purposes.", "neural", "sufficient", "behavior", "bias", "predictions", "capacity", "that", "behaviors,", "sets,", "between", "influenced", "recommendations", "language", "different", "languages;", "acutely", "transformer", "sets", "natural", "contrast,", "approaches", "bleu", "size", "allocation", "translated", "provide", "(nmt).", "find", "investigate", "depending", "encoder,", "review.", "also", "release", "laws", "text).", "describes", "quality", "power", "this", "which", "encoder-decoder", "loss.", "study.preprint.", "improving", "after", "loss", "formula", "bivariate", "under", "study", "variety", "cross-entropy", "human", "properties", "encoder"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238198466": {"id": "238198466", "openalex": null, "doi": null, "title": "A FIRST-OCCUPANCY REPRESENTATION FOR REINFORCEMENT LEARNING", "abstract": "Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states. The successor representation (SR), which measures the expected cumulative, discounted state occupancy under a fixed policy, enables efficient transfer to different reward structures in an otherwise constant Markovian environment and has been hypothesized to underlie aspects of biological behavior and neural activity. However, in the real world, rewards may move or only be available for consumption once, may shift location, or agents may simply aim to reach goal states as rapidly as possible without the constraint of artificially imposed task horizons. In such cases, the most behaviorally-relevant representation would carry information about when the agent was likely to first reach states of interest, rather than how often it should expect to visit them over a potentially infinite time span. To reflect such demands, we introduce the firstoccupancy representation (FR), which measures the expected temporal discount to the first time a state is accessed. We demonstrate that the FR facilitates exploration, the selection of efficient paths to desired states, allows the agent, under certain conditions, to plan provably optimal trajectories defined by a sequence of subgoals, and induces similar behavior to animals avoiding threatening stimuli.", "authors": [], "concepts": ["environments", "demonstrate", "discount", "accessed.", "most", "tasks", "constant", "reach", "them", "available", "policy,", "carry", "information", "traverse", "efficient", "such", "certain", "about", "only", "artificially", "span.", "would", "artificial", "been", "rewarding", "expected", "from", "visit", "induces", "however,", "rapidly", "(fr),", "demands,", "activity.", "should", "exploration,", "task", "agents", "agent,", "optimal", "reinforcement", "selection", "likely", "rather", "enable", "transfer", "both", "plan", "conditions,", "support", "learning", "goal", "desired", "measures", "infinite", "hypothesized", "without", "environment", "horizons.", "cumulative,", "potentially", "when", "consumption", "animals", "states,", "stimuli.", "provably", "reflect", "location,", "neural", "across", "behavior", "otherwise", "reward", "threatening", "successor", "states", "rapid", "trajectories", "structures", "underlie", "shift", "first-occupancy", "that", "states.", "constraint", "real", "their", "fixed", "benefit", "facilitates", "different", "world,", "move", "discounted", "sequence", "representations", "paths", "efficiently", "simply", "first", "cases,", "allows", "rewards", "temporal", "possible", "(sr),", "once,", "introduce", "avoiding", "interest,", "than", "over", "biological", "occupancy", "which", "state", "enables", "firstoccupancy", "agent", "time", "defined", "subgoals,", "often", "aspects", "expect", "imposed", "similar", "under", "markovian", "representation", "behaviorally-relevant"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238215654": {"id": "238215654", "openalex": null, "doi": null, "title": "PAC-BAYES INFORMATION BOTTLENECK", "abstract": "Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the compression of information stored in weights (IIW) is proved to play a key role in NNs generalization based on the PAC-Bayes theorem. However, no solution of IIW has ever been provided, which builds a barrier for further investigation of the IIW's property and its potential in practical deep learning. In this paper, we propose an algorithm for the efficient approximation of IIW. Then, we build an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, we can empirically identify the fitting to compressing phase transition during NNs' training and the concrete connection between the IIW compression and the generalization. Besides, we verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, overparameterization, and noisy labels. Moreover, we propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of IIW in enhancing NNs in practice. . Information dropout: Learning optimal representations through noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2897-2905, 2018b.Vivek S Borkar and Sanjoy K Mitter. A strong approximation theorem for stochastic recursive algorithms.", "authors": [], "concepts": ["machine", "ieee", "paper,", "provided,", "generalization.", "most", "pib.", "theoretical", "characterized", "besides,", "trying", "verify", "broad", "theorem.", "mitter.", "dropout:", "algorithms.", "nns'", "information", "pib,", "efficient", "been", "iiw's", "theorem", "through", "able", "barrier", "mcmc-based", "from", "deep", "however,", "pac-bayes", "concrete", "accuracy", "derive", "e.g.,", "varying", "propose", "optimal", "explain", "labels.", "there", "generalization", "learning.", "bottleneck", "complexity", "nns.", "40(12):2897-2905,", "superior", "iiw-based", "compression", "learning", "problems", "recently,", "ability", "investigation", "source", "understanding", "stored", "practice.", "analysis", "pattern", "based", "2018b.vivek", "transition", "borkar", "namely", "potential", "sizes,", "series", "(iiw)", "training", "works", "noisy", "identify", "sanjoy", "play", "nns,", "that", "connection", "between", "weights", "solution", "strong", "proved", "weight", "compressing", "approximation", "builds", "representations", "fulfills", "trade-off", "empirically", "build", "transactions", "intelligence,", "cases,", "batch", "practical", "overparameterization,", "computation.", "moreover,", "phase", "non-vacuous", "ever", "enhancing", "sample", "research.", "this", "which", "property", "during", "algorithm", "recursive", "iiw.", "bounds", "have", "role", "important", "posterior", "fitting", "then,", "remains", "further", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238407774": {"id": "238407774", "openalex": null, "doi": null, "title": "EQUIVARIANT SUBGRAPH AGGREGATION NETWORKS", "abstract": "Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures. * Equal contribution, authors are in alphabetical order.Figure 1: We present a provably expressive graph learning framework based on representing graphs as bags of subgraphs and processing them with an equivariant architecture composed of GNNs and set networks. Left: A pair of graphs not distinguishable by the WL test. Right: The corresponding bags (multisets) of edge-deleted subgraphs, which can be distinguished by our framework.Our approach. In an effort to devise simple, intuitive and more flexible provably expressive graph architectures, we develop a novel framework, dubbed Equivariant Subgraph Aggregation Networks (ESAN), to enhance the expressive power of existing GNNs. Our solution emerges from the observation that while two graphs may not be distinguishable by an MPNN, it may be easy to find distinguishable subgraphs. More generally, instead of encoding multisets of node colors as done in MPNNs and the WL test, we opt for encoding bags (multisets) of subgraphs and show that such an encoding can lead to a better expressive power. Following that observation, we advocate representing each graph as a bag of subgraphs chosen according to some predefined policy, e.g., all graphs that can be obtained by removing one edge from the original graph.Figure 1illustrates this idea.Bags of subgraphs are highly structured objects whose symmetry arises from both the structure of each constituent graph as well as the multiset on the whole. We propose an equivariant architecture specifically tailored to capture this object's symmetry group. Specifically, we first formulate the symmetry group for a set of graphs as the direct product of the symmetry groups for sets and graphs. We then construct a neural network comprising layers that are equivariant to this group. Motivated byMaron et al. (2020), these layers employ two base graph encoders as subroutines: The first encoder implements a Siamese network processing each subgraph independently; The second acts as an information sharing module by processing the aggregation of the subgraphs. After being processed by several such layers, a set learning module aggregates the obtained subgraph representations into an invariant representation of the original graph that is used in downstream tasks.An integral component of our method, with major impacts on its complexity and expressivity, is the subgraph selection policy: a function that maps a graph to a bag of subgraphs, which is then processed by our equivariant neural network. In this paper, we explore four simple -yet powerful -subgraph selection policies: node-deleted subgraphs, edge-deleted subgraphs, and two variants of ego-networks. To alleviate the possible computational burden, we also introduce an efficient stochastic version of our method implemented by random sampling of subgraphs according to the aforementioned policies.We provide a thorough theoretical analysis of our approach. We first prove that our architecture can implement novel and provably stronger variants of the well-known WL test, capable of encoding the multiset of subgraphs according to the base graph encoder (e.g., WL for MPNNs). Furthermore, we study how the expressive power of our architecture depends on different main design choices like the underlying base graph encoder or the subgraph selection policy. Notably, we prove that our framework can separate 3-WL indistinguishable graphs using only a 1-WL graph encoder, and that it can enhance the expressive power of stronger architectures such as PPGN (Maron et al., 2019a).We then present empirical results on a wide range of synthetic and real datasets, using several existing GNNs as base encoders. Firstly, we study the expressive power of our approach using the synthetic datasets introduced by Abboud et al. (2020)  and show that it achieves perfect accuracy.Published as a conference paper at ICLR 2022 Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In ICML, 2018. -supervised classification with graph convolutional networks. In ICLR, 2017.Dmitry B Kireev. Chemnet: a novel neural network based method for graph/property mapping.", "authors": [], "concepts": ["architectures,", "policy:", "generally,", "well-known", "(esan),", "component", "each", "distinguishable", "paper,", "siamese", "regina", "aforementioned", "variational", "policies:", "stronger", "data,", "graph-structured", "observation", "theoretical", "choices", "mapping.", "used", "version", "them", "show", "downstream", "group", "-subgraph", "method", "lead", "devise", "they", "simple,", "policy,", "overall", "1-wl", "node-deleted", "multisets", "test", "issue.", "networks.", "information", "proposes", "2019a).we", "efficient", "such", "these", "gnns", "existing", "large", "only", "subgraph", "observation,", "composed", "layers", "accuracy.published", "empirical", "variants", "highly", "explore", "shown", "datasets", "represent", "classification", "generation.", "simplicity", "more", "sampling", "performance", "enhance", "from", "3-wl", "into", "part", "present", "deep", "order.figure", "encoders", "using", "-yet", "convolutional", "subgraphs", "flexible", "well", "groups", "structure", "expressiveness", "while", "tree", "group.", "function", "conference", "specifically", "mpnn,", "iclr,", "e.g.,", "objects", "autoencoder", "2018.", "simple", "invariant", "comprehensive", "framework", "propose", "abboud", "thorough", "viewed", "jaakkola.", "capture", "wengong", "arises", "1illustrates", "graphs.", "according", "selection", "subgraphs.", "unfortunately,", "contribution,", "scalability.", "alphabetical", "achieves", "method,", "being", "networks", "wide", "process", "some", "deal", "encoders.", "encoding", "derived", "affect", "complexity", "both", "furthermore,", "-supervised", "contain", "burden,", "left:", "power.", "popular", "random", "lower", "graph", "message-passing", "weisfeiler-leman", "terms", "better", "intuitive", "(esan)", "processed", "acts", "capable", "expressive", "test,", "policies.we", "architecture's", "pair", "learning", "thus,", "multiset", "like", "comprising", "motivated", "ego-networks.", "implements", "gnns.", "prove", "done", "architectures.", "integral", "(2020),", "experiments", "network", "specifically,", "analysis", "novel", "molecular", "based", "improves", "powerful", "easy", "corresponding", "policy", "representing", "employ", "chosen", "provably", "major", "obtained", "neural", "notably,", "with", "direct", "design", "symmetry", "subroutines:", "network.", "several", "dubbed", "sharing", "approach.", "right:", "framework.our", "maps", "introduced", "cost,", "implement", "four", "approach", "aggregation", "that", "instead", "expressivity,", "predefined", "real", "(maron", "their", "test.", "base", "esan", "graph/property", "bags", "increases", "advocate", "alleviate", "solution", "icml,", "edge-deleted", "independently;", "different", "implemented", "increased", "structured", "(multisets)", "processing", "sets", "separate", "demonstrates", "ppgn", "graphs", "bymaron", "layers,", "module", "mpnns", "graph.figure", "emerges", "(2020)", "scheme,", "representations", "leading", "called", "range", "chemnet:", "first", "datasets,", "impacts", "address", "paper", "tasks.an", "architectures", "provide", "computational", "junction", "(e.g.,", "find", "following", "tailored", "idea.bags", "possible", "moreover,", "indistinguishable", "equivariant", "encoder,", "results", "al.,", "synthetic", "also", "second", "introduce", "then", "depends", "suitable", "aggregates", "formulate", "barzilay,", "(1-wl)", "power", "mpnns).", "this", "which", "2017.dmitry", "product", "colors", "subgraphs,", "jin,", "node", "framework,", "after", "bounds", "removing", "kireev.", "develop", "firstly,", "authors", "often", "1-dimensional", "iclr", "main", "framework.", "perfect", "isomorphism,", "underlying", "original", "limited", "policy.", "(mpnns)", "object's", "architecture", "describe", "whole.", "representation", "distinguished", "constituent", "tommi", "study", "effort", "whose", "architecture.", "further", "variants.", "equal", "construct", "stochastic", "encoder", "edge"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238408001": {"id": "238408001", "openalex": null, "doi": null, "title": "VC DIMENSION OF PARTIALLY QUANTIZED NEURAL NETWORKS IN THE OVERPARAMETRIZED REGIME", "abstract": "Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small generalization error of overparametrized neural networks. Indeed, existing applications of VC theory to large networks obtain upper bounds on VC dimension that are proportional to the number of weights, and for a large class of networks, these upper bound are known to be tight. In this work, we focus on a class of partially quantized networks that we refer to as hyperplane arrangement neural networks (HANNs). Using a sample compression analysis, we show that HANNs can have VC dimension significantly smaller than the number of weights, while being highly expressive. In particular, empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability. We further demonstrate the expressivity of HANNs empirically. On a panel of 121 UCI datasets, overparametrized HANNs match the performance of state-of-the-art full-precision models.", "authors": [], "concepts": ["smaller", "state-of-the-art", "known", "demonstrate", "partially", "indeed,", "bound", "expressivity", "error", "particular,", "rate", "show", "empirically.", "panel", "networks.", "quantized", "these", "existing", "large", "(hanns).", "been", "number", "empirical", "highly", "classification", "performance", "minimax", "using", "while", "explain", "generalization", "hyperplane", "achieves", "being", "networks", "vapnik-chervonenkis", "networks,", "compression", "analysis,", "significantly", "applications", "proportional", "overparametrized", "class", "regime", "probability.", "weights,", "unable", "neural", "with", "upper", "refer", "work,", "dimension", "arrangement", "that", "minimization", "lipschitz", "full-precision", "hanns", "datasets,", "(vc)", "focus", "match", "theory", "expressive.", "tight.", "sample", "than", "over", "models.", "this", "risk", "bounds", "have", "small", "posterior", "further", "obtain"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238408308": {"id": "238408308", "openalex": null, "doi": null, "title": "8-BIT OPTIMIZERS VIA BLOCK-WISE QUANTIZATION", "abstract": "Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization compared to plain stochastic gradient descent but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-sourceour 8-bit optimizers as a drop-in replacement that only requires a two-line code change.Increasing model size is an effective way to achieve better performance for given resourcesHenighan et al., 2020;Raffel et al., 2019;Lewis et al., 2021). However, training such large models requires storing the model, gradient, and state of the optimizer (e.g., exponentially smoothed sum and squared sum of previous gradients for Adam), all in a fixed amount of available memory. Although significant research has focused on enabling larger model training by reducing or efficiently distributing the memory required for the model parameters(Shoeybi et al., 2019;Lepikhin et al., 2020;Fedus et al., 2021;Brown et al., 2020;Rajbhandari et al., 2020), reducing the memory footprint of optimizer gradient statistics is much less studied. This is a significant missed opportunity since these optimizer states use 33-75% of the total memory footprint during training. For example, the Adam optimizer states for the largest GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2019) models are 11 GB and 41 GB in size. In this paper, we develop a fast, high-precision non-linear quantization method -block-wise dynamic quantization -that enables stable 8-bit optimizers (e.g., Adam, AdamW, and Momentum) which maintain 32-bit performance at a fraction of the memory footprint and without any changes to the original hyperparameters. 1 While most current work uses 32-bit optimizer states, recent high-profile efforts to use 16-bit optimizers report difficultly for large models with more than 1B parameters(Ramesh et al., 2021). Going from 16-bit optimizers to 8-bit optimizers reduces the range of possible values from 2 16 = 65536 values to just 2 8 = 256. To our knowledge, this has not been attempted before.Effectively using this very limited range is challenging for three reasons: quantization accuracy, computational efficiency, and large-scale stability. To maintain accuracy, it is critical to introduce some form of non-linear quantization to reduce errors for both common small magnitude values 1  We study 8-bit optimization with current best practice model and gradient representations (typically 16-bit mixed precision), to isolate optimization challenges. Future work could explore further compressing all three.", "authors": [], "concepts": ["values,", "changes:", "smaller", "yielding", "stability.", "size.", "future", "challenging", "machine", "each", "paper,", "quantization,", "computational,", "total", "parameters,", "most", "2020),", "2021;brown", "model", "difficultly", "used", "2020;rajbhandari", "challenges,", "comes", "method", "available", "recent", "resulting", "tokens", "report", "fraction", "faster", "such", "larger", "gpt-2", "these", "large", "only", "parameters(shoeybi", "optimizer", "been", "embedding", "just", "footprint", "highly", "explore", "memory.", "much", "1.5b", "allocated", "more", "performance", "non-uniform", "fast,", "from", "hyperparameters.", "into", "however,", "gradients", "using", "magnitude", "example,", "drop-in", "plain", "contrastive", "while", "might", "compared", "-that", "optimizers", "e.g.,", "very", "2019;lewis", "requires", "large-scale", "2019)", "precision),", "16-bit", "models", "2020;raffel", "largest", "critical", "block-wise", "storing", "reduces", "limiting", "significant", "some", "current", "parallel", "both", "tensors", "replacement", "block", "reasons:", "work", "better", "attempted", "time,", "processed", "distributing", "gradient", "additional", "mixed", "translation,", "without", "levels", "non-linear", "could", "performance,", "errors", "focused", "overcome", "memory", "missed", "stable", "before.effectively", "high", "studied.", "uses", "stability", "changes", "exponentially", "practice.", "quantization", "high-profile", "model,", "blocks", "code", "form", "33-75%", "states,", "maintain", "past", "input", "achieve", "accuracy,", "since", "training", "three.", "resourceshenighan", "although", "2020;fedus", "across", "with", "challenges.", "including", "gradient,", "smoothed", "adam),", "modeling,", "(adam)", "given", "precise", "otherwise", "variance", "less", "high-precision", "states", "quantization.", "roberta", "parameters(ramesh", "trained", "wmt'14", "dynamic", "common", "maintaining", "that", "states.", "distribution", "training.", "finetuning,", "imagenet", "fixed", "required", "(radford", "cores,", "two-line", "-block-wise", "adamw,", "language", "previous", "compressing", "maximum", "pretraining,", "open-sourceour", "efforts", "adam,", "effective", "(typically", "squared", "(raffel", "momentum)", "representations", "efficiency,", "reduce", "efficiently", "range", "enabling", "first", "256.", "reducing", "independently", "accelerate", "size", "result,", "statistics", "divides", "research", "opportunity", "amount", "parameter", "three", "computational", "(e.g.,", "values.", "possible", "quantized.", "combine", "isolate", "adam", "al.,", "layer", "introduce", "best", "than", "over", "models.", "tasks,", "this", "which", "state", "pretraining+finetuning,", "during", "(sgd", "enables", "change.increasing", "develop", "8-bit", "small", "optimization", "precision", "2021).", "original", "limited", "going", "classification,", "practice", "2019;lepikhin", "study", "thereby", "stateful", "32-bit", "moco", "glue", "knowledge,", "further", "values", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238408406": {"id": "238408406", "openalex": null, "doi": null, "title": "ON THE IMPORTANCE OF FIRTH BIAS REDUCTION IN FEW-SHOT CLASSIFICATION", "abstract": "Learning accurate classifiers for novel categories from very few examples, known as few-shot image classification, is a challenging task in statistical machine learning and computer vision. The performance in few-shot classification suffers from the bias in the estimation of classifier parameters; however, an effective underlying bias reduction technique that could alleviate this issue in training few-shot classifiers has been overlooked. In this work, we demonstrate the effectiveness of Firth bias reduction in few-shot classification. Theoretically, Firth bias reduction removes the O(N \u22121 ) first order term from the small-sample bias of the Maximum Likelihood Estimator. Here we show that the general Firth bias reduction technique simplifies to encouraging uniform class assignment probabilities for multinomial logistic classification, and almost has the same effect in cosine classifiers. We derive an easy-to-implement optimization objective for Firth penalized multinomial logistic and cosine classifiers, which is equivalent to penalizing the cross-entropy loss with a KL-divergence between the uniform label distribution and the predictions. Then, we empirically evaluate that it is consistently effective across the board for few-shot image classification, regardless of (1) the feature representations from different backbones, (2) the number of samples per class, and (3) the number of classes. Finally, we show the robustness of Firth bias reduction, in the case of imbalanced data distribution. Our implementation is available at", "authors": [], "concepts": ["case", "challenging", "computer", "known", "demonstrate", "machine", "overlooked.", "term", "accurate", "encouraging", "simplifies", "here", "uniform", "classifiers,", "show", "available", "reduction", "predictions.", "classification.", "parameters;", "feature", "been", "vision.", "cosine", "general", "number", "classification", "performance", "firth", "from", "however,", "order", "classifier", "reduction,", "derive", "logistic", "easy-to-implement", "task", "very", "issue", "regardless", "finally,", "statistical", "learning", "probabilities", "equivalent", "theoretically,", "examples,", "could", "suffers", "estimator.", "backbones,", "estimation", "importance", "classifiers.", "classes.", "imbalanced", "novel", "label", "class", "image", "categories", "data", "training", "across", "with", "kl-divergence", "few-shot", "small-sample", "bias", "work,", "that", "distribution", "between", "alleviate", "different", "implementation", "robustness", "maximum", "effective", "effectiveness", "technique", "representations", "empirically", "first", "board", "objective", "same", "almost", "consistently", "this", "which", "samples", "penalized", "loss", "classifiers", "multinomial", "optimization", "likelihood", "underlying", "evaluate", "assignment", "classification,", "effect", "removes", "then,", "cross-entropy", "penalizing", "class,", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238419359": {"id": "238419359", "openalex": null, "doi": null, "title": "GNN IS A COUNTER? REVISITING GNN FOR QUESTION ANSWERING", "abstract": "Question Answering (QA) has been a long-standing research topic in AI and NLP fields, and a wealth of studies have been conducted to attempt to equip QA systems with human-level reasoning capability. To approximate the complicated human reasoning process, state-of-the-art QA systems commonly use pre-trained language models (LMs) to access knowledge encoded in LMs together with elaborately designed modules based on Graph Neural Networks (GNNs) to perform reasoning over knowledge graphs (KGs). However, many problems remain open regarding the reasoning functionality of these GNN-based modules. Can these GNN-based modules really perform a complex reasoning process? Are they under-or overcomplicated for QA? To open the black box of GNN and investigate these problems, we dissect state-of-the-art GNN modules for QA and analyze their reasoning capability. We discover that even a very simple graph neural counter can outperform all the existing GNN modules on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets which heavily rely on knowledge-aware reasoning. Our work reveals that existing knowledge-aware GNN modules may only carry out some simple reasoning such as counting. It remains a challenging open problem to build comprehensive reasoning modules for knowledge-powered QA. * Work done during an internship at MSRA", "authors": [], "concepts": ["state-of-the-art", "challenging", "counter", "commonsenseqa", "outperform", "access", "studies", "counter?", "reasoning.", "problem", "approximate", "conducted", "question", "gnn-based", "they", "remain", "carry", "such", "reveals", "these", "existing", "only", "equip", "been", "datasets", "(lms)", "however,", "counting.", "revisiting", "many", "human-level", "open", "very", "simple", "comprehensive", "models", "answering", "networks", "some", "popular", "designed", "graph", "work", "fields,", "knowledge-aware", "heavily", "problems", "rely", "done", "under-or", "modules.", "based", "complicated", "knowledge-powered", "modules", "neural", "reasoning", "with", "overcomplicated", "systems", "really", "benchmark", "that", "internship", "msra", "their", "knowledge", "analyze", "language", "(kgs).", "regarding", "graphs", "black", "build", "encoded", "research", "investigate", "dissect", "perform", "together", "commonly", "problems,", "openbookqa,", "(qa)", "topic", "over", "which", "during", "discover", "functionality", "process,", "have", "pre-trained", "elaborately", "even", "long-standing", "capability.", "wealth", "(gnns)", "remains", "human", "complex", "process?", "attempt"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238582721": {"id": "238582721", "openalex": null, "doi": null, "title": "Frequency-aware SGD for Efficient Embedding Learning with Provable Benefits", "abstract": "Embedding learning has found widespread applications in recommendation systems and natural language modeling, among other domains. To learn quality embeddings efficiently, adaptive learning rate algorithms have demonstrated superior empirical performance over SGD, largely accredited to their token-dependent learning rate. However, the underlying mechanism for the efficiency of token-dependent learning rate remains underexplored. We show that incorporating frequency information of tokens in the embedding learning problems leads to provably efficient algorithms, and demonstrate that common adaptive algorithms implicitly exploit the frequency information to a large extent. Specifically, we propose (Counterbased) Frequency-aware Stochastic Gradient Descent, which applies a frequency-dependent learning rate for each token, and exhibits provable speed-up compared to SGD when the token distribution is imbalanced. Empirically, we show the proposed algorithms are able to improve or match adaptive algorithms on benchmark recommendation tasks and a large-scale industrial recommendation system, closing the performance gap between SGD and adaptive algorithms, while using significantly lower memory. Our results are the first to show token-dependent learning rate provably improves convergence for non-convex embedding learning problems. * Work done while an intern at Facebook. Corresponding email:{yli939, tourzhao, gl68}@gatech.edu. 1 arXiv:2110.04844v3 [cs.LG] 23 Nov 2021 1.1 Related Literature Adaptive algorithms for non-convex problems. There has been a fruitful line of research on analyzing the convergence of adaptive learning rate algorithms in non-convex setting. These results aim to match the convergence rate of standard SGD given by O(1/ \u221a T) (Ghadimi and Lan, 2013), however often with additional factor of log T (Ward et al., 2018; D\u00e9fossez et al., 2020; Chen et al., 2018; Reddi et al., 2018), or with worse dimension dependence (Zhou et al., 2018a) for smooth problem (assumed byalmost all prior works). Moreover, all existing works aim to analyze the convergence for general non-convex problems, ignoring unique data features in embedding learning problems, where adaptive algorithms are most successful. We explicitly take account into the sparsity of stochastic gradient, and token distribution imbalancedness into the design and analysis of our proposed algorithms, which are the keys to better convergence properties. Adaptive algorithms and SGD. To the best of our knowledge, the study on understanding why adaptive learning rate algorithms outperform SGD is very limited. Zhang et al. (2019)  argue that BERT pretraining (Devlin et al., 2018)  has heavy-tailed noise, implying unbounded variance and possible non-convergence of SGD. Normalized gradient clipping method is proposed therein and converges for a family of heavy-tailed noise distributions. Our results focus on a different direction by showing that imbalanced token distribution is an important factor that can be leveraged to design more efficient algorithms for embedding learning problems. Our result also does not rely on the noise to be heavy-tailed for the convergence benefits of the proposed FA/CF-SGD to take effect. Notations: For a vector/matrix, we use \u00b7 to denotes its 2 -norm/Frobenius norm. We use \u00b7 2 to denote the spectral norm of a matrix.", "authors": [], "concepts": ["setting.", "applies", "learn", "demonstrate", "sgd,", "arxiv:2110.04844v3", "fruitful", "each", "ignoring", "lan,", "outperform", "most", "tasks", "problems.", "token,", "problem", "non-convex", "2013),", "rate", "show", "method", "(counterbased)", "norm", "literature", "pretraining", "system,", "tokens", "speed-up", "byalmost", "information", "efficient", "empirically,", "these", "existing", "large", "(2019)", "prior", "normalized", "does", "efficiently,", "been", "embedding", "general", "empirical", "denote", "memory.", "able", "adaptive", "domains.", "more", "performance", "into", "converges", "however,", "recommendation", "leads", "using", "explicitly", "account", "while", "closing", "compared", "demonstrated", "imbalancedness", "very", "algorithms,", "frequency", "spectral", "propose", "large-scale", "2018),", "convergence", "there", "properties.", "(ghadimi", "extent.", "found", "gl68}@gatech.edu.", "2018)", "keys", "lower", "(ward", "work", "unique", "non-convergence", "features", "among", "better", "chen", "incorporating", "superior", "analyzing", "line", "learning", "direction", "improve", "problems", "gradient", "additional", "sgd.", "significantly", "limited.", "reddi", "applications", "noise,", "rely", "(zhou", "done", "other", "understanding", "email:{yli939,", "dependence", "family", "argue", "leveraged", "specifically,", "imbalanced", "2018a)", "analysis", "embeddings", "rate.", "when", "-norm/frobenius", "improves", "token-dependent", "corresponding", "data", "provably", "zhang", "facebook.", "works).", "vector/matrix,", "works", "with", "widespread", "gradient,", "modeling,", "given", "design", "standard", "variance", "systems", "frequency-dependent", "efficiency", "common", "benchmark", "frequency-aware", "dimension", "that", "related", "distribution", "heavy-tailed", "between", "d\u00e9fossez", "their", "noise", "analyze", "exhibits", "o(1/", "intern", "token", "language", "exploit", "different", "2020;", "denotes", "take", "successful.", "provable", "therein", "natural", "distributions.", "sparsity", "2018;", "result", "matrix.", "effect.", "first", "imbalanced.", "focus", "research", "showing", "match", "algorithms", "tourzhao,", "descent,", "possible", "(assumed", "moreover,", "results", "[cs.lg]", "problems,", "al.,", "also", "notations:", "best", "where", "quality", "over", "industrial", "implying", "which", "(devlin", "underexplored.", "benefits", "implicitly", "factor", "however", "largely", "have", "unbounded", "smooth", "often", "proposed", "underlying", "important", "bert", "fa/cf-sgd", "worse", "clipping", "study", "accredited", "mechanism", "norm.", "remains", "knowledge,", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238583252": {"id": "238583252", "openalex": null, "doi": null, "title": "PHASE COLLAPSE IN NEURAL NETWORKS", "abstract": "Deep convolutional classifiers linearly separate image classes and improve accuracy as depth increases. They progressively reduce the spatial dimension whereas the number of channels grows with depth. Spatial variability is therefore transformed into variability along channels. A fundamental challenge is to understand the role of non-linearities together with convolutional filters in this transformation. ReLUs with biases are often interpreted as thresholding operators that improve discrimination through sparsity. This paper demonstrates that it is a different mechanism called phase collapse which eliminates spatial variability while linearly separating classes. We show that collapsing the phases of complex wavelet coefficients is sufficient to reach the classification accuracy of ResNets of similar depths. However, replacing the phase collapses with thresholding operators that enforce sparsity considerably degrades the performance. We explain these numerical results by showing that the iteration of phase collapses progressively improves separation of classes, as opposed to thresholding non-linearities.", "authors": [], "concepts": ["increases.", "separating", "enforce", "understand", "classes,", "separation", "reach", "show", "wavelet", "they", "collapsing", "spatial", "these", "discrimination", "number", "non-linearities", "through", "classification", "performance.", "into", "deep", "however,", "depths.", "convolutional", "along", "while", "accuracy", "collapse", "channels.", "fundamental", "explain", "networks", "iteration", "resnets", "improve", "opposed", "coefficients", "interpreted", "linearly", "classes", "numerical", "classes.", "sparsity.", "improves", "image", "biases", "neural", "operators", "with", "sufficient", "depth", "whereas", "dimension", "that", "considerably", "transformed", "different", "eliminates", "collapses", "replacing", "separate", "demonstrates", "sparsity", "called", "reduce", "relus", "degrades", "paper", "showing", "challenge", "progressively", "phase", "results", "together", "depth.", "this", "which", "phases", "grows", "filters", "classifiers", "often", "role", "therefore", "variability", "similar", "channels", "mechanism", "transformation.", "thresholding", "complex", "non-linearities."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238634584": {"id": "238634584", "openalex": null, "doi": null, "title": "CRYSTAL DIFFUSION VARIATIONAL AUTOENCODER FOR PERIODIC MATERIAL GENERATION", "abstract": "Generating the periodic structure of stable materials is a long-standing challenge for the material design community. This task is difficult because stable materials only exist in a low-dimensional subspace of all possible periodic arrangements of atoms: 1) the coordinates must lie in the local energy minimum defined by quantum mechanics, and 2) global stability also requires the structure to follow the complex, yet specific bonding preferences between different atom types. Existing methods fail to incorporate these factors and often lack proper invariances. We propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the physical inductive bias of material stability. By learning from the data distribution of stable materials, the decoder generates materials in a diffusion process that moves atomic coordinates towards a lower energy state and updates atom types to satisfy bonding preferences between neighbors. Our model also explicitly encodes interactions across periodic boundaries and respects permutation, translation, rotation, and periodic invariances. We significantly outperform past methods in three tasks: 1) reconstructing the input structure, 2) generating valid, diverse, and realistic materials, and 3) generating materials that optimize a specific property. We also provide several standard datasets and evaluation metrics for the broader machine learning community. 1", "authors": [], "concepts": ["stability.", "machine", "must", "boundaries", "outperform", "variational", "exist", "atom", "captures", "bonding", "model", "community.", "these", "existing", "only", "updates", "specific", "datasets", "materials,", "from", "interactions", "broader", "follow", "explicitly", "generation", "structure", "towards", "task", "autoencoder", "requires", "physical", "global", "propose", "factors", "materials", "process", "encodes", "optimize", "because", "lower", "methods", "generating", "learning", "neighbors.", "permutation,", "decoder", "translation,", "significantly", "atoms:", "coordinates", "low-dimensional", "satisfy", "local", "crystal", "stable", "diffusion", "stability", "inductive", "types", "energy", "proper", "invariances.", "past", "property.", "data", "input", "across", "quantum", "atomic", "design", "standard", "complex,", "preferences", "several", "arrangements", "bias", "periodic", "that", "distribution", "between", "diverse,", "tasks:", "different", "rotation,", "lack", "provide", "realistic", "challenge", "three", "difficult", "fail", "metrics", "possible", "subspace", "valid,", "material", "also", "structure,", "evaluation", "this", "state", "(cdvae)", "defined", "mechanics,", "often", "incorporate", "types.", "long-standing", "respects", "minimum", "reconstructing", "moves", "generates"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "238857129": {"id": "238857129", "openalex": null, "doi": null, "title": "WHY PROPAGATE ALONE? PARALLEL USE OF LABELS AND FEATURES ON GRAPHS", "abstract": "Graph neural networks (GNNs) and label propagation represent two interrelated modeling strategies designed to exploit graph structure in tasks such as node property prediction. The former is typically based on stacked message-passing layers that share neighborhood information to transform node features into predictive embeddings. In contrast, the latter involves spreading label information to unlabeled nodes via a parameter-free diffusion process, but operates independently of the node features. Given then that the material difference is merely whether features or labels are smoothed across the graph, it is natural to consider combinations of the two for improving performance. In this regard, it has recently been proposed to use a randomly-selected portion of the training labels as GNN inputs, concatenated with the original node features for making predictions on the remaining labels. This so-called label trick accommodates the parallel use of features and labels, and is foundational to many of the top-ranking submissions on the Open Graph Benchmark (OGB) leaderboard. And yet despite its wide-spread adoption, thus far there has been little attempt to carefully unpack exactly what statistical properties the label trick introduces into the training pipeline, intended or otherwise. To this end, we prove that under certain simplifying assumptions, the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that naturally resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts to graph size and connectivity. Later, we leverage this perspective to motivate a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions.", "authors": [], "concepts": ["(ogb)", "nodes", "what", "former", "typically", "term", "extensions.", "merely", "interrelated", "tasks", "adoption,", "recently", "verify", "latter", "interpretable,", "information", "modeling", "such", "inputs,", "certain", "these", "difference", "composed", "alone?", "despite", "been", "layers", "reduced", "represent", "performance.", "carefully", "serves", "leverage", "into", "broader", "strategies", "exactly", "consider", "many", "structure", "while", "embeddings.", "submissions", "predictive", "share", "open", "labels.", "there", "networks", "later,", "thus", "trick", "parallel", "combinations", "neighborhood", "designed", "graph", "features", "message-passing", "unpack", "statistical", "randomly-selected", "naturally", "operates", "prove", "perspective", "experiments", "diffusion", "wide-spread", "issues,", "spreading", "conditioned", "based", "label", "accommodates", "regularization", "leaderboard.", "labels,", "potential", "training", "introduces", "neural", "making", "across", "with", "regard,", "smoothed", "given", "deterministic", "top-ranking", "predictions", "portion", "whether", "benchmark", "propagation", "pipeline,", "that", "so-called", "simplifying", "parameter-free", "exploit", "end,", "natural", "graphs", "unlabeled", "contrast,", "range", "features.", "first", "cases,", "independently", "size", "leakage", "provide", "objective", "foundational", "resolves", "labels", "stacked", "propagate", "remaining", "material", "prediction.", "second", "then", "this", "property", "node", "adapts", "efficacy", "improving", "factor", "little", "process,", "transform", "data-fitting", "intended", "proposed", "concatenated", "involves", "graph,", "original", "assumptions,", "under", "connectivity.", "motivate", "(gnns)", "properties", "factors.", "attempt", "otherwise.", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "239009452": {"id": "239009452", "openalex": null, "doi": null, "title": "SOUND AND COMPLETE NEURAL NETWORK REPAIR WITH MINIMALITY AND LOCALITY GUARANTEES", "abstract": "We present a novel methodology for repairing neural networks that use ReLU activation functions. Unlike existing methods that rely on modifying the weights of a neural network which can induce a global change in the function space, our approach applies only a localized change in the function space while still guaranteeing the removal of the buggy behavior. By leveraging the piecewise linear nature of ReLU networks, our approach can efficiently construct a patch network tailored to the linear region where the buggy input resides, which when combined with the original network, provably corrects the behavior on the buggy input. Our method is both sound and complete -the repaired network is guaranteed to fix the buggy input, and a patch is guaranteed to be found for any buggy input. Moreover, our approach preserves the continuous piecewise linear nature of ReLU networks, automatically generalizes the repair to all the points including other undetected buggy inputs inside the repair region, is minimal in terms of changes in the function space, and guarantees that outputs on inputs away from the repair region are unaltered. On several benchmarks, we show that our approach significantly outperforms existing methods in terms of locality and limiting negative side effects. Our code is available on GitHub: https://github.com/BU-DEPEND-Lab/REASSURE. arXiv:2110.07682v3 [cs.LG] 22 Jul 2022 REASSURE A PREPRINT Retraining or direct weight modification Decoupled DNN Our approach Figure 1: Comparison of different approaches to the neural network repair problem. The black lines represent the original neural network function. The red dot represents the buggy input. The colored lines represent the functions after the repairs are done.2. Direct weight modification. These approaches directly manipulate the weights in a neural network to fix the buggy inputs. The repair problem is typically cast into an optimization problem or a verification problem. For example,Dong et al. [2020]proposes to minimize a loss defined based on the buggy inputs.Goldberger et al. [2020]uses an SMT solver to identify minimal weight changes to the output layer of the network so that the undesirable behaviors are removed. In general, the optimization-based approach cannot guarantee removal of the buggy behaviors, and the verification-based approach does not scale beyond networks of a few hundred neurons. In addition, both approaches suffer from substantial accuracy drops on normal inputs since weight changes may be a poor proxy for changes in the function space.3. Architecture extension. The third category of approaches extends the given NN architecture, such as by introducing more weight parameters, to facilitate more efficient repairs. The so-called Decoupled DNN architecture Sotoudeh and Thakur [2021]  is the only work we know that falls into this category. Their idea is to decouple the activations of the network from values of the network by augmenting the original network. Their construction allows the formulation of any single-layer repair as an linear programming (LP) problem. The decoupling, however, causes the repaired network to become discontinuous (in the functional sense). In addition, it still cannot isolate the output change to a single buggy input from the rest of the inputs.", "authors": [], "concepts": ["applies", "decoupled", "modification", "typically", "single-layer", "guaranteed", "parameters,", "problem", "modifying", "continuous", "behaviors", "activations", "side", "show", "method", "available", "[2020]uses", "lines", "idea", "complete", "category.", "example,dong", "undetected", "efficient", "solver", "extension.", "such", "these", "existing", "only", "scale", "does", "removed.", "represent", "minimize", "verification", "more", "from", "into", "present", "problem.", "however,", "facilitate", "rest", "induce", "locality", "relu", "sound", "while", "preserves", "function", "accuracy", "benchmarks,", "inputs.goldberger", "global", "nature", "neurons.", "networks", "found", "limiting", "patch", "extends", "away", "both", "sense).", "networks,", "methods", "(lp)", "work", "causes", "inputs.", "done.2.", "terms", "retraining", "linear", "verification-based", "functional", "functions.", "thakur", "inside", "region,", "general,", "[2020]proposes", "automatically", "activation", "significantly", "output", "outputs", "rely", "localized", "manipulate", "discontinuous", "effects.", "other", "space.3.", "space", "inputs", "network", "changes", "minimality", "minimal", "poor", "repaired", "novel", "when", "based", "code", "modification.", "drops", "points", "unlike", "represents", "addition,", "proxy", "construction", "optimization-based", "input", "since", "negative", "provably", "neural", "input.", "with", "introducing", "including", "behavior", "direct", "input,", "identify", "arxiv:2110.07682v3", "given", "space,", "network.", "several", "sotoudeh", "comparison", "falls", "beyond", "region", "approach", "outperforms", "that", "behaviors,", "functions", "so-called", "generalizes", "preprint", "programming", "function.", "category", "their", "cast", "weights", "repairs", "-the", "different", "weight", "suffer", "network,", "augmenting", "third", "piecewise", "repairs.", "black", "figure", "approaches", "resides,", "efficiently", "hundred", "corrects", "behavior.", "repair", "leveraging", "know", "github:", "allows", "normal", "combined", "change", "formulation", "tailored", "moreover,", "isolate", "[cs.lg]", "unaltered.", "layer", "where", "this", "which", "undesirable", "still", "decouple", "methodology", "after", "architecture,", "loss", "cannot", "repairing", "defined", "directly", "optimization", "buggy", "guaranteeing", "substantial", "original", "architecture", "colored", "removal", "[2021]", "decoupling,", "reassure", "https://github.com/bu-depend-lab/reassure.", "guarantees", "guarantee", "values", "single", "construct", "become"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "239016615": {"id": "239016615", "openalex": null, "doi": null, "title": "TRIGGER HUNTING WITH A TOPOLOGICAL PRIOR FOR TROJAN DETECTION", "abstract": "Despite their success and popularity, deep neural networks (DNNs) are vulnerable when facing backdoor attacks. This impedes their wider adoption, especially in mission critical applications. This paper tackles the problem of Trojan detection, namely, identifying Trojaned models -models trained with poisoned data. One popular approach is reverse engineering, i.e., recovering the triggers on a clean image by manipulating the model's prediction. One major challenge of reverse engineering approach is the enormous search space of triggers. To this end, we propose innovative priors such as diversity and topological simplicity to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. Moreover, by encouraging a diverse set of trigger candidates, our method can perform effectively in cases with unknown target labels. We demonstrate that these priors can significantly improve the quality of the recovered triggers, resulting in substantially improved Trojan detection accuracy as validated on both synthetic and publicly available TrojAI benchmarks.", "authors": [], "concepts": ["demonstrate", "identifying", "encouraging", "vulnerable", "impedes", "problem", "effectively", "adoption,", "engineering", "method", "poisoned", "available", "resulting", "cases", "such", "triggers,", "these", "innovative", "only", "prior", "-models", "despite", "increase", "simplicity", "deep", "trojan", "clean", "accuracy", "diverse", "backdoor", "propose", "models", "labels.", "critical", "networks", "found", "data.", "both", "popular", "chances", "improved", "topological", "improve", "model's", "target", "substantially", "i.e.,", "significantly", "especially", "priors", "space", "trojaned", "popularity,", "hunting", "engineering,", "success", "recovered", "validated", "when", "image", "trigger", "search", "major", "neural", "facing", "with", "appropriate", "mission", "trained", "detection", "wider", "approach", "candidates,", "that", "finding", "their", "end,", "trojai", "tackles", "manipulating", "paper", "unknown", "challenge", "(dnns)", "perform", "moreover,", "synthetic", "prediction.", "also", "triggers", "triggers.", "detection,", "recovering", "publicly", "quality", "applications.", "this", "diversity", "enormous", "benchmarks.", "attacks.", "namely,", "reverse"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "239049633": {"id": "239049633", "openalex": null, "doi": null, "title": "Actor-critic is implicitly biased towards high entropy optimal policies", "abstract": "We show that the simplest actor-critic method -a linear softmax policy updated with TD through interaction with a linear MDP, but featuring no explicit regularization or explorationdoes not merely find an optimal policy, but moreover prefers high entropy optimal policies. To demonstrate the strength of this bias, the algorithm not only has no regularization, no projections, and no exploration like \u01eb-greedy, but is moreover trained on a single trajectory with no resets. The key consequence of the high entropy bias is that uniform mixing assumptions on the MDP, which exist in some form in all prior work, can be dropped: the implicit regularization of the high entropy bias is enough to ensure that all chains mix and an optimal policy is reached with high probability. As auxiliary contributions, this work decouples concerns between the actor and critic by writing the actor update as an explicit mirror descent, provides tools to uniformly bound mixing times within KL balls of policy space, and provides a projection-free TD analysis with its own implicit bias which can be run from an unmixed starting distribution. . On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1-76, 2021b. J Andrew Bagnell and Jeff Schneider. Covariant policy search. 2003. Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. arXiv preprint arXiv:1806.02450, 2018. Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning. Machine learning, 22(1):33-57, 1996. S\u00e9bastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 2015. . Neural temporal-difference and q-learning provably converge to global optima. arXiv preprint arXiv:1905.", "authors": [], "concepts": ["complexity.", "demonstrate", "writing", "machine", "chains", "prefers", "bound", "learning,", "merely", "exist", "daniel", "uniform", "mirror", "show", "bias,", "method", "concerns", "policy,", "within", "ensure", "regularization,", "only", "arxiv:1905.", "prior", "difference", "contributions,", "update", "actor-critic", "through", "actor", "featuring", "approximation.", "arxiv", "times", "from", "critic", "unmixed", "trends", "explicit", "function", "auxiliary", "towards", "2018.", "global", "optimal", "projections,", "convex", "learning.", "some", "singal.", "\u01eb-greedy,", "work", "biased", "arxiv:1806.02450,", "optimization:", "mdp,", "linear", "barto.", "research,", "learning", "dropped:", "andrew", "explorationdoes", "policies.", "like", "temporal-difference", "gradient", "moreover", "updated", "reached", "approximation,", "interaction", "s\u00e9bastien", "high", "raghav", "optimality,", "methods:", "simplest", "analysis", "form", "regularization", "steven", "probability.", "policy", "exploration", "provably", "russo,", "neural", "with", "optima.", "implicit", "balls", "space,", "bias", "trained", "projection-free", "work,", "shift.", "assumptions", "that", "distribution", "jeff", "22(1):33-57,", "preprint", "between", "trajectory", "tools", "converge", "covariant", "softmax", "jalaj", "decouples", "strength", "bubeck.", "policies", "bradtke", "search.", "schneider.", "algorithms", "find", "temporal", "theory", "descent,", "starting", "2003.", "22(98):1-76,", "1996.", "uniformly", "entropy", "this", "which", "enough", "finite", "provides", "resets.", "consequence", "algorithm", "mixing", "bagnell", "implicitly", "2021b.", "time", "least-squares", "bhandari,", "foundations", "journal", "2015.", "single", "q-learning", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "240070335": {"id": "240070335", "openalex": null, "doi": null, "title": "TRAIL: NEAR-OPTIMAL IMITATION LEARNING WITH SUBOPTIMAL DATA", "abstract": "The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human experts can be expensive to obtain in large number. On the other hand, it is often much easier to obtain large quantities of suboptimal or task-agnostic trajectories, which are not useful for direct imitation, but can nevertheless provide insight into the dynamical structure of the environment, showing what could be done in the environment even if not what should be done. We ask the question, is it possible to utilize such suboptimal offline datasets to facilitate provably improved downstream imitation learning? In this work, we answer this question affirmatively and present training objectives that use offline datasets to learn a factored transition model whose structure enables the extraction of a latent action space. Our theoretical analysis shows that the learned latent action space can boost the sample-efficiency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of auxiliary non-expert data. To learn the latent action space in practice, we propose TRAIL (Transition-Reparametrized Actions for Imitation Learning), an algorithm that learns an energy-based transition model contrastively, and uses the transition model to reparametrize the action space for sample-efficient imitation learning. We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks. Our results verify the benefits suggested by our theory and show that TRAIL is able to improve baseline imitation learning by up to 4x in performance. 1", "authors": [], "concepts": ["actions", "learn", "high-quality", "what", "shows", "learning,", "action", "effectively", "expensive", "sample-efficient", "theoretical", "question", "model", "show", "downstream", "verify", "trajectories,", "such", "large", "much", "datasets", "through", "performance.", "able", "from", "into", "present", "(transition-reparametrized", "however,", "facilitate", "learning?", "contrastively,", "structure", "auxiliary", "should", "baseline", "practice,", "locomotion", "propose", "demonstrations.", "number.", "learning.", "environment,", "data.", "utilize", "hand,", "demonstrations", "factored", "improved", "learning", "affirmatively", "improve", "trail:", "could", "done.", "question,", "done", "latent", "offline", "other", "boost", "experiments", "space", "environment", "uses", "analysis", "energy-based", "transition", "nevertheless", "data", "training", "provably", "experts", "tasks.", "imitation,", "learns", "with", "direct", "extraction", "reparametrize", "space.", "navigation", "work,", "answer", "that", "suboptimal", "near-optimal", "expert", "easier", "dynamical", "learned", "useful", "effective", "non-expert", "suggested", "need", "trail", "reducing", "provide", "objective", "utilizing", "showing", "policies", "objectives", "theory", "possible", "results", "imitation", "learning),", "this", "which", "task-agnostic", "insight", "enables", "practicality", "algorithm", "benefits", "often", "evaluate", "even", "quantities", "whose", "sample-efficiency", "human", "obtain"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "243756979": {"id": "243756979", "openalex": null, "doi": null, "title": "LARGE-SCALE REPRESENTATION LEARNING ON GRAPHS VIA BOOTSTRAPPING", "abstract": "Self-supervised learning provides a promising path towards eliminating the need for costly label information in representation learning on graphs. However, to achieve state-of-the-art performance, methods often need large numbers of negative examples and rely on complex augmentations. This can be prohibitively expensive, especially for large graphs. To address these challenges, we introduce Bootstrapped Graph Latents (BGRL) -a graph representation learning method that learns by predicting alternative augmentations of the input. BGRL uses only simple augmentations and alleviates the need for contrasting with negative examples, and is thus scalable by design. BGRL outperforms or matches prior methods on several established benchmarks, while achieving a 2-10x reduction in memory costs. Furthermore, we show that BGRL can be scaled up to extremely large graphs with hundreds of millions of nodes in the semi-supervised regime -achieving state-ofthe-art performance and improving over supervised baselines where representations are shaped only through label information. In particular, our solution centered on BGRL constituted one of the winning entries to the Open Graph Benchmark -Large Scale Challenge at KDD Cup 2021, on a graph orders of magnitudes larger than all previously available benchmarks, thus demonstrating the scalability and effectiveness of our approach.", "authors": [], "concepts": ["state-of-the-art", "nodes", "achieving", "eliminating", "particular,", "entries", "challenges,", "show", "method", "available", "reduction", "path", "information", "demonstrating", "larger", "these", "large", "only", "scale", "prior", "examples", "through", "performance", "magnitudes", "however,", "constituted", "extremely", "contrasting", "while", "benchmarks,", "towards", "open", "simple", "large-scale", "graphs.", "matches", "thus", "furthermore,", "methods", "graph", "hundreds", "information.", "learning", "augmentations.", "orders", "scalability", "examples,", "performance,", "numbers", "especially", "rely", "memory", "previously", "winning", "bootstrapping", "uses", "promising", "augmentations", "self-supervised", "label", "state-ofthe-art", "costs.", "regime", "predicting", "-large", "achieve", "negative", "alleviates", "semi-supervised", "learns", "input.", "with", "several", "centered", "approach.", "design.", "benchmark", "outperforms", "that", "(bgrl)", "scaled", "alternative", "solution", "scalable", "2-10x", "graphs", "effectiveness", "need", "representations", "address", "baselines", "challenge", "introduce", "established", "where", "bootstrapped", "than", "bgrl", "over", "this", "shaped", "provides", "improving", "costly", "often", "expensive,", "prohibitively", "representation", "-achieving", "2021,", "millions", "supervised", "complex", "latents"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "244117789": {"id": "244117789", "openalex": null, "doi": null, "title": "DISTRIBUTION COMPRESSION IN NEAR-LINEAR TIME", "abstract": "In distribution compression, one aims to accurately summarize a probability distribution P using a small number of representative points. Near-optimal thinning procedures achieve this goal by sampling n points from a Markov chain and identifying \u221a n points with O(1/ \u221a n) discrepancy to P. Unfortunately, these algorithms suffer from quadratic or super-quadratic runtime in the sample size n. To address this deficiency, we introduce Compress++, a simple meta-procedure for speeding up any thinning algorithm while suffering at most a factor of 4 in error. When combined with the quadratic-time kernel halving and kernel thinning algorithms of Dwivedi and Mackey(2021), Compress++ delivers \u221a n points with O( log n/n) integration error and better-than-Monte-Carlo maximum mean discrepancy in O(n log 3 n) time and O( \u221a n log 2 n) space. Moreover, Compress++ enjoys the same near-linear runtime given any quadratic-time input and reduces the runtime of super-quadratic algorithms by a square-root factor. In our benchmarks with high-dimensional Monte Carlo samples and Markov chains targeting challenging differential equation posteriors, Compress++ matches or nearly matches the accuracy of its input algorithm in orders of magnitude less time.arXiv:2111.07941v6 [stat.ML] 18 Oct 2022Published as a conference paper at ICLR 2022Definition 2 (Sub-Gaussian thinning algorithm) For a function f , we call a thinning algorithm ALG f -sub-Gaussian with parameter \u03bd and write ALG \u2208 G f (\u03bd) ifDef. 2 is equivalent to a sub-Gaussian tail bound for the integration error, and, by Boucheron et al.  (2013, Section 2.3), if ALG \u2208 G f (\u03bd) then E[P SALG f | S in ] = P Sin f and, for all \u03b4 \u2208 (0, 1), |P Sin f \u2212P SALG f | \u2264 \u03bd(n) 2 log( 2 \u03b4 ), with probability at least 1 \u2212 \u03b4 given S in .", "authors": [], "concepts": ["challenging", "aims", "chains", "identifying", "bound", "time.arxiv:2111.07941v6", "error", "and,", "most", "compression,", "monte", "ifdef.", "-sub-gaussian", "posteriors,", "quadratic-time", "carlo", "(2013,", "these", "nearly", "number", "sampling", "from", "thinning", "deficiency,", "speeding", "using", "representative", "differential", "magnitude", "while", "function", "error,", "accuracy", "conference", "simple", "unfortunately,", "matches", "compress++,", "benchmarks", "reduces", "discrepancy", "boucheron", "high-dimensional", "section", "chain", "enjoys", "compression", "goal", "\u03bd(n)", "probability", "orders", "equivalent", "2022published", "integration", "summarize", "near-linear", "when", "points", "(sub-gaussian", "sub-gaussian", "input", "achieve", "tail", "mean", "dwivedi", "with", "given", "least", "delivers", "less", "better-than-monte-carlo", "space.", "procedures", "call", "super-quadratic", "distribution", "markov", "log(", "near-optimal", "points.", "o(1/", "maximum", "equation", "halving", "suffer", "quadratic", "salg", "compress++", "algorithm)", "size", "address", "paper", "combined", "2022definition", "kernel", "parameter", "algorithms", "2.3),", "moreover,", "suffering", "[stat.ml]", "mackey(2021),", "accurately", "introduce", "same", "then", "error.", "sample", "write", "this", "runtime", "samples", "algorithm", "factor", "factor.", "time", "n/n)", "small", "iclr", "square-root", "targeting", "meta-procedure"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "244920632": {"id": "244920632", "openalex": null, "doi": null, "title": "A CONDITIONAL POINT DIFFUSION-REFINEMENT PARADIGM FOR 3D POINT CLOUD COMPLETION", "abstract": "3D point cloud is an important 3D representation for capturing real world 3D objects. However, real-scanned 3D point clouds are often incomplete, and it is important to recover complete point clouds for downstream applications. Most existing point cloud completion methods use Chamfer Distance (CD) loss for training. The CD loss estimates correspondences between two point clouds by searching nearest neighbors, which does not capture the overall point density distribution on the generated shape, and therefore likely leads to non-uniform point cloud generation. To tackle this problem, we propose a novel Point Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The CGNet uses a conditional generative model called the denoising diffusion probabilistic model (DDPM) to generate a coarse completion conditioned on the partial observation. DDPM establishes a one-to-one pointwise mapping between the generated point cloud and the uniform ground truth, and then optimizes the mean squared error loss to realize uniform generation. The RFNet refines the coarse output of the CGNet and further improves quality of the completed point cloud. Furthermore, we develop a novel dual-path architecture for both networks. The architecture can (1) effectively and efficiently extract multi-level features from partially observed point clouds to guide completion, and (2) accurately manipulate spatial locations of 3D points to obtain smooth surfaces and sharp details. Extensive experimental results on various benchmark datasets show that our PDR paradigm outperforms previous state-of-the-art methods for point cloud completion. Remarkably, with the help of the RFNet, we can accelerate the iterative generation process of the DDPM by up to 50 times without much performance drop. * Equal Contribution.Code is released at https://github.com/ZhaoyangLyu/Point_ Diffusion_Refinement. . 3d shape generation and completion through point-voxel diffusion. arXiv preprint arXiv:2104.03670, 2021.", "authors": [], "concepts": ["state-of-the-art", "partially", "rfnet,", "(rfnet).", "https://github.com/zhaoyanglyu/point_", "arxiv:2104.03670,", "error", "denoising", "most", "nearest", "effectively", "one-to-one", "(cgnet)", "recover", "cgnet", "uniform", "tackle", "model", "show", "downstream", "neighbors,", "observed", "surfaces", "clouds", "rfnet", "overall", "complete", "observation.", "networks.", "consists", "spatial", "existing", "drop.", "does", "contribution.code", "multi-level", "much", "datasets", "through", "probabilistic", "generation.", "performance", "non-uniform", "arxiv", "times", "from", "however,", "leads", "completion.", "iterative", "generation", "objects.", "cloud.", "generative", "remarkably,", "incomplete,", "realize", "propose", "capture", "likely", "dual-path", "estimates", "conditional", "coarse", "process", "both", "furthermore,", "methods", "features", "details.", "problem,", "extract", "refines", "mapping", "pointwise", "experimental", "output", "without", "completion", "manipulate", "diffusion", "network", "uses", "distance", "capturing", "truth,", "refinement", "novel", "conditioned", "improves", "2021.", "points", "generated", "cloud", "(pdr)", "(ddpm)", "mean", "world", "with", "completion,", "point-voxel", "searching", "shape", "ground", "point", "paradigm", "benchmark", "shape,", "outperforms", "that", "distribution", "training.", "preprint", "between", "real", "sharp", "previous", "locations", "diffusion.", "guide", "ddpm", "chamfer", "squared", "called", "partial", "efficiently", "accelerate", "help", "optimizes", "completed", "results", "real-scanned", "accurately", "various", "then", "quality", "applications.", "this", "which", "(cd)", "loss", "develop", "released", "density", "diffusion_refinement.", "smooth", "often", "therefore", "important", "architecture", "establishes", "representation", "correspondences", "generate", "diffusion-refinement", "further", "equal", "obtain", "extensive"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "245117682": {"id": "245117682", "openalex": null, "doi": null, "title": "AN EXPERIMENTAL DESIGN PERSPECTIVE ON MODEL-BASED REINFORCEMENT LEARNING", "abstract": "In many practical applications of RL, it is expensive to observe state transitions from the environment. For example, in the problem of plasma control for nuclear fusion, computing the next state for a given state-action pair requires querying an expensive transition function which can lead to many hours of computer simulation or dollars of scientific research. Such expensive data collection prohibits application of standard RL algorithms which usually require a large number of observations to learn. In this work, we address the problem of efficiently learning a policy while making a minimal number of state-action queries to the transition function. In particular, we leverage ideas from Bayesian optimal experimental design to guide the selection of state-action queries for efficient learning. We propose an acquisition function that quantifies how much information a state-action pair would provide about the optimal solution to a Markov decision process. At each iteration, our algorithm maximizes this acquisition function, to choose the most informative state-action pair to be queried, thus yielding a data-efficient RL approach. We experiment with a variety of simulated continuous control problems and show that our approach learns an optimal policy with up to 5 -1, 000\u00d7 less data than modelbased RL baselines and 10 3 -10 5 \u00d7 less data than model-free RL baselines. We also provide several ablated comparisons which point to substantial improvements arising from the principled method of obtaining data.", "authors": [], "concepts": ["yielding", "improvements", "computer", "each", "informative", "most", "particular,", "problem", "arising", "expensive", "continuous", "show", "method", "lead", "learn.", "comparisons", "principled", "information", "efficient", "such", "simulated", "about", "large", "would", "nuclear", "number", "much", "from", "leverage", "example,", "quantifies", "many", "while", "function", "ideas", "requires", "propose", "optimal", "reinforcement", "selection", "learning.", "thus", "computing", "data.", "environment.", "pair", "learning", "usually", "bayesian", "experimental", "application", "ablated", "problems", "iteration,", "maximizes", "model-free", "applications", "state-action", "perspective", "decision", "minimal", "control", "transition", "acquisition", "observe", "policy", "data", "learns", "making", "with", "obtaining", "given", "design", "standard", "several", "less", "prohibits", "point", "approach.", "choose", "work,", "approach", "that", "function,", "markov", "hours", "function.", "transitions", "solution", "queried,", "querying", "guide", "data-efficient", "efficiently", "next", "plasma", "address", "baselines", "simulation", "provide", "practical", "modelbased", "algorithms", "scientific", "observations", "also", "model-based", "collection", "000\u00d7", "research.", "than", "this", "which", "state", "algorithm", "baselines.", "dollars", "fusion,", "substantial", "experiment", "process.", "require", "variety", "queries"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "245123905": {"id": "245123905", "openalex": null, "doi": null, "title": "STEP-UNROLLED DENOISING AUTOENCODERS FOR TEXT GENERATION", "abstract": "In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets. SUNDAE achieves state-of-the-art results (among non-autoregressive methods) on the WMT'14 English-to-German translation task and good qualitative results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. The non-autoregressive nature of SUNDAE opens up possibilities beyond left-to-right prompted generation, by filling in arbitrary blank patterns in a template.Autoregressive (AR) models have shown excellent results in generating text (e.g., GPT-3, Brown  et al., 2020). However, while their training scales very well, sampling is prohibitively slow for many practical applications. Moreover, there are limitations to the kinds of conditioning AR models can seamlessly handle: the left-to-right restriction makes it hard to \"fill in the gaps\" in a partially written text draft. Even more importantly, this prohibits iterative refinement of complete text drafts to make them more self-consistent, which is a common task for human writers. Finally, AR models require network architectures to be causal, severely limiting the kinds of neural network architectures that can be used for text-modeling. All of these motivated the machine learning community to make extensive efforts to propose alternatives to AR models.Machine translation (MT) was perhaps one of the first tasks where non-AR approaches were shown to seriously rival the AR-based state of the art: methods like CMLM (Ghazvininejad et al., 2019)  and DisCo (Kasai et al., 2020)  show promising results and their decoding speed is excellent compared to AR. However, while their performance is competitive, they are still behind the AR benchmark and actually require distillation of a larger AR model -without which, performance drops considerably.Non-AR methods have proven hard to apply to the general unconditional language modeling (LM) task. When there is no conditioning, the multi-modality problem becomes paramount, as shown by Gu et al. (2017), which likely makes it problematic to use methods like CMLM and DisCo because their decoding mechanism is deterministic and does not model uncertainty. Yet, recently the community has seen promising results from non-AR models like Multinomial Diffusion (Hoogeboom et al., 2021)  and D3PM (Austin et al., 2021). These methods optimize a lower bound (ELBO) on likelihoods and have shown negative log-likelihood (NLL) results approaching AR models on several benchmarks like text8 (Mahoney, 2011) and LM1B (Chelba et al., 2013). However, a major gap in NLL persists, and samples from those models lack coherence.In this paper we propose a novel non-autoregressive method which shows state-of-the-art results in machine translation on WMT'14 EN\u2192DE raw data (without distillation from AR) amongst non-AR methods and good qualitative results on unconditional language modeling on the Colossal Clean Common Crawl (C4) dataset(Raffel et al., 2019)and a dataset of Python code from GitHub. Our model operates as a time-homogeneous Markov Chain similar to that of Lee et al. (2018): conditioned on the corrupted data, it tries to approximate the original uncorrupted samples by a per-token * Shared first authorship. , et al. Language models are few-shot learners. arXiv preprint arXiv:One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005.Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020a.Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Pre-training transformers as energy-based cloze models. arXiv preprint arXiv:2012.08561, 2020b.Cyprien de Masson d'Autume, Mihaela Rosca, Jack Rae, and Shakir Mohamed. Training language gans from scratch. arXiv preprint arXiv:1905.09922, 2019.", "authors": [], "concepts": ["state-of-the-art", "partially", "machine", "each", "bound", "shows", "generation,", "abs/1312.3005,", "perhaps", "scales", "denoising", "(among", "data,", "tasks", "problem", "importantly,", "approximate", "model", "recently", "used", "them", "show", "method", "2020b.cyprien", "clark,", "dataset", "they", "translation", "filling", "complete", "modeling", "methods,", "written", "larger", "unconditional", "these", "drafts", "limitations", "proven", "does", "were", "pre-training", "general", "seen", "time-homogeneous", "well,", "text,", "2020)", "dataset(raffel", "shown", "rae,", "good", "more", "sampling", "performance", "arxiv", "from", "converges", "present", "mihaela", "however,", "cleaned", "encoders", "iterative", "generation", "clean", "brown", "(austin", "causal,", "many", "generative", "while", "word", "text-modeling.", "possibilities", "compared", "task", "applied", "gpt-3,", "very", "alternatives", "autoencoder", "simple", "2013).", "propose", "2019)", "corr,", "(ghazvininejad", "nature", "2020).", "convergence.", "excellent", "models", "likely", "there", "community", "achieves", "behind", "rival", "benchmarks", "limiting", "rather", "until", "optimize", "self-consistent,", "amongst", "because", "random", "conditioning,", "lower", "methods", "tries", "gaps\"", "chain", "finally,", "better", "christopher", "datasets.", "(elbo)", "repeatedly", "generating", "2020a.kevin", "sundae", "(2017),", "statistical", "(sundae),", "(ar)", "learning", "per-token", "measuring", "like", "non-autoregressive", "motivated", "github.", "draft.", "seriously", "operates", "d'autume,", "ar-based", "2019)and", "rely", "billion", "inputs", "becomes", "actually", "log-likelihood", "competitive,", "diffusion", "network", "promising", "decoding", "text", "mohamed.", "refinement", "novel", "conditioned", "energy-based", "slow", "shakir", "when", "code", "cmlm", "scratch.", "writers.", "drops", "approaching", "multi-modality", "persists,", "severely", "makes", "shared", "arxiv:1905.09922,", "(chelba", "data", "training", "negative", "problematic", "(mahoney,", "blank", "major", "(nll)", "neural", "en\u2192de", "restriction", "few-shot", "text8", "disco", "lm1b", "deterministic", "step-unrolled", "several", "uncertainty.", "prohibits", "gans", "wmt'14", "opens", "left-to-right", "common", "speed", "conditioning", "-without", "benchmark", "beyond", "improvement", "jack", "which,", "generators.", "task.", "that", "python", "preprint", "markov", "transformers", "their", "english-to-german", "electra:", "http://arxiv.org/abs/1312.3005.kevin", "discriminators", "arbitrary", "language", "paramount,", "2019.", "likelihoods", "those", "tokens,", "operator", "autoregressive", "autoencoders", "efforts", "natural", "sequence", "patterns", "make", "considerably.non-ar", "2013.", "approaches", "crawl", "apply", "seamlessly", "distillation", "arxiv:2012.08561,", "first", "quoc", "luong,", "(2018):", "paper", "techniques,", "lack", "art:", "architectures", "producing", "kinds", "(kasai", "practical", "modeling.", "(e.g.,", "cloze", "non-ar", "progress", "fewer", "moreover,", "starting", "results", "template.autoregressive", "corrupted", "(without", "al.,", "yet,", "handle:", "\"fill", "uncorrupted", "rosca,", "(mt)", "where", "arxiv:one", "(lm)", "than", "models.", "applications.", "methods)", "models.machine", "this", "which", "d3pm", "state", "still", "samples", "improving", "arxiv:2003.10555,", "time", "prompted", "have", "multinomial", "learners.", "qualitatively", "2021).", "original", "iterations", "prohibitively", "similar", "coherence.in", "2021)", "masson", "authorship.", "qualitative", "minh-thang", "manning.", "even", "(hoogeboom", "require", "mechanism", "hard", "human", "colossal", "(c4)", "similarly", "extensive", "2011)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "245335427": {"id": "245335427", "openalex": null, "doi": null, "title": "OBJECT PURSUIT: BUILDING A SPACE OF OBJECTS VIA DISCRIMINATIVE WEIGHT GENERATION", "abstract": "We propose a framework to continuously learn object-centric representations for visual learning and understanding. Existing object-centric representations either rely on supervisions that individualize objects in the scene, or perform unsupervised disentanglement that can hardly deal with complex scenes in the real world. To mitigate the annotation burden and relax the constraints on the statistical complexity of the data, our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations. Throughout learning, objects are streamed one by one in random order with unknown identities, and are associated with latent codes that can synthesize discriminative weights for each object through a convolutional hypernetwork. Moreover, re-identification of learned objects and forgetting prevention are employed to make the learning process efficient and robust. We perform an extensive study of the key features of the proposed framework and analyze the characteristics of the learned representations. Furthermore, we demonstrate the capability of the proposed framework in learning representations that can improve label efficiency in downstream tasks. Our code and trained models are made publicly available at: https://github.com/pptrick/ Object-Pursuit. * Equal Contribution.arXiv:2112.07954v3 [cs.CV] 3 Apr 2022Published as a conference paper at ICLR 2022We employ network weights as the object-centric representations. Specifically, the proposed method learns an object-centric representation from the data collected by manipulating a single object, through learning a latent code that can be translated into a neural network. The neural network is produced by a discriminative weight generation hypernetwork and is able to distinguish the represented object from anything else. In order to learn representations for objects that stream in one by one, the proposed framework is augmented with an object re-identification procedure to avoid learning seen objects. Moreover, we hypothesize that object representations are embedded in a low-dimensional manifold, so the proposed framework first checks whether a new object can be represented by learned objects; if not, the new object will be learned as a base object serving the purpose of representing future objects, thus the name object pursuit. Furthermore, the proposed framework deals with the catastrophic forgetting of learned object representations by enforcing the hypernetwork to maintain the mapping between the learned representations and their corresponding network weights.In summary, our work makes the following contributions: 1) we propose a novel framework named object pursuit that can continuously learn object-centric representations using training data collected from interactions with individual objects, 2) we perform an extensive study to understand the pursuit dynamics and characterize its typical behaviors regarding the key design features, and 3) we analyze the learned object space, in terms of its succinctness and effectiveness in representing objects, and empirically demonstrate its potential for label efficient visual learning. learning. arXiv preprint arXiv:1710.10368, 2017.Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In . Fearnet: Brain-inspired model for incremental learning.", "authors": [], "concepts": ["future", "learn", "demonstrate", "each", "understand", "learning,", "individualize", "stream", "associated", "hypernetwork.", "data,", "effectively", "behaviors", "model", "disentanglement", "object", "downstream", "timo", "method", "typical", "available", "supervisions", "purpose", "networks.", "name", "arxiv:1710.10368,", "either", "efficient", "checks", "features,", "will", "existing", "objects,", "procedure", "seen", "enforcing", "through", "able", "prevention", "arxiv", "from", "interactions", "into", "world.", "order", "objects;", "using", "contributions:", "convolutional", "generation", "objects.", "samuli", "generative", "while", "object-centric", "produced", "signals", "conference", "diverse", "representations.", "mitigate", "objects", "pursuit:", "framework", "propose", "pursuit", "codes", "models", "learning.", "thus", "process", "forgetting", "deal", "complexity", "furthermore,", "random", "work", "features", "made", "avoid", "terms", "laine,", "robust.", "relax", "statistical", "mapping", "learning", "improve", "characterize", "brain-inspired", "leverages", "characteristics", "aila.", "low-dimensional", "discriminative", "embedded", "karras,", "rely", "fearnet:", "latent", "space", "2022published", "network", "throughout", "specifically,", "hardly", "constraints", "understanding.", "novel", "style-based", "code", "label", "synthesize", "serving", "burden", "maintain", "corresponding", "https://github.com/pptrick/", "deals", "individual", "makes", "potential", "data", "representing", "object,", "training", "employ", "tasks.", "re-identification", "neural", "learns", "anything", "with", "not,", "space,", "design", "network.", "efficiency", "trained", "whether", "variations", "that", "preprint", "between", "real", "employed", "their", "base", "contribution.arxiv:2112.07954v3", "weights", "catastrophic", "analyze", "collected", "weight", "manifold,", "scenes", "regarding", "learned", "scene,", "augmented", "distinguish", "make", "effectiveness", "representations", "identities,", "empirically", "first", "manipulating", "paper", "hypothesize", "translated", "2022we", "unknown", "unsupervised", "building", "following", "continuously", "perform", "moreover,", "succinctness", "sample", "dynamics", "annotation", "publicly", "pursuit.", "streamed", "else.", "adversarial", "generator", "proposed", "iclr", "visual", "named", "[cs.cv]", "represented", "capability", "architecture", "incremental", "representation", "study", "object-pursuit.", "2017.tero", "summary,", "one,", "hypernetwork", "single", "equal", "complex", "extensive", "weights.in"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "245906072": {"id": "245906072", "openalex": null, "doi": null, "title": "Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks", "abstract": "We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow. We show that in the underparameterized regime the network learns eigenfunctions of an integral operator TK\u221e determined by the Neural Tangent Kernel (NTK) at rates corresponding to their eigenvalues. For example, for uniformly distributed data on the sphere S d\u22121 and rotation invariant weight distributions, the eigenfunctions of TK\u221e are the spherical harmonics. Our results can be understood as describing a spectral bias in the underparameterized regime. The proofs use the concept of \"Damped Deviations\", where deviations of the NTK matter less for eigendirections with large eigenvalues due to the occurence of a damping factor. Aside from the underparameterized regime, the damped deviations point-of-view can be used to track the dynamics of the empirical risk in the overparameterized setting, allowing us to extend certain results in the literature. We conclude that damped deviations offers a simple and unifying perspective of the dynamics when optimizing the squared error.", "authors": [], "concepts": ["eigenvalues.", "error", "used", "understood", "show", "describing", "concept", "conclude", "certain", "large", "occurence", "setting,", "empirical", "literature.", "from", "optimizing", "example,", "offers", "function", "regime,", "rotation", "rates", "simple", "invariant", "spectral", "networks", "flow.", "tangent", "unifying", "(ntk)", "harmonics.", "gradient", "integral", "perspective", "space", "spherical", "network", "\"damped", "when", "corresponding", "regime", "data", "mean", "neural", "learns", "with", "allowing", "implicit", "less", "bias", "distributions,", "regime.", "deviations", "that", "determined", "their", "matter", "weight", "operator", "damping", "eigenvalues", "eigendirections", "extend", "squared", "track", "kernel", "results", "point-of-view", "aside", "uniformly", "where", "error.", "eigenfunctions", "dynamics", "overparameterized", "deviations\",", "risk", "factor.", "proofs", "optimization", "distributed", "underparameterized", "study", "sphere", "damped"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "246294808": {"id": "246294808", "openalex": null, "doi": null, "title": "ON THE CONVERGENCE OF MSGD AND ADAGRAD FOR STOCHASTIC OPTIMIZATION", "abstract": "As one of the most fundamental stochastic optimization algorithms, stochastic gradient descent (SGD) has been intensively developed and extensively applied in machine learning in the past decade. There have been some modified SGD-type algorithms, which outperform the SGD in many competitions and applications in terms of convergence rate and accuracy, such as momentum-based SGD (mSGD) and adaptive gradient algorithm (AdaGrad). Despite these empirical successes, the theoretical properties of these algorithms have not been well established due to technical difficulties. With this motivation, we focus on convergence analysis of mSGD and AdaGrad for any smooth (possibly non-convex) loss functions in stochastic optimization. First, we prove that the iterates of mSGD are asymptotically convergent to a connected set of stationary points with probability one, which is more general than existing works on subsequence convergence or convergence of time averages. Moreover, we prove that the loss function of mSGD decays at a certain rate faster than that of SGD. In addition, we prove the iterates of AdaGrad are asymptotically convergent to a connected set of stationary points with probability one. Also, this result extends the results from the literature on subsequence convergence and the convergence of time averages. Despite the generality of the above convergence results, we have relaxed some assumptions of gradient noises, convexity of loss functions, as well as boundedness of iterates.", "authors": [], "concepts": ["relaxed", "asymptotically", "machine", "also,", "outperform", "developed", "most", "theoretical", "(adagrad).", "rate", "literature", "convergent", "successes,", "faster", "such", "certain", "these", "existing", "extensively", "despite", "been", "general", "empirical", "adaptive", "more", "decays", "from", "convexity", "well", "many", "function", "technical", "fundamental", "applied", "algorithms,", "convergence", "there", "averages.", "motivation,", "some", "extends", "adagrad", "terms", "stationary", "learning", "difficulties.", "probability", "iterates", "gradient", "functions,", "sgd.", "first,", "applications", "(possibly", "prove", "competitions", "(sgd)", "analysis", "modified", "msgd", "points", "past", "addition,", "accuracy,", "works", "non-convex)", "with", "noises,", "generality", "assumptions", "that", "functions", "optimization.", "momentum-based", "subsequence", "connected", "result", "focus", "algorithms", "moreover,", "results", "sgd-type", "established", "than", "boundedness", "this", "which", "one.", "above", "results,", "algorithm", "loss", "time", "(msgd)", "have", "smooth", "intensively", "decade.", "optimization", "properties", "one,", "iterates.", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "246430476": {"id": "246430476", "openalex": null, "doi": null, "title": "THE KFIOU LOSS FOR ROTATED OBJECT DETECTION", "abstract": "Differing from the well-developed horizontal object detection area whereby the computing-friendly IoU based loss is readily adopted and well fits with the detection metrics. In contrast, rotation detectors often involve a more complicated loss based on SkewIoU which is unfriendly to gradient-based training. In this paper, we propose an effective approximate SkewIoU loss based on Gaussian modeling and Gaussian product, which mainly consists of two items. The first term is a scale-insensitive center point loss, which is used to quickly narrow the distance between the center points of the two bounding boxes. In the distance-independent second term, the product of the Gaussian distributions is adopted to inherently mimic the mechanism of SkewIoU by its definition, and show its alignment with the SkewIoU loss at trend-level within a certain distance (i.e. within 9 pixels). This is in contrast to recent Gaussian modeling based rotation detectors e.g. GWD loss and KLD loss that involve a human-specified distribution distance metric which require additional hyperparameter tuning that vary across datasets and detectors. The resulting new loss called KFIoU loss is easier to implement and works better compared with exact SkewIoU loss, thanks to its full differentiability and ability to handle the non-overlapping cases. We further extend our technique to the 3-D case which also suffers from the same issues as 2-D. Extensive results on various public datasets (2-D/3-D, aerial/text/face images) with different base detectors show the effectiveness of our approach.Published as a conference paper at ICLR 20231 See an open-source version with thousands of lines of code for implementing the loss at https:// github.com/open-mmlab/mmcv/pull/1854, while our new loss only costs tens of lines of code. 2  The product of the Gaussian distributions is an important procedure in Kalman filtering. Inspired by Kalman filtering, we mark the proposed loss as KFIoU loss.", "authors": [], "concepts": ["case", "kfiou", "metric", "paper,", "term", "mainly", "approximate", "filtering.", "used", "object", "version", "show", "lines", "recent", "code.", "readily", "resulting", "within", "inspired", "inherently", "term,", "modeling", "metrics.", "consists", "certain", "only", "procedure", "datasets", "contrast", "more", "from", "mark", "github.com/open-mmlab/mmcv/pull/1854,", "well", "tens", "while", "whereby", "conference", "items.", "compared", "rotation", "detectors", "propose", "distance-independent", "human-specified", "full", "narrow", "scale-insensitive", "cases.", "mimic", "better", "loss,", "differentiability", "quickly", "well-developed", "differing", "additional", "gaussian", "ability", "suffers", "filtering,", "thanks", "computing-friendly", "trend-level", "distance", "boxes.", "kalman", "2-d.", "based", "code", "points", "complicated", "works", "across", "hyperparameter", "with", "point", "exact", "tuning", "adopted", "detection", "implement", "detectors.", "non-overlapping", "center", "unfriendly", "that", "distribution", "training.", "between", "base", "vary", "skewiou", "easier", "handle", "involve", "(2-d/3-d,", "different", "aerial/text/face", "distributions", "extend", "effective", "contrast,", "effectiveness", "horizontal", "technique", "fits", "called", "first", "paper", "gradient-based", "https://", "product,", "thousands", "results", "also", "second", "bounding", "various", "same", "rotated", "open-source", "costs", "this", "which", "pixels).", "product", "area", "loss.", "issues", "implementing", "loss", "alignment", "definition,", "often", "proposed", "iclr", "important", "require", "approach.published", "mechanism", "(i.e.", "images)", "further", "extensive", "public", "e.g."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "246431014": {"id": "246431014", "openalex": null, "doi": null, "title": "LEARNING SUPER-FEATURES FOR IMAGE RETRIEVAL", "abstract": "Methods that combine local and global features have recently shown excellent performance on multiple challenging deep image retrieval benchmarks, but their use of local features raises at least two issues. First, these local features simply boil down to the localized map activations of a neural network, and hence can be extremely redundant. Second, they are typically trained with a global loss that only acts on top of an aggregation of local features; by contrast, testing is based on local feature matching, which creates a discrepancy between training and testing. In this paper, we propose a novel architecture for deep image retrieval, based solely on mid-level features that we call Super-features. These Super-features are constructed by an iterative attention module and constitute an ordered set in which each element focuses on a localized and discriminant image pattern. For training, they require only image labels. A contrastive loss operates directly at the level of Super-features and focuses on those that match across images. A second complementary loss encourages diversity. Experiments on common landmark retrieval benchmarks validate that Super-features substantially outperform state-of-the-art methods when using the same number of features, and only require a significantly smaller memory footprint to match their performance. Code and models are available at: https://github.com/naver/FIRe.arXiv:2201.13182v1 [cs.CV] 31 Jan 2022Published as a conference paper at ICLR 2022 1 The term 'query' has a precise meaning for retrieval; yet, for this subsection only, we overload the term to refer to one of the inputs of the dot-product attention, consistently with the terminology from seminal works on attention byVaswani et al. (2017). 2  The attention maps presented in Eq.(4) are technically taken at iteration t, but we omit iteration superscripts for clarity. For the rest of the paper and visualizations, we use attention maps to refer to the attention maps of Eq.(4) after the final (T -th) iteration of the iterative module. 3  The MLP function consists of a layer-norm, a fully-connected layer with half the dimensions of the features, a ReLU activation and a fully-connected layer that projects features back to their initial dimension.", "authors": [], "concepts": ["smaller", "state-of-the-art", "challenging", "each", "typically", "paper,", "term", "diversity.", "outperform", "focuses", "layer-norm,", "recently", "activations", "available", "they", "consists", "features,", "these", "mid-level", "only", "feature", "fully-connected", "footprint", "multiple", "number", "shown", "performance.", "element", "performance", "from", "deep", "rest", "extremely", "using", "iterative", "relu", "contrastive", "function", "conference", "meaning", "benchmarks,", "global", "propose", "excellent", "models", "labels.", "(2017).", "iteration", "benchmarks", "https://github.com/naver/fire.arxiv:2201.13182v1", "seminal", "discrepancy", "back", "final", "methods", "features", "presented", "acts", "issues.", "learning", "super-features.", "activation", "substantially", "significantly", "dimensions", "first,", "operates", "only,", "overload", "redundant.", "local", "localized", "memory", "attention,", "hence", "visualizations,", "experiments", "inputs", "2022published", "creates", "novel", "testing.", "constructed", "when", "based", "code", "subsection", "image", "dimension.", "dot-product", "second,", "training", "validate", "neural", "works", "across", "with", "precise", "least", "refer", "retrieval,", "trained", "call", "common", "eq.(4)", "maps", "training,", "aggregation", "that", "landmark", "solely", "technically", "between", "complementary", "their", "testing", "down", "encourages", "features;", "initial", "those", "terminology", "network,", "clarity.", "module", "contrast,", "byvaswani", "attention", "-th)", "simply", "superscripts", "paper", "match", "retrieval", "combine", "layer", "yet,", "images.", "second", "half", "same", "matching,", "consistently", "this", "which", "projects", "omit", "super-features", "discriminant", "ordered", "after", "taken", "module.", "loss", "raises", "have", "directly", "iclr", "[cs.cv]", "retrieval;", "architecture", "level", "require", "constitute", "boil", "pattern.", "'query'"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "246634167": {"id": "246634167", "openalex": null, "doi": null, "title": "DISTRIBUTIONALLY ROBUST FAIR PRINCIPAL COMPONENTS VIA GEODESIC DESCENTS", "abstract": "Principal component analysis is a simple yet useful dimensionality reduction technique in modern machine learning pipelines. In consequential domains such as college admission, healthcare and credit approval, it is imperative to take into account emerging criteria such as the fairness and the robustness of the learned projection. In this paper, we propose a distributionally robust optimization problem for principal component analysis which internalizes a fairness criterion in the objective function. The learned projection thus balances the trade-off between the total reconstruction error and the reconstruction error gap between subgroups, taken in the min-max sense over all distributions in a moment-based ambiguity set. The resulting optimization problem over the Stiefel manifold can be efficiently solved by a Riemannian subgradient descent algorithm with a sub-linear convergence rate. Our experimental results on real-world datasets show the merits of our proposed method over state-of-the-art baselines.", "authors": [], "concepts": ["state-of-the-art", "solved", "machine", "ambiguity", "component", "paper,", "error", "imperative", "total", "robust", "problem", "internalizes", "show", "method", "reduction", "approval,", "resulting", "balances", "such", "projection.", "sub-linear", "datasets", "into", "admission,", "min-max", "criterion", "emerging", "moment-based", "account", "simple", "propose", "convergence", "riemannian", "thus", "sense", "learning", "experimental", "reconstruction", "consequential", "manifold", "merits", "fairness", "dimensionality", "healthcare", "analysis", "rate.", "real-world", "subgradient", "criteria", "projection", "with", "distributionally", "components", "college", "domains", "stiefel", "geodesic", "between", "function.", "robustness", "take", "learned", "useful", "distributions", "technique", "efficiently", "trade-off", "set.", "objective", "principal", "results", "subgroups,", "over", "this", "which", "algorithm", "taken", "baselines.", "proposed", "optimization", "fair", "descents", "pipelines.", "credit", "modern", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "246634506": {"id": "246634506", "openalex": null, "doi": null, "title": "HANDLING DISTRIBUTION SHIFTS ON GRAPHS: AN INVARIANCE PERSPECTIVE", "abstract": "There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given the two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem for node-level prediction on graphs and develop a new domain-invariant learning approach, named Explore-to-Extrapolate Risk Minimization, that facilitates GNNs to leverage invariant graph features for prediction. The key difference to existing invariant models is that we design multiple context explorers (specified as graph editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution.Recent studies of the OOD generalization problem likeRojas-Carulla et al. (2018); B\u00fchlmann (2018);", "authors": [], "concepts": ["case", "nodes", "demonstrate", "editers", "paper,", "informative", "studies", "data,", "problem", "graph-structured", "spurious", "approach,", "model", "invariance", "comes", "method", "observed", "information", "such", "features,", "gnns", "existing", "out-of-distribution", "difference", "artificial", "multiple", "(2018);", "datasets", "from", "leverage", "into", "suggesting", "clear", "evolution.recent", "induces", "generation", "case)", "risks", "fundamental", "invariant", "minimization,", "models", "there", "generalization", "spotlight.", "environment,", "current", "adversarially", "networks'", "graph", "(ood)", "features", "among", "graphs:", "learning", "validity", "(specified", "structural", "prove", "euclidean", "perspective", "inter-connection", "sensitivity", "valid", "environment", "non-iid", "real-world", "points", "data", "input", "context", "under-explored,", "theoretically", "node-level", "shifts", "neural", "cross-domain", "transfers", "likerojas-carulla", "environments.", "given", "design", "challenges:", "variance", "maximize", "trained", "dynamic", "common", "b\u00fchlmann", "that", "distribution", "domain-invariant", "solution", "facilitates", "two-fold", "prediction", "explorers", "graphs", "extrapolate", "nonetheless,", "focus", "research", "showing", "formulation", "increasing", "prediction.", "also", "shifts,", "explore-to-extrapolate", "various", "same", "formulate", "mostly", "power", "this", "which", "evidence", "enables", "risk", "develop", "endeavors", "named", "graph,", "under", "even", "virtual", "remains", "handling", "guarantee", "further", "single"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "246822414": {"id": "246822414", "openalex": null, "doi": null, "title": "UNSUPERVISED DISENTANGLEMENT WITH TENSOR PRODUCT REPRESENTATIONS ON THE TORUS", "abstract": "The current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations. In this work, we propose to employ a tensor product structure for this purpose. This way, the obtained representations are naturally disentangled. In contrast to the conventional variations methods, which are targeted toward normally distributed features, the latent space in our representation is distributed uniformly over a set of unit circles. We argue that the torus structure of the latent space captures the generative factors effectively. We employ recent tools for measuring unsupervised disentanglement, and in an extensive set of experiments demonstrate the advantage of our method in terms of disentanglement, completeness, and informativeness. The code for our proposed method is available at https://github.com/rotmanmi/ Unsupervised-Disentanglement-Torus.", "authors": [], "concepts": ["way,", "demonstrate", "captures", "disentanglement", "targeted", "method", "available", "recent", "methods,", "conventional", "features,", "unsupervised-disentanglement-torus.", "contrast", "generative", "structure", "representations.", "tensor", "propose", "factors", "disentangled.", "current", "advantage", "purpose.", "methods", "terms", "effectively.", "learning", "naturally", "disentanglement,", "measuring", "exclusively", "toward", "informativeness.", "latent", "experiments", "space", "auto-encoders", "argue", "code", "employ", "obtained", "with", "variations", "work,", "that", "tools", "representations", "normally", "unsupervised", "completeness,", "uniformly", "almost", "over", "vectors", "this", "which", "circles.", "product", "unit", "proposed", "distributed", "representation", "extensive", "https://github.com/rotmanmi/", "torus"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247058853": {"id": "247058853", "openalex": null, "doi": null, "title": "LEARNING FAST AND SLOW FOR ONLINE TIME SERIES FORECASTING", "abstract": "Despite the recent success of deep learning for time series forecasting, these methods are not scalable for many real-world applications where data arrives sequentially. Training deep neural forecasters on the fly is notoriously challenging because of their limited ability to adapt to non-stationary environments and remember old knowledge. We argue that the fast adaptation capability of deep neural networks is critical and successful solutions require handling changes to both new and recurring patterns effectively. In this work, inspired by the Complementary Learning Systems (CLS) theory, we propose Fast and Slow learning Network (FS-Net) as a novel framework to address the challenges of online forecasting. Particularly, FSNet improves the slowly-learned backbone by dynamically balancing fast adaptation to recent changes and retrieving similar old knowledge. FSNet achieves this mechanism via an interaction between two novel complementary components: (i) a per-layer adapter to support fast learning from individual layers, and (ii) an associative memory to support remembering, updating, and recalling repeating events. Extensive experiments on real and synthetic datasets validate FSNet's efficacy and robustness to both new and recurring patterns. Our code is available at https://github.com/salesforce/fsnet.", "authors": [], "concepts": ["environments", "retrieving", "challenging", "solutions", "(fs-net)", "slowly-learned", "particularly,", "available", "recent", "non-stationary", "inspired", "recurring", "these", "despite", "(ii)", "fast", "datasets", "from", "deep", "dynamically", "many", "fsnet's", "framework", "propose", "adapter", "achieves", "critical", "networks", "both", "because", "backbone", "methods", "effectively.", "forecasting", "support", "learning", "updating,", "successful", "online", "ability", "arrives", "associative", "applications", "interaction", "repeating", "components:", "memory", "experiments", "forecasting,", "argue", "network", "changes", "challenges", "success", "events.", "novel", "slow", "code", "notoriously", "real-world", "improves", "individual", "data", "series", "training", "validate", "neural", "remember", "systems", "adaptation", "work,", "(cls)", "sequentially.", "that", "balancing", "between", "complementary", "real", "their", "recalling", "robustness", "scalable", "forecasting.", "fsnet", "patterns", "layers,", "address", "remembering,", "synthetic", "patterns.", "where", "this", "adapt", "efficacy", "time", "per-layer", "knowledge.", "theory,", "forecasters", "limited", "similar", "capability", "require", "mechanism", "https://github.com/salesforce/fsnet.", "handling", "extensive"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247222761": {"id": "247222761", "openalex": null, "doi": null, "title": "CONTINUOUS-TIME META-LEARNING WITH FORWARD MODE DIFFERENTIATION", "abstract": "Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a taskspecific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems. . Modular Meta-Learning with Shrinkage. Neural Information Processing Systems, 2020b.John R Dormand and Peter J Prince. A family of embedded Runge-Kutta formulae. Journal of computational and applied mathematics, 1980.Leonhard Euler. De integratione aequationum differentialium per approximationem. Opera Omnia, 1913.William F Feehery, John E Tolsma, and Paul I Barton. Efficient sensitivity analysis of large-scale differential-algebraic systems.", "authors": [], "concepts": ["opera", "differentiation", "(comln),", "problems.", "importantly,", "constant", "meta-learning", "aequationum", "addition", "show", "1913.william", "devise", "systems.", "vector", "information", "efficient", "such", "follows", "scale", "analytical", "field.", "number", "memory.", "classification", "forward", "from", "dormand", "order", "using", "meta-gradients", "differential", "omnia,", "formulae.", "offers", "classifier", "usage,", "trajectory,", "differentiation,", "task", "applied", "feehery,", "large-scale", "notable", "thus", "process", "solve", "advantage", "outer-loop", "optimize", "conditions", "methods", "taskspecific", "terms", "paul", "runge-kutta", "linear", "learning", "opposed", "prince.", "tolsma,", "gradient", "inspiration", "meta-learning.", "embedded", "meta-learned", "memory", "sensitivity", "inputs", "longer", "family", "stability", "differential-algebraic", "specifically,", "analysis", "based", "necessary", "image", "ordinary", "obtained", "treating", "neural", "with", "allowing", "systems,", "differentialium", "few-shot", "discrete", "standard", "exact", "efficiency", "drawing", "mode", "illustrate", "john", "continuous,", "adaptation", "that", "trajectory", "1980.leonhard", "fixed", "required", "solution", "integratione", "continuous-time", "initial", "steps.", "equation", "processing", "effectiveness", "mathematics,", "representations", "infinitely", "range", "empirically", "barton.", "approximationem.", "gradient-based", "provide", "2020b.john", "amount", "computational", "descent,", "shrinkage.", "requirements", "introduce", "where", "dynamics", "comln,", "peter", "runtime", "steps,", "algorithm", "length", "small", "updates,", "practice", "euler.", "(ode).", "whose", "guarantees", "journal", "compute", "modular", "consequence,", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247222973": {"id": "247222973", "openalex": null, "doi": null, "title": "Min-Max Multi-objective Bilevel Optimization with Applications in Robust Machine Learning", "abstract": "We consider a generic min-max multi-objective bilevel optimization problem with applications in robust machine learning such as representation learning and hyperparameter optimization. We design MORBiT, a novel single-loop gradient descent-ascent bilevel optimization algorithm, to solve the generic problem and present a novel analysis showing that MORBiT converges to the first-order stationary point at a rate of O(n 1 /2 K \u22122 /5 ) for a class of weakly convex problems with n objectives upon K iterations of the algorithm. Our analysis utilizes novel results to handle the non-smooth min-max multi-objective setup and to obtain a sublinear dependence in the number of objectives n. Experimental results on robust representation learning and robust hyperparameter optimization showcase (i) the advantages of considering the min-max multi-objective setup, and (ii) convergence properties of the proposed MORBiT. Our code is at https://github.com/minimario/MORBiT.", "authors": [], "concepts": ["weakly", "machine", "https://github.com/minimario/morbit.", "robust", "upon", "problem", "non-smooth", "rate", "single-loop", "such", "(ii)", "number", "converges", "present", "min-max", "consider", "convergence", "convex", "solve", "algorithm.", "stationary", "learning", "experimental", "showcase", "problems", "gradient", "morbit,", "applications", "dependence", "first-order", "analysis", "novel", "code", "class", "hyperparameter", "with", "setup,", "design", "point", "that", "optimization.", "bilevel", "setup", "algorithm,", "handle", "morbit.", "showing", "considering", "objectives", "results", "generic", "descent-ascent", "morbit", "advantages", "utilizes", "proposed", "optimization", "multi-objective", "sublinear", "iterations", "representation", "properties", "obtain"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247292326": {"id": "247292326", "openalex": null, "doi": null, "title": "ACCELERATION OF FEDERATED LEARNING WITH ALLEVIATED FORGETTING IN LOCAL TRAINING", "abstract": "Federated learning (FL) enables distributed optimization of machine learning models while protecting privacy by independently training local models on each client and then aggregating parameters on a central server, thereby producing an effective global model. Although a variety of FL algorithms have been proposed, their training efficiency remains low when the data are not independently and identically distributed (non-i.i.d.) across different clients. We observe that the slow convergence rates of the existing methods are (at least partially) caused by the catastrophic forgetting issue during the local training stage on each individual client, which leads to a large increase in the loss function concerning the previous training data at the other clients. Here, we propose FedReg, an algorithm to accelerate FL with alleviated knowledge forgetting in the local training stage by regularizing locally trained parameters with the loss on generated pseudo data, which encode the knowledge of previous training data learned by the global model. Our comprehensive experiments demonstrate that FedReg not only significantly improves the convergence rate of FL, especially when the neural network architecture is deep and the clients' data are extremely non-i.i.d., but is also able to protect privacy better in classification problems and more robust against gradient inversion attacks. The code is available at: https://github.com/Zoesgithub/FedReg. * Minlie Huang and Tao Jiang are the co-corresponding authors.", "authors": [], "concepts": ["demonstrate", "machine", "each", "parameters", "here,", "data,", "robust", "rate", "https://github.com/zoesgithub/fedreg.", "available", "authors.", "locally", "central", "huang", "alleviated", "jiang", "existing", "large", "only", "concerning", "been", "protect", "proposed,", "classification", "increase", "able", "more", "stage", "deep", "federated", "leads", "extremely", "pseudo", "while", "function", "rates", "comprehensive", "global", "propose", "issue", "convergence", "models", "forgetting", "identically", "(non-i.i.d.)", "fedreg", "methods", "clients.", "better", "learning", "problems", "gradient", "significantly", "especially", "local", "other", "experiments", "network", "co-corresponding", "slow", "when", "code", "improves", "observe", "individual", "generated", "data", "training", "partially)", "inversion", "although", "neural", "non-i.i.d.,", "across", "with", "least", "acceleration", "efficiency", "trained", "that", "their", "knowledge", "catastrophic", "previous", "different", "learned", "model.", "effective", "minlie", "(fl)", "independently", "accelerate", "producing", "privacy", "algorithms", "client,", "also", "then", "encode", "which", "protecting", "aggregating", "during", "enables", "algorithm", "loss", "clients'", "have", "regularizing", "optimization", "fedreg,", "client", "caused", "architecture", "distributed", "thereby", "variety", "attacks.", "remains", "against", "server,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247450846": {"id": "247450846", "openalex": null, "doi": null, "title": "SURROGATE GAP MINIMIZATION IMPROVES SHARPNESS-AWARE TRAINING", "abstract": "The recently proposed Sharpness-Aware Minimization (SAM) improves generalization by minimizing a perturbed loss defined as the maximum loss within a neighborhood in the parameter space. However, we show that both sharp and flat minima can have a low perturbed loss, implying that SAM does not always prefer flat minima. Instead, we define a surrogate gap, a measure equivalent to the dominant eigenvalue of Hessian at a local minimum when the radius of neighborhood (to derive the perturbed loss) is small. The surrogate gap is easy to compute and feasible for direct minimization during training. Based on the above observations, we propose Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), a novel improvement over SAM with negligible computation overhead. Conceptually, GSAM consists of two steps: 1) a gradient descent like SAM to minimize the perturbed loss, and 2) an ascent step in the orthogonal direction (after gradient decomposition) to minimize the surrogate gap and yet not affect the perturbed loss. GSAM seeks a region with both small loss (by step 1) and low sharpness (by step 2), giving rise to a model with high generalization capabilities. Theoretically, we show the convergence of GSAM and provably better generalization than SAM. Empirically, GSAM consistently improves generalization (e.g., +3.2% over SAM and +5.4% over AdamW on ImageNet top-1 accuracy for ViT-B/32). Code is released at https", "authors": [], "concepts": ["loss)", "minimizing", "sam.", "guided", "model", "recently", "show", "within", "adamw", "consists", "empirically,", "does", "minimize", "radius", "however,", "eigenvalue", "hessian", "accuracy", "derive", "measure", "propose", "convergence", "generalization", "orthogonal", "affect", "both", "neighborhood", "better", "loss,", "prefer", "direction", "like", "perturbed", "(gsam),", "vit-b/32).", "gradient", "equivalent", "theoretically,", "overhead.", "local", "high", "(after", "top-1", "novel", "when", "define", "based", "code", "step", "improves", "rise", "easy", "dominant", "steps:", "ascent", "training", "provably", "sharpness", "with", "direct", "negligible", "space.", "improvement", "region", "that", "capabilities.", "training.", "https", "imagenet", "giving", "feasible", "sharp", "minimization", "maximum", "flat", "observations,", "gsam", "always", "seeks", "minima.", "small.", "parameter", "(e.g.,", "sharpness-aware", "conceptually,", "(sam)", "than", "over", "consistently", "implying", "decomposition)", "during", "gap,", "loss.", "above", "instead,", "loss", "defined", "released", "have", "computation", "small", "proposed", "+3.2%", "minima", "surrogate", "minimum", "+5.4%", "compute", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247518687": {"id": "247518687", "openalex": null, "doi": null, "title": "ON THE PITFALLS OF HETEROSCEDASTIC UNCERTAINTY ESTIMATION WITH PROBABILISTIC NEURAL NETWORKS", "abstract": "Capturing aleatoric uncertainty is a critical part of many machine learning systems. In deep learning, a common approach to this end is to train a neural network to estimate the parameters of a heteroscedastic Gaussian distribution by maximizing the logarithm of the likelihood function under the observed data. In this work, we examine this approach and identify potential hazards associated with the use of log-likelihood in conjunction with gradient-based optimizers. First, we present a synthetic example illustrating how this approach can lead to very poor but stable parameter estimates. Second, we identify the culprit to be the log-likelihood loss, along with certain conditions that exacerbate the issue. Third, we present an alternative formulation, termed \u03b2\u2212NLL, in which each data point's contribution to the loss is weighted by the \u03b2-exponentiated variance estimate. We show that using an appropriate \u03b2 largely mitigates the issue in our illustrative example. Fourth, we evaluate this approach on a range of domains and tasks and show that it achieves considerable improvements and performs more robustly concerning hyperparameters, both in predictive RMSE and log-likelihood criteria. arXiv:2203.09168v2 [cs.LG] 1 Apr 2022", "authors": [], "concepts": ["improvements", "machine", "each", "parameters", "learning,", "uncertainty", "fourth,", "associated", "performs", "tasks", "third,", "example.", "show", "observed", "lead", "systems.", "hazards", "issue.", "certain", "concerning", "exacerbate", "examine", "probabilistic", "more", "robustly", "part", "present", "deep", "using", "estimate", "along", "many", "function", "predictive", "\u03b2-exponentiated", "very", "issue", "achieves", "critical", "networks", "maximizing", "data.", "both", "conditions", "point's", "illustrating", "loss,", "conjunction", "learning", "contribution", "rmse", "example", "\u03b2\u2212nll,", "gaussian", "first,", "considerable", "estimation", "stable", "log-likelihood", "network", "capturing", "poor", "weighted", "potential", "data", "second,", "neural", "with", "appropriate", "train", "identify", "variance", "common", "work,", "approach", "domains", "that", "distribution", "criteria.", "alternative", "optimizers.", "culprit", "termed", "formulation,", "range", "arxiv:2203.09168v2", "pitfalls", "illustrative", "estimates.", "gradient-based", "parameter", "estimate.", "[cs.lg]", "synthetic", "logarithm", "this", "which", "loss", "largely", "aleatoric", "hyperparameters,", "likelihood", "evaluate", "under", "heteroscedastic", "mitigates"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247570285": {"id": "247570285", "openalex": null, "doi": null, "title": "DISTRIBUTIONALLY ROBUST MODELS WITH PARAMETRIC LIKELIHOOD RATIOS", "abstract": "As machine learning models are deployed ever more broadly, it becomes increasingly important that they are not only able to perform well on their training distribution, but also yield accurate predictions when confronted with distribution shift. The Distributionally Robust Optimization (DRO) framework proposes to address this issue by training models to minimize their expected risk under a collection of distributions, to imitate test-time shifts. This is most commonly achieved by instance-level re-weighting of the training objective to emulate the likelihood ratio with possible test distributions, which allows for estimating their empirical risk via importance sampling (assuming that they are subpopulations of the training distribution). However, re-weighting schemes in the literature are usually limited due to the difficulty of keeping the optimization problem tractable and the complexity of enforcing normalization constraints. In this paper, we show that three simple ideas -mini-batch level normalization, a KL penalty and simultaneous gradient updates -allow us to train models with DRO using a broader class of parametric likelihood ratios. In a series of experiments on both image and text classification benchmarks, we find that models trained with the resulting parametric adversaries are consistently more robust to subpopulation shifts when compared to other DRO approaches, and that the method performs reliably well with little hyper-parameter tuning. 1", "authors": [], "concepts": ["machine", "paper,", "accurate", "-allow", "performs", "ratios", "most", "robust", "problem", "show", "method", "literature", "they", "subpopulations", "resulting", "test", "proposes", "achieved", "parametric", "only", "updates", "instance-level", "enforcing", "empirical", "minimize", "classification", "able", "broadly,", "expected", "more", "sampling", "broader", "shifts.", "however,", "using", "(dro)", "well", "schemes", "benchmarks,", "compared", "ideas", "simple", "framework", "issue", "increasingly", "deployed", "models", "re-weighting", "complexity", "both", "confronted", "learning", "normalization,", "usually", "gradient", "simultaneous", "approaches,", "other", "experiments", "becomes", "yield", "importance", "text", "when", "class", "image", "test-time", "series", "training", "difficulty", "shifts", "with", "ratios.", "train", "predictions", "distributionally", "distributions,", "trained", "emulate", "shift.", "that", "distribution", "their", "-mini-batch", "normalization", "tuning.", "distribution,", "adversaries", "(assuming", "distribution).", "address", "subpopulation", "allows", "objective", "three", "hyper-parameter", "find", "constraints.", "tractable", "possible", "perform", "commonly", "also", "ever", "ratio", "collection", "imitate", "consistently", "this", "which", "risk", "penalty", "little", "optimization", "likelihood", "estimating", "limited", "under", "important", "level", "reliably", "keeping"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247595263": {"id": "247595263", "openalex": null, "doi": null, "title": "SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS", "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).", "authors": [], "concepts": ["striking", "typically", "paper,", "shows", "margin", "encouraging", "arithmetic", "most", "problem", "used", "strategyqa", "ways", "consistent", "achieved", "large", "only", "correct", "thinking", "multiple", "prompting", "empirical", "performance", "(+6.4%)", "self-consistency,", "marginalizing", "diverse", "benchmarks,", "propose", "greedy", "models", "self-consistency", "paths.", "popular", "unique", "chain", "arc-challenge", "(+17.9%),", "leverages", "(+11.0%),", "naive", "intuition", "decoding", "thought", "admits", "improves", "(+12.2%),", "tasks.", "reasoning", "commonsense", "with", "chain-of-thought", "including", "answer", "that", "instead", "boosts", "prompting.", "language", "different", "svamp", "aqua", "leading", "paths", "range", "first", "combined", "strategy,", "selects", "answer.", "results", "then", "evaluation", "this", "taking", "samples", "gsm8k", "replace", "pre-trained", "(+3.9%).", "sampled", "one,", "complex", "extensive"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247628166": {"id": "247628166", "openalex": null, "doi": null, "title": "VISION-BASED MANIPULATORS NEED TO ALSO SEE FROM THEIR HANDS", "abstract": "We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms outof-distribution generalization. To mitigate this, we propose to regularize the thirdperson information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation. 1Figure 1: Illustration suggesting the role that visual perspective can play in facilitating the acquisition of symmetries with respect to certain transformations on the world state s. T0: planar translation of the end-effector and cube. T1: vertical translation of the table surface, end-effector, and cube. T2: addition of distractor objects. O3: third-person perspective. O h : hand-centric perspective.", "authors": [], "concepts": ["case", "state-of-the-art", "facilitating", "surface,", "transformations", "learning,", "bottleneck.", "variational", "stream", "generalization.", "tasks", "used", "addition", "translation", "1figure", "regularize", "information", "simulated", "cube.", "certain", "these", "only", "hold", "insights", "out-of-distribution", "reduced", "end-effector", "more", "from", "suggesting", "however,", "analyzes", "representative", "objects.", "while", "systematically", "compared", "planar", "mitigate", "simple", "algorithms,", "varying", "physical", "operating", "global", "propose", "reinforcement", "generalization", "some", "doing", "both", "vertical", "work", "sensor", "perspectives", "this,", "learning", "experimental", "affects", "illustration", "hand-centric", "end-effector,", "end-to-end", "long", "perspective", "vision-based", "manipulators", "settings,", "robotic", "when", "perspective,", "necessary", "improves", "acquisition", "training", "context", "thirdperson", "world", "across", "with", "including", "adapted", "play", "observability,", "efficiency", "(eye-in-hand)", "task.", "that", "broadly", "distribution", "outof-distribution", "harms", "real", "their", "apparatuses.", "distractor", "table", "robots,", "meta-world", "learned", "choice", "need", "cameras", "observations.", "otherwise,", "sufficient;", "robot", "find", "results", "commonly", "also", "shifts,", "third-person", "consistently", "symmetries", "this", "state", "respect", "practitioners", "provides", "benefits", "improving", "perspective.", "agent", "benchmark,", "observability", "have", "manipulation.", "role", "visual", "affords", "manipulation", "applicable", "study", "variety", "every", "hands"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247741267": {"id": "247741267", "openalex": null, "doi": null, "title": "DO USERS BENEFIT FROM INTERPRETABLE VISION? A USER STUDY, BASELINE, AND DATASET", "abstract": "A variety of methods exist to explain image classification models. However, it remains unclear whether they provide any benefit to users over simply comparing various inputs and the model's respective predictions. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies.", "authors": [], "concepts": ["unclear", "future", "performs", "exist", "conducted", "user", "show", "respective", "dataset", "they", "predictions.", "test", "such", "about", "baseline,", "counterfactual", "classification", "interpretable", "more", "from", "however,", "comparing", "users", "biasing", "explanation", "well", "technical", "compared", "reason", "baseline", "attributes", "explain", "(n=240)", "rather", "some", "methods", "capable", "measuring", "model's", "study,", "inputs", "invertible", "network", "importance", "model,", "relying", "image", "quantifying", "highlight", "outperformed", "individual", "contribute", "proxy", "evaluations", "tasks.", "biases", "neural", "identify", "accurately.", "allowed", "whether", "that", "solely", "their", "explanations.", "benefit", "vision?", "end,", "model.", "baseline.", "technique", "still,", "concept-based", "simply", "blue-print", "performed", "provide", "studies.", "results", "synthetic", "various", "serve", "open-source", "than", "over", "models.", "this", "ground-truth.", "assess", "generator", "explanations", "relevance", "relevant", "study", "variety", "remains", "participants", "against", "similarly"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247741535": {"id": "247741535", "openalex": null, "doi": null, "title": "TOPOLOGICAL EXPERIENCE REPLAY", "abstract": "State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from the set of terminal states and successively moving backwards. We empirically show that our method is substantially more data-efficient than several baselines on a diverse range of goal-reaching tasks. Notably, the proposed method also outperforms baselines that consume more batches of training experience and operates from high-dimensional observational data such as images. The code is available at: https://github.com/Improbable-AI/ter.", "authors": [], "concepts": ["state's", "state-of-the-art", "dependency", "moving", "batches", "strategy", "(td)", "each", "tracks", "action.", "show", "ignores", "method", "terminal", "lead", "available", "such", "difference", "updates", "update", "state,", "more", "sampling", "from", "into", "deep", "strategies", "using", "estimate", "explicitly", "prioritizes", "consume", "diverse", "mitigate", "issue,", "incorrect", "high-dimensional", "because", "methods", "observational", "graph", "learning", "topological", "measures", "substantially", "successively", "operates", "backups", "based", "code", "transition", "represents", "data", "training", "search", "tasks.", "executing", "notably,", "several", "backwards.", "successor", "states", "organize", "q-value", "agent's", "outperforms", "useless", "that", "states.", "randomly", "between", "inefficient", "q-function", "data-efficient", "range", "empirically", "next", "baselines", "value", "q-values", "temporal", "perform", "q-values.", "starting", "experience", "also", "images.", "uniformly", "depends", "error.", "than", "https://github.com/improbable-ai/ter.", "this", "state", "samples", "often", "proposed", "replay", "precision", "tuples", "goal-reaching", "breadth-first", "sampled", "buffer.", "single", "q-learning", "edge"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "247996981": {"id": "247996981", "openalex": null, "doi": null, "title": "LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification", "abstract": "We introduce LilNetX, an end-to-end trainable technique for neural networks that enables learning models with specified accuracy-rate-computation trade-off. Prior works approach these problems one at a time and often require post-processing or multistage training which become less practical and do not scale very well for large datasets or architectures. Our method constructs a joint training objective that penalizes the self information of network parameters in a reparameterized latent space to encourage small model size while also introducing priors to increase structured sparsity in the parameter space to reduce computation. We achieve up to 50% smaller model size and 98% model sparsity on ResNet-20 while retaining the same accuracy on the CIFAR-10 dataset as well as 35% smaller model size and 42% structured sparsity on ResNet-50 trained on ImageNet, when compared to existing state-of-the-art model compression methods. Code is available at https://github.com/Sharath-girish/LilNetX. Recent research in deep neural networks (DNNs) has shown that large performance gains can be achieved on a variety of computer vision tasks simply by employing larger parameter-heavy and computationally intensive architectures[13,26]. However, as the DNNs proliferate in the industry, they often need to be trained repeatedly, transmitted over the network to different devices, and need to perform under hardware constraints with minimal loss in accuracy, all at the same time. Hence, finding ways to reduce the storage size of the models on the devices while simultaneously improving their run-time is of utmost importance. This paper proposes a general purpose neural network training framework to jointly optimize the model parameters for accuracy, model size on the disk and computation, on any given task.Over the last few years, the research on training smaller and efficient DNNs has followed two seemingly parallel tracks with different goals: One line of work focuses on model compression to deal with the storage and communication network bottlenecks when deploying a large number of models over the air. While they achieve high levels of compression in terms of memory, their focus is not on reducing computation. They either require additional algorithms with some form of post hoc training[71]or quantize the network parameters at the cost of network performance[10,39]. The other line of work focuses on reducing computation through various model pruning techniques[16]. The focus of these works is to decrease the number of Floating Point Operations (FLOPs) of the network at the inference time, albeit they are also able to achieve some compression due to fewer parameters. Typically, the cost of storing these pruned networks on disk is much higher than the dedicated model compression works.Preprint. Under review.", "authors": [], "concepts": ["parameter-heavy", "smaller", "state-of-the-art", "inference", "computer", "decrease", "parameters", "focuses", "penalizes", "tracks", "tasks", "computation,", "sparsification", "model", "method", "deploying", "available", "recent", "dataset", "they", "purpose", "extreme", "self", "constructs", "ways", "information", "proposes", "either", "efficient", "achieved", "joint", "larger", "these", "cifar-10", "existing", "large", "scale", "prior", "pruned", "devices,", "general", "number", "shown", "much", "datasets", "through", "increase", "able", "lilnetx,", "performance", "deep", "however,", "lightweight", "run-time", "well", "while", "accuracy", "imagenet,", "compared", "very", "framework", "trainable", "resnet-20", "models", "networks", "storing", "some", "seemingly", "deal", "memory,", "parallel", "optimize", "employing", "work", "terms", "time,", "transmitted", "disk", "line", "compression", "bottlenecks", "learning", "typically,", "industry,", "specified", "dnns", "problems", "intensive", "additional", "simultaneously", "techniques[16].", "levels", "methods.", "end-to-end", "albeit", "computationally", "(flops)", "retaining", "architectures.", "latent", "priors", "other", "space", "high", "network", "minimal", "parameters.", "cost", "constraints", "when", "air.", "code", "form", "achieve", "vision", "accuracy,", "training", "quantize", "followed", "neural", "works", "with", "introducing", "last", "given", "jointly", "less", "point", "trained", "approach", "that", "resnet-50", "finding", "goals:", "https://github.com/sharath-girish/lilnetx.", "their", "repeatedly,", "communication", "architectures[13,26].", "multistage", "hardware", "different", "higher", "floating", "structured", "performance[10,39].", "post", "lilnetx:", "storage", "sparsity", "need", "technique", "reduce", "simply", "reducing", "size", "paper", "focus", "objective", "research", "parameter", "time.", "(dnns)", "proliferate", "practical", "hence,", "algorithms", "training[71]or", "fewer", "computation.", "perform", "review.", "also", "works.preprint.", "introduce", "various", "same", "than", "over", "devices", "pruning", "this", "which", "importance.", "operations", "dedicated", "enables", "improving", "loss", "time", "accuracy-rate-computation", "computation", "small", "often", "under", "task.over", "post-processing", "require", "trade-off.", "gains", "variety", "reparameterized", "utmost", "years,", "encourage", "become"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "248887351": {"id": "248887351", "openalex": null, "doi": null, "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning", "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.", "authors": [], "concepts": ["inference", "probe", "poorly", "implications", "here", "tasks", "impressive", "logical", "problems.", "reasoning.", "model", "used", "modules,", "show", "they", "carry", "within", "interpretable,", "exploiting", "larger", "large", "been", "setting,", "multiple", "general", "light", "safety", "shown", "interpretable", "more", "performance", "trustworthiness", "however,", "(si)", "accompanied", "well", "produced", "compared", "baseline", "comprehensive", "framework", "propose", "selection", "models", "tend", "suite", "solve", "final", "chain", "this,", "capable", "llms", "equivalent", "steps", "selection-inference:", "significantly", "multi-step", "selection-inference", "casual", "step", "5-shot", "series", "answers", "fine-tuning,", "tasks.", "fairly", "reasoning", "with", "few-shot", "struggle", "improvement", "outperforms", "that", "between", "280b", "natural-language-based", "trace,", "language", "different", "processing", "100%", "leading", "generalisation", "yields", "vanilla", "causal", "parameter", "perform", "moreover,", "answer.", "together", "exploits", "same", "evaluation", "over", "tasks,", "which", "still", "system.", "alternates", "have", "aspects", "pre-trained", "important", "even", "generate", "single", "complex", "entailment", "(llms)", "setting"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "248887644": {"id": "248887644", "openalex": null, "doi": null, "title": "RIEMANNIAN METRIC LEARNING VIA OPTIMAL TRANSPORT", "abstract": "We introduce an optimal transport-based model for learning a metric tensor from cross-sectional samples of evolving probability measures on a common Riemannian manifold. We neurally parametrize the metric as a spatially-varying matrix field and efficiently optimize our model's objective using a simple alternating scheme. Using this learned metric, we can nonlinearly interpolate between probability measures and compute geodesics on the manifold. We show that metrics learned using our method improve the quality of trajectory inference on scRNA and bird migration data at the cost of little additional cross-sectional data.", "authors": [], "concepts": ["inference", "metric", "field", "model", "show", "method", "spatially-varying", "from", "matrix", "using", "tensor", "simple", "optimal", "neurally", "riemannian", "data.", "optimize", "learning", "improve", "probability", "additional", "model's", "measures", "cost", "alternating", "evolving", "data", "common", "cross-sectional", "that", "between", "metric,", "trajectory", "migration", "bird", "learned", "geodesics", "parametrize", "efficiently", "transport-based", "objective", "metrics", "introduce", "scrna", "manifold.", "quality", "nonlinearly", "this", "scheme.", "samples", "transport", "little", "interpolate", "compute"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "249191952": {"id": "249191952", "openalex": null, "doi": null, "title": "Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors", "abstract": "Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an unlabeled target domain supervised by a black-box predictor trained on a source domain. It does not require access to both the source-domain data and the predictor parameters, thus addressing the data privacy and portability issues of standard domain adaptation. Existing DABP approaches mostly rely on model distillation from the black-box predictor, i.e., training the model with its noisy target-domain predictions, which however inevitably introduces the confirmation bias accumulated from the prediction noises. To mitigate such bias, we propose a new method, named BETA, to incorporate knowledge distillation and noisy label learning into one coherent framework. This is enabled by a new divide-to-adapt strategy. BETA divides the target domain into an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain. Then it deploys mutually-teaching twin networks to filter the predictor errors for each other and improve them progressively, from the easy to hard subdomains. As such, BETA effectively purifies the noisy labels and reduces error accumulation. We theoretically show that the target error of BETA is minimized by decreasing the noise ratio of the subdomains. Extensive experiments demonstrate BETA outperforms existing methods on all DABP benchmarks, and is even comparable with the standard domain adaptation methods that use the source-domain data.", "authors": [], "concepts": ["aims", "learn", "demonstrate", "divide", "each", "subdomains.", "noises.", "access", "comparable", "error", "parameters,", "effectively", "model", "domain.", "them", "show", "bias,", "adapt:", "beta,", "such", "existing", "accumulated", "twin", "does", "subdomain.", "strategy.", "from", "minimized", "into", "progressively,", "predictions,", "filter", "benchmarks,", "mitigate", "addressing", "propose", "method,", "networks", "reduces", "thus", "data.", "domain", "both", "methods", "subdomain", "learning", "improve", "target", "i.e.,", "inevitably", "errors", "rely", "source", "other", "experiments", "label", "predictor,", "easy", "data", "(dabp)", "target-domain", "training", "introduces", "theoretically", "source-domain", "with", "noisy", "standard", "less", "bias", "trained", "adaptation", "confirmation", "outperforms", "mitigating", "that", "noise", "knowledge", "such,", "prediction", "black-box", "unlabeled", "enabled", "adaptation.", "approaches", "mutually-teaching", "distillation", "predictors", "deploys", "divides", "labels", "beta", "privacy", "dabp", "then", "coherent", "ratio", "purifies", "mostly", "this", "which", "easy-to-adapt", "issues", "predictor", "however", "portability", "accumulation.", "incorporate", "framework.", "named", "divide-to-adapt", "even", "require", "supervised", "hard-to-adapt", "hard", "extensive", "decreasing"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "249209856": {"id": "249209856", "openalex": null, "doi": null, "title": "Fooling SHAP with Stealthily Biased Sampling", "abstract": "SHAP explanations aim at identifying which features contribute the most to the difference in model prediction at a specific input versus a background distribution. Recent studies have shown that they can be manipulated by malicious adversaries to produce arbitrary desired explanations. However, existing attacks focus solely on altering the black-box model itself. In this paper, we propose a complementary family of attacks that leave the model intact and manipulate SHAP explanations using stealthily biased sampling of the data points used to approximate expectations w.r.t the background distribution. In the context of fairness audit, we show that our attack can reduce the importance of a sensitive feature when explaining the difference in outcomes between groups, while remaining undetected. These results highlight the manipulability of SHAP explanations and encourage auditors to treat post-hoc explanations with skepticism. Recently, several studies reported that such a manipulation is possible, e.g. by modifying the Preprint. Under review.", "authors": [], "concepts": ["explaining", "identifying", "paper,", "studies", "most", "approximate", "preprint.", "modifying", "model", "used", "manipulability", "show", "undetected.", "recent", "they", "altering", "such", "these", "existing", "itself.", "difference", "feature", "specific", "shown", "sampling", "malicious", "however,", "using", "while", "propose", "stealthily", "features", "biased", "desired", "post-hoc", "recently,", "produce", "manipulate", "family", "manipulated", "fairness", "importance", "audit,", "when", "outcomes", "highlight", "points", "contribute", "background", "data", "input", "context", "with", "sensitive", "several", "that", "possible,", "solely", "between", "complementary", "explanations.", "intact", "w.r.t", "arbitrary", "expectations", "prediction", "reported", "groups,", "black-box", "adversaries", "reduce", "versus", "focus", "review.", "results", "remaining", "leave", "this", "which", "attack", "have", "treat", "fooling", "attacks", "shap", "explanations", "manipulation", "under", "auditors", "skepticism.", "encourage", "distribution.", "e.g."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "249209990": {"id": "249209990", "openalex": null, "doi": null, "title": "POST-HOC CONCEPT BOTTLENECK MODELS", "abstract": "Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (\"the bottleneck\") and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model \"sees\" in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via conceptlevel feedback can provide significant performance gains without using data from the target domain or model retraining. The code for our paper can be found in https://github.com/mertyg/post-hoc-cbm. arXiv:2205.15480v2 [cs.LG] 1 Feb 2023 Published as a conference paper at ICLR 2023 2. Performance: CBMs often do not match the accuracy of an unrestricted model, potentially reducing the incentive to use them in practice. When the concepts are not enough to solve the desired task, it is not clear how to improve the CBM and match the original model performance, while retaining the interpretability benefits. 3. Model editing: Koh et al. (2020) discuss intervening on the model to fix the prediction for a singleinput, yet it is not shown how to holistically edit and improve the model itself. Intervening only changes the model behavior for a single sample, but global editing changes the model behavior completely. When the model picks up an unintended cue, or learns spurious associations, using the latter approach and editing the concept bottleneck can improve the model performance more generally than an intervention tailored toward one specific input. Prior work on CBMs does not discuss how to globally edit a model's behavior. Ideally, we would like to edit models with the help of human input in order to lower computational costs and remove assumptions about data access.Our contributions. In this work, we propose the Post-hoc Concept Bottleneck Model (PCBM) to address these important challenges. PCBMs can convert any pre-trained model into a concept bottleneck model in a data-efficient manner, and enhance the model with the desired interpretability benefits. When the training data does not have concept annotations, which is often the case, PCBM can flexibly leverage concepts annotated in other datasets and natural language descriptions of concepts. When applicable, PCBMs can remove the laborious concept annotation process by leveraging multimodal models to obtain concept representations; this results in richer and more expressive bottlenecks using natural language descriptions of a concept, making PCBMs more accessible in various settings. Furthermore, when the available concepts are not sufficiently rich, we introduce a residual modeling step to the PCBM to recover the original blackbox model's performance. In experiments across several tasks, we show that PCBMs can be used with comparable performance compared to black-box models. While prior work(Koh et al., 2020)demonstrated the possibility of performing local model interventions to change individual predictions, here we propose interventions for changing global model behavior. Through user studies, we show that PCBM enables efficient global model edits without retraining or access to data from the target domain and that users can improve PCBM performance by using concept-level feedback to drive editing decisions.", "authors": [], "concepts": ["learn", "work(koh", "correlations", "unrestricted", "what", "performance:", "flexibly", "understand", "bottleneck.", "access", "comparable", "singleinput,", "here", "restrictive", "data,", "globally", "recover", "spurious", "model", "used", "user", "accessible", "them", "show", "decisions.", "available", "they", "predictions.", "(pcbm)", "latter", "remove", "modeling", "efficient", "access.our", "richer", "concept", "these", "about", "only", "limitations", "prior", "itself.", "generally", "would", "does", "deploy", "specific", "(pcbms).", "discuss", "update", "shown", "datasets", "through", "interpretable", "performance.", "more", "performance", "enhance", "from", "leverage", "into", "interventions", "clear", "however,", "order", "users", "using", "predictions,", "pcbm", "edits,", "while", "concepts.", "intervening", "accuracy", "conference", "unintended", "compared", "possibility", "cue,", "global", "propose", "models", "generalization", "completely.", "task,", "found", "bottleneck", "significant", "process", "solve", "domain", "transfer", "studies,", "furthermore,", "holistically", "lower", "contributions.", "work", "cbms", "debug", "retraining", "pcbms", "expressive", "sufficiently", "bottlenecks", "improve", "desired", "quickly", "applicable,", "like", "post-hoc", "bottleneck\")", "model's", "target", "study,", "without", "performance,", "drive", "toward", "local", "retaining", "other", "enhances", "experiments", "inputs", "edits", "investigated", "laborious", "network", "changes", "annotations,", "practice.", "potentially", "picks", "model,", "when", "code", "step", "blackbox", "ideally,", "sacrificing", "\"sees\"", "individual", "data", "input", "since", "training", "representations;", "neural", "learns", "making", "works", "input.", "across", "published", "with", "introducing", "challenges.", "behavior", "several", "retraining.", "conceptlevel", "performing", "dense", "work,", "approach", "assumptions", "(cbms)", "benefits.", "settings.", "that", "sample,", "case,", "benefit", "language", "previous", "turn", "prediction", "black-box", "network,", "residual", "natural", "distributions.", "make", "(2020)", "deemed", "model-editing", "multimodal", "associations,", "data-efficient", "reduce", "behavior.", "leveraging", "reducing", "address", "paper", "allows", "provide", "feedback", "https://github.com/mertyg/post-hoc-cbm.", "change", "match", "help", "computational", "concepts", "tailored", "annotated", "editing", "descriptions", "moreover,", "changing", "results", "[cs.lg]", "al.,", "prediction.", "introduce", "various", "annotations", "important.", "concept-level", "2020)demonstrated", "annotation", "than", "models.", "costs", "tasks,", "this", "which", "enough", "still", "enables", "interpretability", "incentive", "(\"the", "editing:", "have", "rich,", "often", "iclr", "onto", "pre-trained", "original", "practice", "edit", "important", "convert", "require", "arxiv:2205.15480v2", "intervention", "gains", "human", "single", "obtain", "manner,", "concept,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "249395483": {"id": "249395483", "openalex": null, "doi": null, "title": "Asymptotic Instance-Optimal Algorithms for Interactive Decision Making", "abstract": "Past research on interactive decision making problems (bandits, reinforcement learning, etc.) mostly focuses on the minimax regret that measures the algorithm's performance on the hardest instance. However, an ideal algorithm should adapt to the complexity of a particular problem instance and incur smaller regrets on easy instances than worst-case instances. In this paper, we design the first asymptotic instance-optimal algorithm for general interactive decision making problems with finite number of decisions under mild conditions. On every instance f , our algorithm outperforms all consistent algorithms (those achieving non-trivial regrets on all instances), and has asymptotic regret C(f ) ln n, where C(f ) is an exact characterization of the complexity of f . The key step of the algorithm involves hypothesis testing with active data collection. It computes the most economical decisions with which the algorithm collects observations to test whether an estimated instance is indeed correct; thus, the complexity C(f ) is the minimum cost to test the instance f against other instances. Our results, instantiated on concrete problems, recover the classical gap-dependent bounds for multi-armed bandits [Lai et al., 1985]  and prior works on linear bandits [Lattimore and Szepesvari, 2017], and improve upon the previous best instance-dependent upper bound [Xu et al., 2021] for reinforcement learning. , et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):414-419, 2022. Kefan Dong, Jiaqi Yang, and Tengyu Ma. Provable model-based nonlinear bandit and reinforcement learning: Shelve optimism, embrace virtual curvature. arXiv preprint arXiv:2102.04168, 2021. Dylan Foster and Alexander Rakhlin. Beyond UCB: Optimal and efficient contextual bandits with regression oracles. In . Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective. arXiv preprint arXiv:2010.03104, 2020. Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021. Manuel Gil, Fady Alajaji, and Tamas Linder. R\u00e9nyi divergence measures for commonly used univariate continuous distributions. Information Sciences, 249:124-131, 2013.Todd L Graves and Tze Leung Lai. Asymptotically efficient adaptive choice of control laws incontrolled markov chains.", "authors": [], "concepts": ["smaller", "2013.todd", "achieving", "asymptotically", "bound", "paper,", "learning,", "focuses", "instances.", "foster,", "regret", "most", "upon", "problem", "estimated", "particular", "collection.", "recover", "contextual", "indeed", "continuous", "tokamak", "used", "gap-dependent", "qian,", "alajaji,", "test", "classical", "linder.", "decisions", "interactive", "information", "sciences,", "efficient", "consistent", "nature,", "hardest", "prior", "characterization", "dylan", "bandit", "general", "number", "1985]", "through", "chains.", "computes", "adaptive", "performance", "arxiv", "minimax", "learning:", "deep", "however,", "asymptotic", "concrete", "instances", "should", "correct;", "regression", "yang,", "fady", "optimal", "reinforcement", "249:124-131,", "learning.", "hypothesis", "r\u00e9nyi", "complexity", "kakade,", "mild", "collects", "incur", "linear", "statistical", "improve", "thus,", "bandits", "problems", "measures", "manuel", "instantiated", "regrets", "gil,", "magnetic", "embrace", "other", "kefan", "2021]", "decision", "leung", "optimism,", "cost", "alexander", "[lai", "step", "control", "easy", "2021.", "instance-optimal", "past", "data", "instances),", "ideal", "divergence", "making", "works", "with", "upper", "arxiv:2010.03104,", "design", "(bandits,", "exact", "[lattimore", "active", "whether", "beyond", "outperforms", "incontrolled", "that", "2017],", "preprint", "markov", "making.", "algorithm's", "testing", "(those", "previous", "tamas", "provable", "univariate", "distributions.", "choice", "lai.", "rakhlin.", "first", "non-trivial", "ucb:", "shelve", "dong,", "nonlinear", "multi-armed", "economical", "research", "algorithms", "graves", "observations", "commonly", "problems,", "al.,", "conditions.", "jian", "best", "where", "2020.", "602(7897):414-419,", "tengyu", "model-based", "instance", "laws", "than", "mostly", "2022.", "this", "which", "worst-case", "finite", "instance.", "adapt", "plasmas", "disagreement-based", "results,", "algorithm", "etc.)", "arxiv:2112.13487,", "bounds", "perspective.", "sham", "arxiv:2102.04168,", "involves", "szepesvari,", "jiaqi", "under", "instance-dependent", "oracles.", "foster", "virtual", "minimum", "against", "every", "curvature."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "249461537": {"id": "249461537", "openalex": null, "doi": null, "title": "Constructive TT-representation of the tensors given as index interaction functions with applications", "abstract": "This paper presents a method to build explicit tensor-train (TT) representations. We show that a wide class of tensors can be explicitly represented with sparse TT-cores, obtaining, in many cases, optimal TT-ranks. Numerical experiments show that our method outperforms the existing ones in several practical applications, including game theory problems. Theoretical estimations of the number of operations show that in some problems, such as permanent calculation, our methods are close to the known optimal asymptotics, which are obtained by a completely different type of methods.", "authors": [], "concepts": ["known", "problems.", "theoretical", "show", "method", "(tt)", "index", "asymptotics,", "such", "existing", "game", "number", "explicitly", "many", "explicit", "applications,", "representations.", "tt-ranks.", "optimal", "wide", "some", "tensors", "tt-cores,", "methods", "sparse", "applications", "methods.", "interaction", "completely", "experiments", "numerical", "tensor-train", "class", "obtaining,", "obtained", "with", "constructive", "including", "given", "several", "close", "outperforms", "that", "functions", "different", "tt-representation", "presents", "build", "calculation,", "cases,", "estimations", "paper", "practical", "theory", "problems,", "type", "this", "which", "operations", "ones", "represented", "permanent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "249538336": {"id": "249538336", "openalex": null, "doi": null, "title": "ESCHER: ESCHEWING IMPORTANCE SAMPLING IN GAMES BY COMPUTING A HISTORY VALUE FUNCTION TO ESTIMATE REGRET", "abstract": "Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promising line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art-DREAM and neural fictitious self play (NFSP)-on a number of games and the difference becomes dramatic as game size increases. In the very large game of dark chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over 90% of the time.", "authors": [], "concepts": ["sampling.", "increases.", "dark", "learn", "term", "guaranteed", "trains", "regret", "estimated", "approximate", "model", "monte", "show", "method", "recent", "techniques", "self", "principled", "carlo", "escher:", "large", "only", "prior", "difference", "does", "game", "number", "counterfactual", "able", "sampling", "from", "leverage", "extremely", "estimate", "(strategies).", "magnitude", "function", "games", "very", "propose", "optimal", "approximating", "head-to-head", "method,", "networks", "art-dream", "current", "computing", "cfr-based", "lower", "equilibrium", "unbiased", "competition", "line", "history", "orders", "target", "model-free", "nash", "other", "becomes", "high", "network", "uses", "promising", "importance", "beat", "probability.", "neural", "with", "(mccfr).", "variance", "play", "eschewing", "escher,", "outperforms", "that", "free", "converge", "games,", "minimization", "escher", "scalable", "fictitious", "size", "value", "paper", "research", "time.", "policies", "(cfr)", "then", "than", "over", "nfsp", "this", "state", "approximately", "baselines.", "have", "therefore", "(nfsp)-on", "require", "inherited", "equilibria", "chess,", "dream,", "dramatic", "modern", "variants.", "dream"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "249538415": {"id": "249538415", "openalex": null, "doi": null, "title": "DIFFUSION PROBABILISTIC MODELING OF PROTEIN BACKBONES IN 3D FOR THE MOTIF-SCAFFOLDING PROBLEM", "abstract": "Construction of a scaffold structure that supports a desired motif, conferring protein function, shows promise for the design of vaccines and enzymes. But a general solution to this motif-scaffolding problem remains open. Current machine-learning techniques for scaffold design are either limited to unrealistically small scaffolds (up to length 20) or struggle to produce multiple diverse scaffolds. We propose to learn a distribution over diverse and longer protein backbone structures via an E(3)equivariant graph neural network. We develop SMCDiff to efficiently sample scaffolds from this distribution conditioned on a given motif; our algorithm is the first to theoretically guarantee conditional samples from a diffusion model in the largecompute limit. We evaluate our designed backbones by how well they align with AlphaFold2-predicted structures. We show that our method can (1) sample scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for a fixed motif. 1 * Contributed equally to this work. . Robust deep learning-based protein sequence design using ProteinMPNN. Science, 378(6615):49-56, 2022.Arnaud Doucet and Adam M Johansen. A tutorial on particle filtering and smoothing: Fifteen years later.", "authors": [], "concepts": ["learn", "tutorial", "years", "open.", "shows", "proteinmpnn.", "structures.", "robust", "problem", "fifteen", "equally", "vaccines", "scaffolds", "model", "show", "method", "they", "techniques", "motif;", "modeling", "either", "2022.arnaud", "align", "scaffolds.", "multiple", "general", "probabilistic", "from", "deep", "smcdiff", "using", "learning-based", "scaffold", "well", "structure", "motif.", "diverse", "propose", "largecompute", "conditional", "current", "particle", "later.", "backbone", "designed", "graph", "alphafold2-predicted", "desired", "limit.", "enzymes.", "doucet", "work.", "produce", "378(6615):49-56,", "longer", "machine-learning", "diffusion", "science,", "backbones", "conditioned", "supports", "filtering", "construction", "achieve", "contributed", "structurally", "theoretically", "neural", "with", "smoothing:", "given", "design", "struggle", "network.", "johansen.", "structures", "motif-scaffolding", "e(3)equivariant", "that", "distribution", "function,", "fixed", "motif,", "protein", "solution", "sequence", "efficiently", "first", "promise", "adam", "sample", "over", "this", "unrealistically", "samples", "algorithm", "residues", "develop", "length", "small", "limited", "evaluate", "conferring", "remains", "guarantee"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "250113584": {"id": "250113584", "openalex": null, "doi": null, "title": "Distilling Model Failures as Directions in Latent Space", "abstract": "Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes.", "authors": [], "concepts": ["challenging", "demonstrate", "correlations", "error", "and,", "failure", "spurious", "model", "used", "method", "subpopulations", "modes.", "within", "consistent", "these", "existing", "labor-intensive", "feature", "datasets", "images", "present", "induce", "framework", "thus", "intervention.", "caption", "methods", "linear", "automatically", "model's", "especially", "latent", "space", "diffusion", "augmentation", "specifically,", "harness", "model,", "modes", "combining", "data", "training", "with", "identify", "space.", "that", "shortcomings,", "scalable", "natural", "patterns,", "make", "dataset-specific.", "remedy", "address", "failures", "allows", "turn,", "perform", "moreover,", "synthetic", "dataset.", "directions", "this", "distilling", "discover", "classifiers", "often", "representation", "analyzed", "require", "generate", "hard", "models,", "isolating", "human", "off-the-shelf", "helps"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "251320393": {"id": "251320393", "openalex": null, "doi": null, "title": "ZEROFL: EFFICIENT ON-DEVICE TRAINING FOR FEDERATED LEARNING WITH LOCAL SPARSITY", "abstract": "When the available hardware cannot meet the memory and compute requirements to efficiently train high performing machine learning models, a compromise in either the training quality or the model complexity is needed. In Federated Learning (FL), nodes are orders of magnitude more constrained than traditional servergrade hardware and are often battery powered, severely limiting the sophistication of models that can be trained under this paradigm. While most research has focused on designing better aggregation strategies to improve convergence rates and in alleviating the communication costs of FL, fewer efforts have been devoted to accelerating on-device training. Such stage, which repeats hundreds of times (i.e. every round) and can involve thousands of devices, accounts for the majority of the time required to train federated models and, the totality of the energy consumption at the client side. In this work, we present the first study on the unique aspects that arise when introducing sparsity at training time in FL workloads. We then propose ZeroFL, a framework that relies on highly sparse operations to accelerate on-device training. Models trained with ZeroFL and 95% sparsity achieve up to 2.3% higher accuracy compared to competitive baselines obtained from adapting a state-of-the-art sparse training framework to the FL setting.Published as a conference paper at ICLR 2022 this way overall device utilization (e.g. fewer local epochs) and number of communication rounds. Other optimization techniques such as quantization and sparsity have been used in the context of FL but mostly as a way to reduce communication costs(Liu et al., 2021;Amiri et al., 2020;Shahid et al., 2021)but not to accelerate on-device training.The use of sparse operations (e.g. convolutions) at training time has recently been shown to be an effective technique to accelerate training in centralised settingsGoli & Aamodt, 2020;Raihan & Aamodt, 2020). The resulting models are as good or close to their densely-trained counterparts despite reducing by up to 90% their FLOPs budget and, resulting in an overall up to 3.3\u00d7 training speedup. Acceleration is achieved by performing sparse convolutions during the forward and/or backward pass, which requires at least one of the operands (i.e. inputs, weights, gradients) to be sufficiently sparse and, software and hardware support for such operations. However, it is unclear how the different FL-specific challenges (i.e. data imbalance, stateless clients, periodic aggregation) will restrict the quality of the global model. This work considers the challenges and opportunities of inducing high levels of sparsity to accelerate training on-device for FL workloads, and provides the following contributions:\u2022 The first framework for Federated Learning that leverages sparsity as a mechanism to accelerate on-device training by inducing up to 95% sparse weights and activations. This work considers three popular datasets: CIFAR-10 and FEMNIST for image classification and, SpeechCommands for audio classification.", "authors": [], "concepts": ["unclear", "state-of-the-art", "nodes", "machine", "stage,", "datasets:", "rounds.", "operations.", "and,", "densely-trained", "most", "speedup.", "model", "recently", "(fl),", "used", "majority", "available", "and/or", "techniques", "resulting", "overall", "needed.", "on-device", "classification.", "relies", "either", "efficient", "achieved", "such", "inputs,", "will", "cifar-10", "centralised", "devices,", "despite", "been", "number", "highly", "shown", "operands", "classification", "good", "more", "forward", "2020;shahid", "times", "from", "present", "federated", "strategies", "however,", "powered,", "repeats", "aamodt,", "gradients)", "magnitude", "side.", "2021;amiri", "activations.", "while", "accuracy", "conference", "compared", "rates", "flops", "requires", "global", "framework", "propose", "clients,", "convergence", "2020).", "models", "fl-specific", "totality", "limiting", "alleviating", "complexity", "popular", "zerofl,", "work", "unique", "hundreds", "counterparts", "better", "support", "sufficiently", "learning", "contributions:\u2022", "costs(liu", "budget", "epochs)", "improve", "leverages", "designing", "orders", "constrained", "levels", "sparse", "local", "speechcommands", "focused", "memory", "2021)but", "other", "high", "setting.published", "software", "challenges", "opportunities", "quantization", "energy", "restrict", "competitive", "accounts", "traditional", "devoted", "when", "3.3\u00d7", "consumption", "pass,", "image", "battery", "zerofl", "severely", "data", "achieve", "training", "context", "2020;raihan", "weights,", "obtained", "with", "introducing", "train", "least", "acceleration", "workloads,", "performing", "trained", "close", "work,", "utilization", "femnist", "aggregation)", "periodic", "aggregation", "inducing", "that", "training.", "imbalance,", "considers", "their", "required", "weights", "communication", "audio", "involve", "hardware", "different", "higher", "efforts", "model.", "effective", "sparsity", "technique", "training.the", "reduce", "stateless", "efficiently", "backward", "sophistication", "first", "reducing", "accelerate", "baselines", "paper", "workloads.", "research", "2.3%", "three", "following", "fewer", "thousands", "accelerating", "al.,", "requirements", "settingsgoli", "then", "than", "quality", "mostly", "costs", "device", "this", "which", "servergrade", "operations", "provides", "paradigm.", "during", "cannot", "time", "convolutions)", "round)", "adapting", "have", "compromise", "often", "aspects", "meet", "iclr", "optimization", "convolutions", "client", "under", "(e.g.", "study", "arise", "mechanism", "(i.e.", "models,", "compute", "every", "zerofl:"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "251320459": {"id": "251320459", "openalex": null, "doi": null, "title": "Agnostic Learning of General ReLU Activation Using Gradient Descent", "abstract": "We provide a convergence analysis of gradient descent for the problem of agnostically learning a single ReLU function under Gaussian distributions. Unlike prior work that studies the setting of zero bias, we consider the more challenging scenario when the bias of the ReLU function is non-zero. Our main result establishes that starting from random initialization, in a polynomial number of iterations gradient descent outputs, with high probability, a ReLU function that achieves a competitive error guarantee when compared to the error of the best ReLU function. We also provide finite sample guarantees, and these techniques generalize to a broader class of marginal distributions beyond Gaussians.", "authors": [], "concepts": ["challenging", "error", "studies", "problem", "agnostically", "bias,", "techniques", "non-zero.", "these", "prior", "general", "number", "marginal", "more", "from", "broader", "using", "consider", "relu", "function", "compared", "convergence", "achieves", "random", "work", "learning", "gradient", "activation", "gaussian", "high", "agnostic", "competitive", "analysis", "when", "class", "guarantees,", "unlike", "zero", "gaussians.", "with", "bias", "beyond", "that", "outputs,", "function.", "polynomial", "initialization,", "distributions", "distributions.", "result", "generalize", "scenario", "provide", "starting", "also", "best", "sample", "finite", "main", "iterations", "under", "establishes", "probability,", "guarantee", "single", "setting", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "251320513": {"id": "251320513", "openalex": null, "doi": null, "title": "Conformal Risk Control", "abstract": "We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an O(1/n) factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.", "authors": [], "concepts": ["computer", "demonstrate", "conformal", "bound", "split", "extensions", "idea", "coverage", "procedure", "multiple", "examples", "distance,", "expected", "from", "quantile", "token-level", "graph", "monotone", "like", "rate,", "o(1/n)", "f1-score.", "control", "vision", "shift,", "negative", "with", "usage", "worked", "distribution", "generalizes", "function.", "expectations", "language", "prediction", "processing", "extend", "natural", "false", "control,", "tight", "value", "prediction,", "together", "also", "introduce", "guarantee.", "algorithm", "risk", "factor.", "adversarial", "loss", "u-statistics."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "251554821": {"id": "251554821", "openalex": null, "doi": null, "title": "DIFFUSION POLICIES AS AN EXPRESSIVE POLICY CLASS FOR OFFLINE REINFORCEMENT LEARNING", "abstract": "Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Qlearning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness of the diffusion model-based policy, and the coupling of the behavior cloning and policy improvement under the diffusion model both contribute to the outstanding performance of Diffusion-QL. We illustrate the superiority of our method compared to prior works in a simple 2D bandit example with a multimodal behavior policy. We then show that our method can achieve state-of-the-art performance on the majority of the D4RL benchmark tasks. * The work was done in part during a summer internship at Twitter. \u2020 Joint senior authors; order determined by flipping a coin. arXiv:2208.06193v3 [cs.LG] 25 Aug 2023Published as a conference paper at ICLR 2023ICLR     et al., 2020Yu et al., 2021); 4) treating offline RL as a problem of sequence prediction with return guidance (Chen et al., 2021; Janner et al., 2021;. Our approach falls into the first category.Empirically, the performance of policy-regularized offline RL methods is typically slightly worse than that of other approaches, and here we show that this is largely because the policy regularization methods perform poorly due to their limited ability to accurately represent the behavior policy. This results in the regularization adversely affecting the policy improvement. For example, the policy regularization may limit the exploration space of the agent to a small region with only suboptimal actions and then the Q-learning will be induced to converge towards a suboptimal policy.The inaccurate policy regularization occurs for two main reasons: 1) policy classes are not expressive enough; 2) the regularization methods are improper. In most prior work, the policy is a Gaussian distribution with mean and diagonal covariance specified by the output of a neural network. However, as offline datasets are often collected by a mixture of policies, the true behavior policy may exhibit strong multi-modalities, skewness, or dependencies between different action dimensions, which cannot be well modeled by diagonal Gaussian policies(Shafiullah et al., 2022). In a particularly extreme, but not uncommon example, a Gaussian policy is used to fit bimodal training data by minimizing the Kullback-Leibler (KL) divergence from the data distribution to the policy distribution. This will result in the policy exhibiting mode-covering behavior and placing high density in the middle area of the two modes, which is actually the low-density region of the training data. In such cases, regularizing a new policy towards the behavior-cloned policy is likely to make the policy learning substantially worse. Second, the regularization, such as the KL divergence and maximum mean discrepancy (MMD) (Kumar et al., 2019), is often not well suited for offline RL. The KL divergence needs access to explicit density values and MMD needs multiple action samples at each state for optimization. These methods require an extra step by first learning a behavior cloned policy to provide density values for KL optimization or random action samples for MMD optimization. Regularizing the current policy towards the behavior cloned policy can further induce approximation errors, since the cloned behavior policy may not model the true behavior policy well, due to limitations in the expressiveness of the policy class. We conduct a simple bandit experiment in Section 4, which illustrates these issues can occur even on a simple bandit task.", "authors": [], "concepts": ["state-of-the-art", "outstanding", "actions", "aims", "learn", "static", "each", "2020yu", "uncommon", "poorly", "typically", "paper,", "term", "access", "here", "minimizing", "action", "most", "problem", "worse.", "approach,", "model", "used", "show", "majority", "method", "lead", "superiority", "recent", "limit", "they", "arxiv:2208.06193v3", "policy,", "summer", "regularization,", "such", "errors,", "joint", "will", "these", "dependencies", "only", "limitations", "prior", "out-of-distribution", "inaccurate", "2021;.", "been", "multiple", "low-density", "bandit", "well,", "return", "highly", "datasets", "represent", "performance", "modes,", "from", "into", "part", "deep", "however,", "order", "janner", "category.empirically,", "using", "induce", "example,", "well", "multi-modalities,", "generative", "expressiveness", "explicit", "while", "cloned", "function", "conference", "towards", "compared", "mitigate", "issue,", "dataset,", "simple", "propose", "optimal", "reinforcement", "likely", "conditional", "maximizing", "current", "data.", "conduct", "affecting", "discrepancy", "both", "exhibit", "extra", "because", "section", "random", "reasons:", "methods", "work", "bimodal", "expressive", "policies,", "learning", "policy-regularized", "actions.", "class.", "specified", "example", "substantially", "constrained", "middle", "output", "gaussian", "ability", "2022).", "(diffusion-ql)", "approaches,", "errors", "done", "offline", "other", "mode-covering", "2019),", "classes", "previously", "space", "actually", "high", "diffusion", "model,", "behavior-cloned", "class", "regularization", "step", "action-value", "regime", "policy", "contribute", "exploration", "data", "representing", "achieve", "second,", "since", "training", "divergence", "(kl)", "mean", "tasks.", "flipping", "treating", "neural", "covariance", "works", "with", "coin.", "senior", "behavior", "occurs", "placing", "standard", "network.", "2021;", "policy.the", "highly-expressive", "falls", "action-values", "illustrate", "modeled", "paradigm", "benchmark", "work,", "improvement", "region", "approach", "induced", "extreme,", "task.", "that", "distribution", "internship", "optimization.", "suboptimal", "determined", "between", "(mmd)", "their", "2023iclr", "converge", "strong", "occur", "collected", "different", "solutions.", "skewness,", "prediction", "maximum", "needs", "(kumar", "guidance", "policies(shafiullah", "approximation", "enough;", "sequence", "make", "multimodal", "result", "adversely", "seeks", "kullback-leibler", "first", "cases,", "paper", "provide", "policies", "cloning", "perform", "2023published", "improvement.", "results", "[cs.lg]", "diagonal", "al.,", "coupling", "accurately", "introduce", "then", "twitter.", "particularly", "model-based", "true", "slightly", "illustrates", "than", "models.", "dimensions,", "this", "which", "state", "improper.", "area", "samples", "suited", "during", "issues", "near", "loss", "cannot", "agent", "mixture", "largely", "density", "have", "(rl),", "small", "utilizes", "often", "proposed", "regularizing", "iclr", "main", "optimization", "qlearning", "2021);", "limited", "policy.", "experiment", "under", "important", "exhibiting", "worse", "d4rl", "diffusion-ql.", "(chen", "even", "require", "variety", "authors;", "further", "values", "q-learning", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252089864": {"id": "252089864", "openalex": null, "doi": null, "title": "Faster federated optimization under second-order similarity", "abstract": "Federated learning (FL) is a subfield of machine learning where multiple clients try to collaboratively learn a model over a network under communication constraints. We consider finite-sum federated optimization under a second-order function similarity condition and strong convexity, and propose two new algorithms: SVRP and Catalyzed SVRP. This second-order similarity condition has grown popular recently, and is satisfied in many applications including distributed statistical learning and differentially private empirical risk minimization. The first algorithm, SVRP, combines approximate stochastic proximal point evaluations, client sampling, and variance reduction. We show that SVRP is communication efficient and achieves superior performance to many existing algorithms when function similarity is high enough. Our second algorithm, Catalyzed SVRP, is a Catalyst-accelerated variant of SVRP that achieves even better performance and uniformly improves upon existing algorithms for federated optimization under second-order similarity and strong convexity. In the course of analyzing these algorithms, we provide a new analysis of the Stochastic Proximal Point Method (SPPM) that might be of independent interest. Our analysis of SPPM is simple, allows for approximate proximal point evaluations, does not require any smoothness assumptions, and shows a clear benefit in communication complexity over ordinary distributed stochastic gradient descent.arXiv:2209.02257v2 [cs.LG] 23 May 2023We answer the above question in the affirmative and show the utility of using client sampling in optimization under second-order similarity for strongly convex objectives. Our main contributions are as follows:\u2022 A new algorithm for federated optimization (SVRP, Algorithm 2). We develop a new algorithm, SVRP (Stochastic Variance-Reduced Proximal Point), that utilizes client sampling to improve upon the existing algorithms for solving Problem 1 under second-order similarity. SVRP has a better dependence on the number of clients M in its communication complexity than all existing algorithms (seeTable 1), and achieves superior performance when the dissimilarity constant \u03b4 is small enough. SVRP trades off a higher computational complexity for less communication.\u2022 Catalyst-accelerated SVRP. By using Catalyst (Lin et al., 2015), we accelerate SVRP and obtain a new algorithm (Catalyzed SVRP) that improves the dependence on the effective conditioning from \u03b4 2 \u00b5 2 to \u03b4 \u00b5 . Catalyzed SVRP also has a better convergence rate (in number of communication steps, ignoring constants and logarithmic factors) than all existing accelerated algorithms for this problem under Assumption 1, reducing the dependence on the number of clients multiplied by the effective conditioning \u03b4 \u00b5 from \u03b4 \u00b5 M to \u03b4 \u00b5 M 3/4 (seeTable 1).While both SVRP and Catalyzed SVRP achieve a communication complexity that is better than algorithms designed for the standard finite-sum setting (like SVRG or SAGA), the computational complexity is a lot worse. This is because we tradeoff local computation complexity for a reduced communication complexity. Additionally, both SVRP and Catalyzed SVRP are based upon a novel combination of variance-reduction techniques and the stochastic proximal point method (SPPM). SPPM is our starting point, and we provide a new analysis for it that might be of independent interest. Our analysis of SPPM is simple, allows for approximate evaluations of the proximal operator, and extends to include variance reduction. In Section 15 we also consider the more general constrained optimization problem and provide similar convergence rates in that setting.Related workDistributed optimization under Assumption 1. There is a long line of work analyzing distributed optimization under Assumption 1 and strong convexity: Shamir et al. (2014)  first gave DANE and analyzed it for quadratics, and showed the benefits of using second-order similarity in the setting of statistical learning for quadratic objectives. Zhang and Lin(2015)developed the DiSCO algorithm that improved upon DANE for quadratics, and also analyzed it for self-concordant objectives. Arjevani and Shamir(2015)gave a lower bound that matched the rate given by DANE, though without allowing for client sampling. The theory of DANE was later improved in (Yuan and Li, 2019), allowing for local convergence for non-quadratic objectives. Another algorithm SCAFFOLD (Karimireddy et al., 2020b) can be seen as a variant of DANE and is also analyzed for quadratics. In the context of decentralized optimization, Sun et al. (2022) gave SONATA and showed a similar rate to DANE but for general strongly convex objectives, then Tian et al. (2022) improved the convergence rate of SONATA by acceleration. Finally, Kovalev et al. (2022) improved the convergence rate of accelerated SONATA even further by removing extra logarithmic factors.We give an overview of all related results inTable 1and provide more thorough comparisons in the theory and algorithms section.", "authors": [], "concepts": ["combination", "sampling.", "differentially", "(karimireddy", "complexity.", "kovalev", "learn", "machine", "ignoring", "bound", "(sppm)", "shows", "upon", "problem", "approximate", "worse.", "factors.we", "question", "constant", "evaluations,", "model", "rate", "show", "method", "convexity:", "minimization.", "simple,", "techniques", "strongly", "comparisons", "later", "(stochastic", "catalyzed", "efficient", "faster", "descent.arxiv:2209.02257v2", "intable", "these", "existing", "constants", "does", "objectives.", "multiple", "general", "seen", "number", "empirical", "interest.", "variance-reduced", "reduced", "1).while", "more", "sampling", "performance", "from", "affirmative", "clear", "federated", "overview", "using", "consider", "second-order", "scaffold", "multiplied", "many", "satisfied", "sampling,", "function", "might", "sppm", "rates", "algorithms,", "propose", "thorough", "shamir", "variance-reduction", "convergence", "tradeoff", "there", "trades", "achieves", "convex", "extends", "showed", "svrg", "complexity", "both", "non-quadratic", "utility", "extra", "because", "popular", "section", "designed", "lower", "arjevani", "work", "finally,", "better", "improved", "superior", "quadratics,", "analyzing", "line", "private", "statistical", "include", "2015),", "learning", "improve", "contributions", "section.", "gradient", "recently,", "solving", "smoothness", "constrained", "without", "similarity.", "applications", "long", "local", "dissimilarity", "2019),", "dependence", "point,", "high", "follows:\u2022", "network", "convexity,", "analysis", "novel", "(yuan", "saga),", "when", "based", "improves", "(svrp,", "proximal", "ordinary", "catalyst", "achieve", "combines", "evaluations", "context", "zhang", "(lin", "svrp)", "allowing", "including", "given", "disco", "standard", "condition", "variance", "svrp.", "less", "sonata", "point", "tian", "independent", "conditioning", "subfield", "additionally,", "answer", "enough.", "point),", "that", "related", "lin(2015)developed", "(seetable", "reduction.", "catalyst-accelerated", "setting.related", "dane,", "benefit", "communication", "algorithm,", "strong", "(2022)", "assumption", "higher", "clients", "communication.\u2022", "svrp", "effective", "quadratic", "course", "(fl)", "finite-sum", "first", "operator,", "reducing", "gave", "matched", "accelerate", "(like", "allows", "provide", "variant", "accelerated", "self-concordant", "computational", "svrp,", "algorithms", "constraints.", "theory", "starting", "results", "[cs.lg]", "al.,", "also", "convexity.", "second", "workdistributed", "uniformly", "optimization,", "where", "then", "than", "shamir(2015)gave", "over", "acceleration.", "this", "steps,", "decentralized", "grown", "above", "objectives,", "algorithm", "benefits", "risk", "2020b)", "(2014)", "removing", "develop", "give", "1and", "computation", "small", "utilizes", "factors)", "dane", "main", "optimization", "client", "assumptions,", "similar", "under", "distributed", "analyzed", "even", "quadratics.", "require", "logarithmic", "2023we", "algorithms:", "collaboratively", "further", "(sppm).", "similarity", "obtain", "another", "(catalyzed", "setting", "though", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252544861": {"id": "252544861", "openalex": null, "doi": null, "title": "HIERARCHICAL SLICED WASSERSTEIN DISTANCE", "abstract": "Sliced Wasserstein (SW) distance has been widely used in different application scenarios since it can be scaled to a large number of supports without suffering from the curse of dimensionality. The value of sliced Wasserstein distance is the average of transportation cost between one-dimensional representations (projections) of original measures that are obtained by Radon Transform (RT). Despite its efficiency in the number of supports, estimating the sliced Wasserstein requires a relatively large number of projections in high-dimensional settings. Therefore, for applications where the number of supports is relatively small compared with the dimension, e.g., several deep learning applications where the mini-batch approaches are utilized, the complexities from matrix multiplication of Radon Transform become the main computational bottleneck. To address this issue, we propose to derive projections by linearly and randomly combining a smaller number of projections which are named bottleneck projections. We explain the usage of these projections by introducing Hierarchical Radon Transform (HRT) which is constructed by applying Radon Transform variants recursively. We then formulate the approach into a new metric between measures, named Hierarchical Sliced Wasserstein (HSW) distance. By proving the injectivity of HRT, we derive the metricity of HSW. Moreover, we investigate the theoretical properties of HSW including its connection to SW variants and its computational and sample complexities. Finally, we compare the computational cost and generative quality of HSW with the conventional SW on the task of deep generative modeling using various benchmark datasets including CIFAR10, CelebA, and Tiny ImageNet 1 .arXiv:2209.13570v5 [stat.ML] 6 Feb 2023Published as a conference paper at ICLR 2023 applications (Le et al., 2021;Xu et al., 2021;Yang et al., 2020). Despite the increasing importance of Wasserstein distance in applications, prior works have alluded to the concerns surrounding the high computational complexity of that distance. When the probability measures have at most n supports, the computational complexity of Wasserstein distance scales with the order of O(n 3 log n) (Pele & Werman, 2009). Additionally, it suffers from the curse of dimensionality, i.e., its sample complexity (the bounding gap of the distance between a probability measure and the empirical measures from its random samples) is of the order of O(n \u22121/d ) (Fournier & Guillin, 2015), where n is the sample size and d is the number of dimensions.Published as a conference paper at ICLR 2023 randomly from the uniform distribution over the corresponding unit-hypersphere. For the same value of L, we show that the bottleneck projection approach is faster than the conventional approach when the values of L and d are large.Contribution: In summary, our main contributions are two-fold:Published as a conference paper at ICLR 2023 Kilian Fatras, Younes Zine, R\u00e9mi Flamary, R\u00e9mi Gribonval, and Nicolas Courty. Learning with minibatch Wasserstein: asymptotic and gradient properties.", "authors": [], "concepts": ["smaller", "wasserstein", "fatras,", "average", "widely", "(sw)", "injectivity", "metric", "(rt).", "(projections)", "bottleneck.", "scales", "mini-batch", "most", "theoretical", "uniform", "surrounding", "used", "show", "younes", "projections", "concerns", "dimensionality,", "radon", "modeling", "faster", "conventional", "these", "large", "nicolas", "prior", "two-fold:published", "complexities", "despite", "been", "number", "empirical", "variants", "datasets", "from", ".arxiv:2209.13570v5", "into", "cifar10,", "relatively", "deep", "distance.", "order", "metricity", "matrix", "using", "(the", "asymptotic", "generative", "applications,", "2021;yang", "conference", "derive", "compared", "issue,", "measure", "task", "e.g.,", "supports,", "multiplication", "requires", "propose", "explain", "flamary,", "2020).", "hrt,", "measures,", "properties.", "zine,", "dimension,", "bottleneck", "dimensionality.", "complexity", "high-dimensional", "random", "transportation", "finally,", "large.contribution:", "hierarchical", "gribonval,", "2015),", "learning", "application", "probability", "contributions", "gradient", "measures", "i.e.,", "kilian", "without", "suffers", "linearly", "applications", "2021;xu", "high", "hsw.", "tiny", "distance", "importance", "cost", "constructed", "when", "combining", "corresponding", "supports", "since", "proving", "projection", "obtained", "works", "dimensions.published", "with", "introducing", "including", "projections.", "several", "\u22121/d", "efficiency", "courty.", "celeba,", "usage", "additionally,", "benchmark", "scenarios", "approach", "settings.", "that", "distribution", "connection", "randomly", "between", "scaled", "utilized,", "imagenet", "one-dimensional", "unit-hypersphere.", "werman,", "different", "approaches", "representations", "size", "address", "value", "paper", "(pele", "(hrt)", "computational", "r\u00e9mi", "sliced", "investigate", "moreover,", "2023published", "complexities.", "suffering", "[stat.ml]", "al.,", "increasing", "bounding", "various", "same", "where", "then", "sample", "formulate", "than", "quality", "over", "minibatch", "this", "which", "wasserstein:", "2009).", "(hsw)", "curse", "recursively.", "transform", "have", "small", "iclr", "main", "named", "estimating", "original", "samples)", "(fournier", "guillin,", "compare", "alluded", "summary,", "properties", "applying", "values", "become", "therefore,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252596087": {"id": "252596087", "openalex": null, "doi": null, "title": "RE-IMAGEN: RETRIEVAL-AUGMENTED TEXT-TO-IMAGE GENERATOR", "abstract": "Research on text-to-image generation has witnessed significant progress in generating diverse and photo-realistic images, driven by diffusion and auto-regressive models trained on large-scale image-text data. Though state-of-the-art models can generate high-quality images of common entities, they often have difficulty generating images of uncommon entities, such as 'Chortai (dog)' or 'Picarones (food)'. To tackle this issue, we present the Retrieval-Augmented Text-to-Image Generator (Re-Imagen), a generative model that uses retrieved information to produce high-fidelity and faithful images, even for rare or unseen entities. Given a text prompt, Re-Imagen accesses an external multi-modal knowledge base to retrieve relevant (image, text) pairs and uses them as references to generate the image. With this retrieval step, Re-Imagen is augmented with the knowledge of highlevel semantics and low-level visual details of the mentioned entities, and thus improves its accuracy in generating the entities' visual appearances. We train Re-Imagen on a constructed dataset containing (image, text, retrieval) triples to teach the model to ground on both text prompt and retrieval. Furthermore, we develop a new sampling strategy to interleave the classifier-free guidance for text and retrieval conditions to balance the text and retrieval alignment. Re-Imagen achieves significant gain on FID score over COCO and WikiImage. To further evaluate the capabilities of the model, we introduce EntityDrawBench, a new benchmark that evaluates image generation for diverse entities, from frequent to rare, across multiple object categories including dogs, foods, landmarks, birds, and characters. Human evaluation on EntityDrawBench shows that Re-Imagen can significantly improve the fidelity of generated images, especially on less frequent entities.", "authors": [], "concepts": ["state-of-the-art", "unseen", "high-quality", "strategy", "uncommon", "shows", "appearances.", "tackle", "model", "object", "them", "dataset", "they", "score", "retrieval-augmented", "information", "such", "(food)'.", "details", "balance", "multi-modal", "low-level", "multiple", "text,", "teach", "re-imagen:", "high-fidelity", "retrieved", "sampling", "images", "from", "present", "mentioned", "generation", "re-imagen", "frequent", "generative", "accuracy", "diverse", "issue,", "(re-imagen),", "large-scale", "wikiimage.", "models", "achieves", "thus", "significant", "semantics", "data.", "prompt,", "both", "furthermore,", "conditions", "generating", "improve", "'picarones", "significantly", "retrieval)", "(image,", "produce", "landmarks,", "especially", "gain", "witnessed", "retrieval.", "diffusion", "highlevel", "uses", "text", "retrieve", "model,", "constructed", "characters.", "improves", "image", "fidelity", "entities,", "categories", "generated", "difficulty", "birds,", "across", "with", "including", "train", "given", "less", "ground", "trained", "text-to-image", "common", "rare,", "prompt", "benchmark", "that", "entities.", "'chortai", "entitydrawbench", "base", "knowledge", "photo-realistic", "containing", "guidance", "entitydrawbench,", "augmented", "faithful", "images,", "text)", "dogs,", "references", "research", "retrieval", "accesses", "progress", "rare", "coco", "driven", "external", "introduce", "image.", "auto-regressive", "evaluation", "over", "this", "interleave", "pairs", "develop", "generator", "classifier-free", "have", "image-text", "alignment.", "often", "(dog)'", "visual", "evaluate", "relevant", "even", "step,", "triples", "foods,", "generate", "entities'", "human", "capabilities", "further", "evaluates", "though"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252596252": {"id": "252596252", "openalex": null, "doi": null, "title": "DIFFUSION POSTERIOR SAMPLING FOR GENERAL NOISY INVERSE PROBLEMS", "abstract": "Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring.", "authors": [], "concepts": ["sampling.", "yielding", "reconstructions", "blended", "most", "problems.", "problem", "recently", "version", "method", "resulting", "settings", "path", "such", "existing", "been", "general", "consistency", "more", "sampling", "non-uniform", "however,", "deblurring.", "iterative", "generative", "compared", "(non)linear", "desirable", "simple", "models", "complexity", "scheme", "linear", "fourier", "problems", "gradient", "solving", "significantly", "constrained", "gaussian", "without", "high", "manifold", "diffusion", "measurement", "settings,", "real-world", "powerful", "combining", "owing", "projection", "works", "with", "noisy", "work,", "that", "their", "strict", "solvers", "noise", "handle", "previous", "interestingly,", "demonstrates", "extend", "approximation", "efficiently", "statistics", "focus", "nonlinear", "inverse", "retrieval", "solvers,", "studies.", "poisson,", "phase", "also", "noiseless", "various", "under-represents", "quality", "this", "which", "solvers.", "ease", "have", "incorporate", "studied", "posterior", "step,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252683543": {"id": "252683543", "openalex": null, "doi": null, "title": "A NON-MONOTONIC SELF-TERMINATING LANGUAGE MODEL", "abstract": "Recent large-scale neural autoregressive sequence models have shown impressive performances on a variety of natural language generation tasks. However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-k sampling, and nucleus sampling. In this paper, we focus on the problem of non-terminating sequences resulting from an incomplete decoding algorithm. We first define an incomplete probable decoding algorithm which includes greedy search, top-k sampling, and nucleus sampling, beyond the incomplete decoding algorithm originally put forward by Welleck et al. (2020). We then propose a non-monotonic self-terminating language model, which significantly relaxes the constraint of monotonically increasing termination probability in the originally proposed self-terminating language model by Welleck et al. (2020), to address the issue of non-terminating sequences when using incomplete probable decoding algorithms. We prove that our proposed model prevents non-terminating sequences when using not only incomplete probable decoding algorithms but also beam search. We empirically validate our model on sequence completion tasks with various architectures. \u2020 New York University \u2021 Prescient Design, Genentech \u00a7 CIFAR Fellow Published as a conference paper at ICLR 2023 example, suppose there are two sequences in our dataset: \"I am a boy\" vs. \"I am a boy, and you are a girl.\". Our language model trained on this dataset may or may not terminate after the former. Once our model decides not to end, it should dramatically reduce the termination probability to continue. The ST language model, which monotonically increase the termination probability, cannot capture such a case, where one sequence is a prefix of another. We thus propose a non-monotonic self-terminating (NMST) language model which guarantees the consistency with respect to greedy search, beam search, top-k sampling, and nucleus sampling without monotonically increasing termination probability.", "authors": [], "concepts": ["sampling.", "decides", "(nmst)", "prescient", "paper,", "continue.", "tasks", "impressive", "welleck", "problem", "model", "genentech", "recent", "dataset", "resulting", "algorithms.", "girl.\".", "includes", "incomplete", "degenerate", "such", "sequences", "only", "prefix", "shown", "another.", "termination,", "increase", "consistency", "sampling", "forward", "non-terminating", "from", "however,", "using", "termination", "generation", "example,", "non-monotonic", "suppose", "sampling,", "conference", "(2020).", "should", "propose", "issue", "large-scale", "capture", "greedy", "models", "there", "once", "design,", "terminate", "thus", "algorithm.", "exhibit", "originally", "beam", "probability", "repetition,", "significantly", "without", "prove", "completion", "architectures.", "(2020),", "dramatically", "non-termination,", "decoding", "model,", "when", "define", "cifar", "probability.", "generated", "probable", "validate", "tasks.", "neural", "relaxes", "search,", "monotonically", "published", "with", "boy,", "trained", "boy\"", "beyond", "that", "case,", "nucleus", "constraint", "their", "language", "end,", "autoregressive", "prevents", "natural", "sequence", "york", "reduce", "empirically", "first", "address", "paper", "focus", "premature", "search.", "algorithms", "performances", "increasing", "also", "various", "where", "then", "self-terminating", "former.", "fellow", "this", "which", "undesirable", "respect", "algorithm", "after", "cannot", "university", "have", "often", "proposed", "iclr", "variety", "probability,", "top-k", "properties", "guarantees", "dataset:"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252693109": {"id": "252693109", "openalex": null, "doi": null, "title": "MEDFAIR: BENCHMARKING FAIRNESS FOR MEDICAL IMAGING", "abstract": "A multitude of work has shown that machine learning-based medical diagnosis systems can be biased against certain subgroups of people. This has motivated a growing number of bias mitigation algorithms that aim to address fairness issues in machine learning. However, it is difficult to compare their effectiveness in medical imaging for two reasons. First, there is little consensus on the criteria to assess fairness. Second, existing bias mitigation algorithms are developed under different settings, e.g., datasets, model selection strategies, backbones, and fairness metrics, making a direct comparison and evaluation based on existing results impossible. In this work, we introduce MEDFAIR, a framework to benchmark the fairness of machine learning models for medical imaging. MEDFAIR covers eleven algorithms from various categories, ten datasets from different imaging modalities, and three model selection criteria. Through extensive experiments, we find that the under-studied issue of model selection criterion can have a significant impact on fairness outcomes; while in contrast, state-of-the-art bias mitigation algorithms do not significantly improve fairness outcomes over empirical risk minimization (ERM) in both in-distribution and out-of-distribution settings. We evaluate fairness from various perspectives and make recommendations for different medical application scenarios that require different ethical principles. Our framework provides a reproducible and easy-to-use entry point for the development and evaluation of future bias mitigation algorithms in deep learning. Code is available at https://github.com/ys-zong/MEDFAIR.Published as a conference paper at ICLR 2023 Figure 1: Components of MEDFAIR benchmark. therefore aim to help diagnosis algorithms learn predictive models that are robust to confounding factors related to sensitive attribute s (Mehrabi et al., 2021).Given the importance of ensuring fairness in medical applications and the special characteristics of medical data, we argue that a systematic and rigorous benchmark is needed to evaluate the bias mitigation algorithms for medical imaging. However, a straightforward comparison of algorithmic fairness for medical imaging is difficult, as there is no consensus on a single metric for fairness of medical imaging models. Group fairness (Dwork et al., 2012; Verma & Rubin, 2018)  is a popular and intuitive definition adopted by many debiasing algorithms, which optimises for equal performance among subgroups. However, this can lead to a trade-off of increasing fairness by decreasing the performance of the advantaged group, reducing overall utility substantially. Doing so may violate the ethical principles of beneficence and non-maleficence (Beauchamp, 2003), especially for some medical applications where all subgroups need to be protected. There are also other fairness definitions, including individual fairness (Dwork et al., 2012), minimax fairness (Diana et al., 2021), counterfactual fairness (Kusner et al., 2017), etc. It is thus important to consider which definition should be used for evaluations.In addition to the use of differing evaluation metrics, different experimental designs used by existing studies prevent direct comparisons between algorithms based on the existing literature. Most obviously, each study tends to use different datasets to evaluate their debiasing algorithms, preventing direct comparisons of results. Furthermore, many bias mitigation studies focus on evaluating tabular data with low-capacity models (Madras et al., 2018; Zhao et al., 2019; Diana et al., 2021), and recent analysis has shown that their conclusions do not generalise to high-capacity deep networks used for the analysis of image data (Zietlow et al., 2022). A crucial but less obvious issue is the choice of model selection strategy for hyperparameter search and early stopping. Individual bias mitigation studies are divergent or vague in their model selection criteria, leading to inconsistent comparisons even if the same datasets are used. Finally, given the effort required to collect and annotate medical imaging data, models are usually deployed in a different domain than the domain used for data collection. (E.g., data collected at hospital A is used to train a model deployed at hospital B). While the maintenance of prediction quality across datasets has been well studied, it is unclear if fairness achieved within one dataset (in-distribution) holds under dataset shift (out-of-distribution).In order to address these challenges, we provide the first comprehensive fairness benchmark for medical imaging -MEDFAIR. We conduct extensive experiments across eleven algorithms, ten datasets, four sensitive attributes, and three model selection strategies to assess bias mitigation algorithms in both in-distribution and out-of-distribution settings. We report multiple evaluation metrics and conduct rigorous statistical tests to find whether any of the algorithms is significantly better. Having trained over 7,000 models using 6,800 GPU-hours, we have the following observations:\u2022 Bias widely exists in ERM models trained in different modalities, which is reflected in the predictive performance gap between different subgroups for multiple metrics. fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias.", "authors": [], "concepts": ["under-studied", "unclear", "state-of-the-art", "future", "learn", "widely", "mitigation", "strategy", "machine", "(beauchamp,", "each", "subgroups.", "holds", "metric", "exists", "developed", "studies", "evaluations.in", "data,", "most", "robust", "verma", "straightforward", "(zietlow", "collection.", "ensuring", "observations:\u2022", "definitions,", "model", "used", "attributes,", "addition", "challenges,", "group", "lead", "available", "recent", "dataset", "medfair:", "within", "overall", "report", "comparisons", "prevent", "medfair", "metrics.", "achieved", "certain", "inconsistent", "principles", "7,000", "these", "algorithmic", "existing", "obvious", "conclusions", "-medfair.", "out-of-distribution", "generalise", "been", "multiple", "(erm)", "high-capacity", "number", "empirical", "shown", "datasets", "counterfactual", "through", "categories,", "literature.", "performance", "from", "annotate", "systematic", "minimax", "deep", "strategies", "however,", "order", "criterion", "using", "studied,", "toolkit", "modalities,", "consider", "learning-based", "well", "many", "while", "optimises", "conference", "predictive", "reproducible", "should", "rigorous", "definition", "consensus", "e.g.,", "comprehensive", "algorithms,", "multitude", "framework", "issue", "deployed", "reflected", "factors", "stopping.", "selection", "models", "there", "tends", "networks", "learning.", "thus", "significant", "some", "360:", "conduct", "doing", "domain", "2021),", "2018)", "both", "furthermore,", "gpu-hours,", "utility", "popular", "work", "among", "biased", "finally,", "https://github.com/ys-zong/medfair.published", "intuitive", "maintenance", "perspectives", "bias.", "attribute", "hospital", "statistical", "learning", "divergent", "usually", "collect", "improve", "experimental", "application", "(out-of-distribution).in", "differing", "motivated", "debiasing", "significantly", "first,", "2022).", "characteristics", "applications", "2017),", "principles.", "having", "especially", "(in-distribution)", "other", "covers", "experiments", "backbones,", "group,", "criteria,", "argue", "settings,", "fairness", "importance", "subgroups", "2003),", "analysis", "better.", "violate", "easy-to-use", "based", "code", "outcomes", "in-distribution", "fairness.", "people.", "zhao", "image", "early", "individual", "data", "second,", "criteria", "search", "designs", "2012;", "etc.", "making", "across", "hyperparameter", "with", "eleven", "metrics,", "including", "train", "direct", "sensitive", "given", "2019;", "less", "evaluating", "non-maleficence", "point", "bias", "systems", "extensible", "components", "comparison", "trained", "adopted", "strategies,", "whether", "benchmark", "work,", "results.", "scenarios", "four", "shift", "mitigating", "settings.", "that", "related", "(madras", "tests", "confounding", "between", "needed", "their", "entry", "criteria.", "required", "used.", "recommendations", "ethical", "minimization", "collected", "preventing", "different", "benchmark.", "prediction", "impact", "2021).given", "growing", "make", "choice", "contrast,", "effectiveness", "2018;", "need", "low-capacity", "reasons.", "figure", "leading", "diana", "trade-off", "(dwork", "first", "reducing", "datasets,", "address", "paper", "focus", "benchmarking", "beneficence", "medfair,", "difficult,", "special", "provide", "experiments,", "diagnosis", "three", "help", "difficult", "(e.g.,", "algorithms", "find", "following", "metrics", "6,800", "detecting", "results", "al.,", "increasing", "(mehrabi", "also", "protected.", "outcomes;", "introduce", "various", "same", "where", "obviously,", "evaluation", "than", "quality", "over", "vague", "models.", "this", "rubin,", "which", "provides", "issues", "tabular", "assess", "risk", "little", "imaging", "medical", "development", "have", "iclr", "therefore", "2012),", "evaluate", "substantially.", "under", "important", "(kusner", "compare", "advantaged", "even", "study", "require", "effort", "imaging.", "(diana", "crucial", "impossible.", "against", "single", "equal", "extensive", "decreasing"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252715596": {"id": "252715596", "openalex": null, "doi": null, "title": "SAFE REINFORCEMENT LEARNING FROM PIXELS USING A STOCHASTIC LATENT REPRESENTATION", "abstract": "We address the problem of safe reinforcement learning from pixel observations. Inherent challenges in such settings are (1) a trade-off between reward optimization and adhering to safety constraints, (2) partial observability, and (3) high-dimensional observations. We formalize the problem in a constrained, partially observable Markov decision process framework, where an agent obtains distinct reward and safety signals. To address the curse of dimensionality, we employ a novel safety critic using the stochastic latent actor-critic (SLAC) approach. The latent variable model predicts rewards and safety violations, and we use the safety critic to train safe policies. Using well-known benchmark environments, we demonstrate competitive performance over existing approaches with respects to computational requirements, final reward return, and satisfying the safety constraints.", "authors": [], "concepts": ["demonstrate", "partially", "well-known", "requirements,", "problem", "model", "inherent", "dimensionality,", "settings", "pixel", "constraints,", "such", "existing", "safety", "actor-critic", "performance", "from", "variable", "critic", "using", "distinct", "pixels", "reinforcement", "process", "(slac)", "high-dimensional", "final", "learning", "policies.", "latent", "decision", "challenges", "competitive", "signals.", "novel", "predicts", "satisfying", "obtains", "employ", "with", "train", "reward", "observable", "observability,", "approach.", "benchmark", "return,", "markov", "between", "formalize", "adhering", "violations,", "approaches", "partial", "trade-off", "address", "observations.", "computational", "safe", "rewards", "constraints.", "where", "constrained,", "over", "framework,", "agent", "curse", "optimization", "representation", "respects", "environments,", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252715693": {"id": "252715693", "openalex": null, "doi": null, "title": "TIME WILL TELL: NEW OUTLOOKS AND A BASELINE FOR TEMPORAL MULTI-VIEW 3D OBJECT DETECTION", "abstract": "While recent camera-only 3D detection methods leverage multiple timesteps, the limited history they use significantly hampers the extent to which temporal fusion can improve object perception. Observing that existing works' fusion of multiframe images are instances of temporal stereo matching, we find that performance is hindered by the interplay between 1) the low granularity of matching resolution and 2) the sub-optimal multi-view setup produced by limited history usage. Our theoretical and empirical analysis demonstrates that the optimal temporal difference between views varies significantly for different pixels and depths, making it necessary to fuse many timesteps over long-term history. Building on our investigation, we propose to generate a cost volume from a long history of image observations, compensating for the coarse but efficient matching resolution with a more optimal multi-view matching setup. Further, we augment the per-frame monocular depth predictions used for long-term, coarse matching with short-term, fine-grained matching and find that long and short term temporal fusion are highly complementary. While maintaining high efficiency, our framework sets new stateof-the-art on nuScenes, achieving first place on the test set and outperforming previous best art by 5.2% mAP and 3.7% NDS on the validation set. Code will be released here: https://github.com/Divadi/SOLOFusion. * Equal contribution. arXiv:2210.02443v1 [cs.CV] 5 Oct 2022 = = \u2212 = \u2212 Two depth hypotheses for this vehicle seen in the current time step. Lower Localization Potential: The depth hypotheses are projected close together, making multi-view stereo matching more difficult. Larger Localization Potential: With longer history, the depth hypotheses are projected far apart, making depth estimation easier. Re fe re nc e Vi ew So ur ce Vi ew So ur ce Vi ew", "authors": [], "concepts": ["volume", "achieving", "term", "theoretical", "here:", "sub-optimal", "used", "object", "nuscenes,", "recent", "long-term,", "they", "test", "efficient", "difficult.", "will", "larger", "existing", "difference", "multiple", "depths,", "seen", "tell:", "empirical", "highly", "more", "performance", "images", "potential:", "from", "leverage", "long-term", "validation", "outlooks", "works'", "complementary.", "stereo", "many", "views", "while", "produced", "instances", "projected", "baseline", "resolution", "extent", "apart,", "pixels", "framework", "propose", "optimal", "matching", "investigation,", "coarse", "augment", "setup.", "fine-grained", "current", "outperforming", "multi-view", "https://github.com/divadi/solofusion.", "lower", "methods", "fusion", "per-frame", "improve", "hampers", "history", "significantly", "interplay", "short", "long", "granularity", "estimation", "longer", "high", "cost", "analysis", "arxiv:2210.02443v1", "fuse", "code", "necessary", "image", "localization", "stateof-the-art", "timesteps,", "history.", "varies", "place", "making", "with", "depth", "predictions", "monocular", "close", "detection", "maintaining", "observing", "that", "between", "setup", "multiframe", "previous", "timesteps", "different", "sets", "observations,", "demonstrates", "contribution.", "camera-only", "compensating", "efficiency,", "usage.", "first", "set.", "3.7%", "history,", "building", "find", "temporal", "together,", "best", "vehicle", "step.", "matching,", "over", "this", "which", "time", "released", "5.2%", "easier.", "limited", "short-term,", "[cs.cv]", "hindered", "perception.", "generate", "hypotheses", "equal", "further,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252735281": {"id": "252735281", "openalex": null, "doi": null, "title": "Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?", "abstract": "Modern deep learning involves training costly, highly overparameterized networks, thus motivating the search for sparser networks that require less compute and memory but can still be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP operates by iterative cycles of training, masking a fraction of smallest magnitude weights, rewinding unmasked weights back to an early training point, and repeating. Despite its simplicity, the underlying principles for when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed, i.e. why can't we prune to very high sparsities in one shot? We develop answers to these questions in terms of the geometry of the error landscape. First, we find that-at higher sparsities-pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey to the rewind point the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training determines a limit on the fraction of weights that can be pruned at each iteration of IMP. This analysis yields a new quantitative link between IMP performance and the Hessian eigenspectrum. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry in the algorithms used to find them.", "authors": [], "concepts": ["known", "perturbations", "what", "each", "rewinding", "motivating", "error", "subnetworks,", "particular,", "demystifying", "third,", "quantitative", "geometry", "used", "show", "limit", "they", "remain", "path", "fraction", "information", "such", "principles", "these", "only", "does", "pruned", "despite", "matching).", "revealing", "return", "highly", "barrier", "performance", "deep", "unmasking", "hypothesis:", "training?", "iterative", "magnitude", "hessian", "(imp)", "accuracy", "smallest", "them.", "fundamental", "very", "sparsities", "axial", "prune.", "matching", "networks", "iteration", "found", "thus", "rewound", "can't", "full", "networks,", "back", "imp.", "terms", "retraining", "finally,", "extract", "linear", "learning", "desired", "what's", "eigenspectrum.", "sparser", "first,", "sparse", "link", "linearly", "operates", "toward", "memory", "winning", "point,", "high", "costly,", "network", "overall,", "analysis", "when", "form", "early", "mask", "determines", "zero", "second,", "training", "flatness", "answers", "weights,", "search", "ticket's", "sparsities-pairs", "with", "identity", "finds", "lottery", "less", "point", "masks", "questions", "trained", "cycles", "mode", "elusive.", "training,", "that", "information?", "matching.", "training.", "sublevel", "between", "landscape", "weights", "convey", "landscape.", "strong", "exploit", "unmasked", "higher", "connected", "ticket", "useful", "make", "allow", "yields", "indicates", "encoded", "successive", "set.", "i.e.", "algorithms", "find", "robustness:", "progress", "results", "subspace", "simplicity,", "same", "tickets", "overparameterized", "pruning", "this", "state", "shot?", "still", "mask?", "algorithm", "masking", "near", "develop", "needed,", "repeating.", "intersects", "rewind", "small", "involves", "role", "prune", "underlying", "iterations", "existence", "tickets.", "require", "beginning", "(i.e.", "that-at", "compute", "modern"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252762275": {"id": "252762275", "openalex": null, "doi": null, "title": "AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS", "abstract": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like \"Let's think step by step\" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the \"Let's think step by step\" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations.", "authors": [], "concepts": ["chains", "each", "tasks", "question", "exceeds", "show", "step\"", "auto-cot.", "auto-cot", "paradigms.", "such", "these", "matters", "large", "composed", "thinking", "just", "prompting", "performance", "however,", "facilitate", "leads", "before", "mitigate", "gpt-3,", "simple", "requires", "propose", "demonstrations.", "models", "answering", "matches", "chain", "demonstrations", "generating", "superior", "method:", "llms", "leverages", "eliminated", "like", "automatically", "steps", "step-by-step", "i.e.,", "constructing", "come", "question.", "other", "uses", "mistakes,", "providing", "\"let's", "thought", "step", "generated", "designs", "major", "reasoning", "with", "chain-of-thought", "questions", "paradigm", "prompt", "benchmark", "manual", "that", "automatic", "(cot)", "prompting.", "language", "steps.", "hand-crafting", "efforts", "intermediate", "called", "leveraging", "find", "perform", "answer.", "also", "second", "let's", "hinges", "consistently", "task-specific", "samples", "one.", "diversity", "mistakes.", "think", "often", "effect", "step,", "generate", "one,", "complex", "construct", "(llms)", "generates", "public"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252815807": {"id": "252815807", "openalex": null, "doi": null, "title": "UNDERSTANDING EMBODIED REFERENCE WITH TOUCH-LINE TRANSFORMER", "abstract": "We study embodied reference understanding, the task of locating referents using embodied gestural signals and language references. Human studies have revealed that objects referred to or pointed to do not lie on the elbow-wrist line, a common misconception; instead, they lie on the so-called virtual touch line. However, existing human pose representations fail to incorporate the virtual touch line. To tackle this problem, we devise the touch-line transformer: It takes as input tokenized visual and textual features and simultaneously predicts the referent's bounding box and a touch-line vector. Leveraging this touch-line prior, we further devise a geometric consistency loss that encourages the co-linearity between referents and touch lines. Using the touch-line as gestural information improves model performances significantly. Experiments on the YouRefIt dataset show our method achieves a +25.0% accuracy improvement under the 0.75 IoU criterion, closing 63.6% of the gap between model and human performances. Furthermore, we computationally verify prior human studies by showing that computational models more accurately locate referents when using the virtual touch line than when using the elbow-wrist line.", "authors": [], "concepts": ["gestural", "line.", "studies", "significantly.", "tackle", "model", "tokenized", "show", "verify", "method", "referred", "devise", "dataset", "they", "information", "referent's", "existing", "prior", "misconception;", "consistency", "more", "however,", "using", "locate", "accuracy", "signals", "closing", "task", "objects", "line,", "models", "achieves", "+25.0%", "furthermore,", "reference", "features", "understanding,", "problem,", "lines.", "line", "simultaneously", "pose", "0.75", "computationally", "textual", "understanding", "references.", "experiments", "geometric", "prior,", "when", "improves", "vector.", "predicts", "takes", "input", "with", "yourefit", "locating", "common", "improvement", "that", "so-called", "between", "revealed", "language", "encourages", "transformer", "touch-line", "criterion,", "representations", "elbow-wrist", "leveraging", "referents", "showing", "computational", "fail", "transformer:", "co-linearity", "63.6%", "performances", "performances.", "accurately", "bounding", "than", "this", "touch", "instead,", "loss", "embodied", "have", "incorporate", "visual", "under", "study", "virtual", "pointed", "human", "further"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252846202": {"id": "252846202", "openalex": null, "doi": null, "title": "GRADIENT-GUIDED IMPORTANCE SAMPLING FOR LEARNING BINARY ENERGY-BASED MODELS", "abstract": "Learning energy-based models (EBMs) is known to be difficult especially on discrete data where gradient-based learning strategies cannot be applied directly. Although ratio matching is a sound method to learn discrete EBMs, it suffers from expensive computation and excessive memory requirements, thereby resulting in difficulties in learning EBMs on high-dimensional data. Motivated by these limitations, in this study, we propose ratio matching with gradient-guided importance sampling (RMwGGIS). Particularly, we use the gradient of the energy function w.r.t. the discrete data space to approximately construct the provably optimal proposal distribution, which is subsequently used by importance sampling to efficiently estimate the original ratio matching objective. We perform experiments on density modeling over synthetic discrete data, graph generation, and training Ising models to evaluate our proposed method. The experimental results demonstrate that our method can significantly alleviate the limitations of ratio matching, perform more effectively in practice, and scale to high-dimensional problems. Our implementation is available at https://github.com/divelab/RMwGGIS.", "authors": [], "concepts": ["gradient-guided", "learn", "known", "demonstrate", "generation,", "data,", "problems.", "requirements,", "effectively", "expensive", "used", "method", "excessive", "particularly,", "available", "ebms", "resulting", "modeling", "subsequently", "ebms,", "these", "scale", "limitations", "more", "sampling", "from", "strategies", "estimate", "sound", "function", "proposal", "applied", "practice,", "propose", "optimal", "models", "matching", "data.", "directly.", "high-dimensional", "graph", "learning", "experimental", "gradient", "motivated", "significantly", "study,", "suffers", "especially", "memory", "experiments", "space", "importance", "energy", "w.r.t.", "difficulties", "energy-based", "data", "training", "provably", "method.", "although", "with", "discrete", "https://github.com/divelab/rmwggis.", "limitations,", "that", "ising", "alleviate", "implementation", "distribution,", "efficiently", "gradient-based", "difficult", "perform", "results", "binary", "synthetic", "where", "matching,", "ratio", "over", "this", "which", "approximately", "cannot", "density", "computation", "proposed", "objective.", "original", "evaluate", "(ebms)", "thereby", "(rmwggis).", "construct"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "252918439": {"id": "252918439", "openalex": null, "doi": null, "title": "QuAnt: Quantum Annealing with Learnt Couplings", "abstract": "Modern quantum annealers can find high-quality solutions to combinatorial optimisation objectives given as quadratic unconstrained binary optimisation (QUBO) problems. Unfortunately, obtaining suitable QUBO forms in computer vision remains challenging and currently requires problem-specific analytical derivations. Moreover, such explicit formulations impose tangible constraints on solution encodings. In stark contrast to prior work, this paper proposes to learn QUBO forms from data through gradient backpropagation instead of deriving them. As a result, the solution encodings can be chosen flexibly and compactly. Furthermore, our methodology is general and virtually independent of the specifics of the target problem type. We demonstrate the advantages of learnt QUBOs on the diverse problem types of graph matching, 2D point cloud alignment and 3D rotation estimation. Our results are competitive with the previous quantum state of the art while requiring much fewer logical and physical qubits, enabling our method to scale to larger problems. The code and the new dataset will be open-sourced. arXiv:2210.08114v1 [quant-ph]", "authors": [], "concepts": ["challenging", "learn", "computer", "demonstrate", "high-quality", "flexibly", "backpropagation", "combinatorial", "solutions", "logical", "problems.", "problem", "method", "dataset", "proposes", "optimisation", "type.", "such", "will", "larger", "scale", "prior", "analytical", "requiring", "open-sourced.", "general", "much", "contrast", "through", "from", "virtually", "qubits,", "explicit", "while", "diverse", "arxiv:2210.08114v1", "them.", "forms", "rotation", "requires", "physical", "unfortunately,", "encodings.", "derivations.", "furthermore,", "formulations", "graph", "annealing", "(qubo)", "encodings", "stark", "compactly.", "gradient", "impose", "target", "currently", "couplings", "learnt", "types", "competitive", "constraints", "code", "problem-specific", "data", "vision", "cloud", "chosen", "with", "quantum", "obtaining", "given", "point", "independent", "work,", "instead", "estimation.", "solution", "tangible", "previous", "quant:", "quadratic", "deriving", "enabling", "result,", "paper", "objectives", "find", "fewer", "moreover,", "results", "binary", "matching,", "suitable", "qubo", "this", "state", "methodology", "specifics", "advantages", "alignment", "qubos", "annealers", "unconstrained", "remains", "modern", "[quant-ph]"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "253080708": {"id": "253080708", "openalex": null, "doi": null, "title": "FOSR: FIRST-ORDER SPECTRAL REWIRING FOR ADDRESSING OVERSQUASHING IN GNNS", "abstract": "Graph neural networks (GNNs) are able to leverage the structure of graph data by passing messages along the edges of the graph. While this allows GNNs to learn features depending on the graph structure, for certain graph topologies it leads to inefficient information propagation and a problem known as oversquashing. This has recently been linked with the curvature and spectral gap of the graph. On the other hand, adding edges to the message-passing graph can lead to increasingly similar node representations and a problem known as oversmoothing. We propose a computationally efficient algorithm that prevents oversquashing by systematically adding edges to the graph based on spectral expansion. We combine this with a relational architecture, which lets the GNN preserve the original graph structure and provably prevents oversmoothing. We find experimentally that our algorithm outperforms existing graph rewiring methods in several graph classification tasks.", "authors": [], "concepts": ["learn", "known", "edges", "problem", "recently", "messages", "lead", "experimentally", "information", "efficient", "certain", "gnns", "existing", "been", "classification", "able", "leverage", "expansion.", "leads", "along", "structure", "while", "systematically", "addressing", "spectral", "propose", "increasingly", "networks", "methods", "hand,", "graph", "features", "message-passing", "oversmoothing.", "fosr:", "computationally", "other", "first-order", "based", "oversquashing.", "data", "provably", "tasks.", "neural", "with", "several", "propagation", "outperforms", "oversquashing", "that", "curvature", "adding", "inefficient", "prevents", "representations", "relational", "allows", "linked", "preserve", "rewiring", "find", "combine", "depending", "structure,", "this", "which", "node", "algorithm", "architecture,", "graph.", "lets", "original", "passing", "similar", "(gnns)", "topologies"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "253224274": {"id": "253224274", "openalex": null, "doi": null, "title": "NOISE INJECTION NODE REGULARIZATION FOR ROBUST LEARNING", "abstract": "We introduce Noise Injection Node Regularization (NINR), a method of injecting structured noise into Deep Neural Networks (DNN) during the training stage, resulting in an emergent regularizing effect.We present theoretical and empirical evidence for substantial improvement in robustness against various test data perturbations for feed-forward DNNs when trained under NINR.The novelty in our approach comes from the interplay of adaptive noise injection and initialization conditions such that noise is the dominant driver of dynamics at the start of training.As it simply requires the addition of external nodes without altering the existing network structure or optimization algorithms, this method can be easily incorporated into many standard problem specifications.We find improved stability against a number of data perturbations, including domain shifts, with the most dramatic improvement obtained for unstructured noise, where our technique outperforms other existing methods such as Dropout or L 2 regularization, in some cases.We further show that desirable generalization properties on clean data are generally maintained.", "authors": [], "concepts": ["cases.we", "unstructured", "nodes", "perturbations", "stage,", "emergent", "most", "robust", "problem", "theoretical", "addition", "injecting", "show", "comes", "method", "feed-forward", "resulting", "test", "regularization,", "altering", "such", "existing", "generally", "number", "empirical", "incorporated", "adaptive", "from", "into", "present", "deep", "clean", "many", "structure", "desirable", "perturbations,", "algorithms,", "requires", "generalization", "networks", "some", "domain", "conditions", "(dnn)", "methods", "training.as", "start", "improved", "learning", "dnns", "effect.we", "novelty", "interplay", "without", "noise,", "other", "network", "stability", "dropout", "driver", "when", "regularization", "ninr.the", "dominant", "data", "training", "maintained.", "obtained", "neural", "with", "including", "standard", "trained", "improvement", "approach", "outperforms", "that", "noise", "easily", "structured", "robustness", "specifications.we", "technique", "simply", "find", "shifts,", "external", "introduce", "various", "where", "dynamics", "this", "during", "evidence", "node", "initialization", "injection", "(ninr),", "regularizing", "optimization", "substantial", "under", "properties", "against", "dramatic", "further"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "253255190": {"id": "253255190", "openalex": null, "doi": null, "title": "CHARACTERIZING INTRINSIC COMPOSITIONALITY IN TRANSFORMERS WITH TREE PROJECTIONS", "abstract": "When trained on language data, do transformers learn some arbitrary computation that utilizes the full capacity of the architecture or do they learn a simpler, treelike computation, hypothesized to underlie compositional meaning systems like human languages? There is an apparent tension between compositional accounts of human language understanding, which are based on a restricted bottom-up computational process, and the enormous success of neural models like transformers, which can route information arbitrarily between different parts of their input. One possibility is that these models, while extremely flexible in principle, in practice learn to interpret language hierarchically, ultimately building sentence representations close to those predictable by a bottom-up, tree-structured model. To evaluate this possibility, we describe an unsupervised and parameter-free method to functionally project the behavior of any transformer into the space of tree-structured networks. Given an input sentence, we produce a binary tree that approximates the transformer's representation-building process and a score that captures how \"treelike\" the transformer's behavior is on the input. While calculation of this score does not require training any additional models, it provably upper-bounds the fit between a transformer and any tree-structured approximation. Using this method, we show that transformers for three different tasks become more tree-like over the course of training, in some cases unsupervisedly recovering the same trees as supervised parsers. These trees, in turn, are predictive of model behavior, with more tree-like models generalizing better on tests of compositional generalization. arXiv:2211.01288v2 [cs.CL] 3 Nov 2022 Prepint. Under Review. Transformer Encoder \u2248 f g \u03d5 proj t r e e s c o r e red apples delicious are Transformer Encoder red apples delicious are red apples delicious are", "authors": [], "concepts": ["learn", "proj", "generalization.", "captures", "data,", "tasks", "characterizing", "computation,", "model", "show", "behavior,", "method", "restricted", "projections", "they", "score", "[cs.cl]", "cases", "networks.", "information", "these", "does", "arxiv:2211.01288v2", "more", "intrinsic", "approximation.", "into", "extremely", "using", "flexible", "route", "while", "tree", "meaning", "predictive", "possibility", "apparent", "models", "there", "method,", "generalizing", "process", "some", "full", "possibility,", "understanding,", "better", "trees", "like", "additional", "interpret", "hypothesized", "upper-bounds", "produce", "parts", "space", "approximates", "success", "accounts", "sentence", "when", "based", "representation-building", "unsupervisedly", "transformer's", "apples", "compositionality", "input", "training", "principle,", "provably", "transformers,", "neural", "input.", "with", "behavior", "given", "systems", "trained", "project", "underlie", "close", "capacity", "training,", "that", "tests", "between", "transformers", "hierarchically,", "their", "predictable", "trees,", "parameter-free", "arbitrary", "language", "different", "prepint.", "those", "transformer", "simpler,", "model.", "course", "representations", "\"treelike\"", "tree-structured", "arbitrarily", "functionally", "unsupervised", "three", "building", "computational", "delicious", "turn,", "review.", "binary", "tension", "calculation", "languages?", "same", "recovering", "over", "this", "which", "enormous", "process,", "computation", "bottom-up", "utilizes", "bottom-up,", "evaluate", "practice", "under", "architecture", "compositional", "describe", "require", "sentence,", "tree-like", "supervised", "treelike", "models,", "ultimately", "human", "parsers.", "become", "encoder"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "253510295": {"id": "253510295", "openalex": null, "doi": null, "title": "EDGE GUIDED GANS WITH CONTRASTIVE LEARNING FOR SEMANTIC IMAGE SYNTHESIS", "abstract": "We propose a novel edge guided generative adversarial network with contrastive learning (ECGAN) for the challenging semantic image synthesis task. Although considerable improvement has been achieved, the quality of synthesized images is far from satisfactory due to three largely unresolved challenges. 1) The semantic labels do not provide detailed structural information, making it difficult to synthesize local details and structures. 2) The widely adopted CNN operations such as convolution, down-sampling, and normalization usually cause spatial resolution loss and thus cannot fully preserve the original semantic information, leading to semantically inconsistent results (e.g., missing small objects). 3) Existing semantic image synthesis methods focus on modeling \"local\" semantic information from a single input semantic layout. However, they ignore \"global\" semantic information of multiple input semantic layouts, i.e., semantic cross-relations between pixels across different input layouts. To tackle 1), we propose to use edge as an intermediate representation which is further adopted to guide image generation via a proposed attention guided edge transfer module. Edge information is produced by a convolutional generator and introduces detailed structure information. To tackle 2), we design an effective module to selectively highlight class-dependent feature maps according to the original semantic layout to preserve the semantic information. To tackle 3), inspired by current methods in contrastive learning, we propose a novel contrastive learning method, which aims to enforce pixel embeddings belonging to the same semantic class to generate more similar image content than those from different classes. Doing so can capture more semantic relations by explicitly exploring the structures of labeled pixels from multiple input semantic layouts. Experiments on three challenging datasets show that our ECGAN achieves significantly better results than state-of-the-art methods.Figure 1: Overview of the proposed ECGAN. It consists of a parameter-sharing encoder E, an edge generator G e , an image generator G i , an attention guided edge transfer module G t , a label generator G l , a similarity loss module, a contrastive learning module G c (not shown for brevity), and a multimodality discriminator D. G e and G i are connected by G t from two levels, i.e., edge feature-level and content-level, to generate realistic images. G s is proposed to preserve the semantic information of the input semantic labels. G l aims to transfer the generated image back to the label for calculating the similarity loss. G c tries to capture more semantic relations by explicitly exploring the structures of labeled pixels from multiple input semantic layouts. D aims to distinguish the outputs from two modalities, i.e., edge and image. The symbol c denotes channel-wise concatenation.Published as a conference paper at ICLR 2023 Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 6", "authors": [], "concepts": ["state-of-the-art", "challenging", "aims", "real-time", "widely", "layouts,", "fully", "enforce", "learning,", "concatenation.published", "structures.", "cause", "guided", "tackle", "show", "\"global\"", "they", "pixel", "inspired", "huang", "information", "modeling", "missing", "such", "consists", "inconsistent", "spatial", "existing", "details", "normalization.", "feature", "been", "multiple", "belonging", "\"local\"", "discriminator", "shown", "datasets", "adaptive", "more", "images", "from", "feature-level", "however,", "overview", "objects).", "synthesis", "ecgan.", "explicitly", "modalities,", "convolutional", "generation", "generative", "structure", "contrastive", "produced", "conference", "serge", "resolution", "pixels", "propose", "information,", "capture", "according", "labels.", "labeled", "achieves", "method,", "thus", "current", "doing", "transfer", "relations", "back", "channel-wise", "methods", "tries", "information.", "exploring", "better", "satisfactory", "learning", "usually", "structural", "layouts.", "i.e.,", "significantly", "outputs", "local", "semantically", "considerable", "experiments", "network", "detailed", "layout.", "classes.", "novel", "embeddings", "label", "class", "synthesize", "image", "highlight", "down-sampling,", "content", "layout", "module,", "generated", "input", "(not", "introduces", "parameter-sharing", "although", "making", "content-level,", "across", "with", "(ecgan)", "challenges.", "achieved,", "design", "structures", "gans", "adopted", "maps", "improvement", "task.", "that", "selectively", "between", "semantic", "methods.figure", "arbitrary", "different", "normalization", "2017.", "those", "denotes", "connected", "guide", "intermediate", "effective", "cross-relations", "distinguish", "module", "attention", "leading", "ecgan", "paper", "focus", "symbol", "provide", "realistic", "labels", "multimodality", "preserve", "three", "difficult", "(e.g.,", "results", "style", "images.", "same", "image.", "levels,", "instance", "than", "quality", "which", "operations", "loss.", "module.", "adversarial", "class-dependent", "loss", "cannot", "generator", "largely", "ignore", "calculating", "small", "proposed", "iclr", "belongie.", "convolution,", "original", "similar", "representation", "unresolved", "iccv,", "generate", "synthesized", "brevity),", "further", "single", "similarity", "encoder", "edge"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "253523474": {"id": "253523474", "openalex": null, "doi": null, "title": "CHARACTERIZING THE SPECTRUM OF THE NTK VIA A POWER SERIES EXPANSION", "abstract": "Under mild conditions on the network initialization we derive a power series expansion for the Neural Tangent Kernel (NTK) of arbitrarily deep feedforward networks in the infinite width limit. We provide expressions for the coefficients of this power series which depend on both the Hermite coefficients of the activation function as well as the depth of the network. We observe faster decay of the Hermite coefficients leads to faster decay in the NTK coefficients and explore the role of depth. Using this series, first we relate the effective rank of the NTK to the effective rank of the inputdata Gram. Second, for data drawn uniformly on the sphere we study the eigenvalues of the NTK, analyzing the impact of the choice of activation function. Finally, for generic data and activation functions with sufficiently fast Hermite coefficient decay, we derive an asymptotic upper bound on the spectrum of the NTK.", "authors": [], "concepts": ["bound", "characterizing", "ntk,", "faster", "fast", "explore", "deep", "series,", "leads", "using", "well", "decay", "asymptotic", "function", "decay,", "derive", "networks", "mild", "both", "tangent", "ntk.", "conditions", "(ntk)", "finally,", "hermite", "expansion", "sufficiently", "analyzing", "feedforward", "limit.", "coefficients", "infinite", "activation", "relate", "network", "gram.", "observe", "data", "series", "second,", "neural", "inputdata", "with", "rank", "upper", "depth", "network.", "functions", "expressions", "function.", "impact", "eigenvalues", "effective", "width", "choice", "first", "arbitrarily", "provide", "kernel", "uniformly", "generic", "depth.", "power", "this", "which", "initialization", "drawn", "role", "under", "coefficient", "spectrum", "depend", "study", "sphere"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "253553242": {"id": "253553242", "openalex": null, "doi": null, "title": "Asynchronous Gradient Play in Zero-Sum Multi-agent Games", "abstract": "Finding equilibria via gradient play in competitive multi-agent games has been attracting a growing amount of attention in recent years, with emphasis on designing efficient strategies where the agents operate in a decentralized and symmetric manner with guaranteed convergence. While significant efforts have been made in understanding zero-sum two-player matrix games, the performance in zero-sum multiagent games remains inadequately explored, especially in the presence of delayed feedbacks, leaving the scalability and resiliency of gradient play open to questions. In this paper, we make progress by studying asynchronous gradient plays in zero-sum polymatrix games under delayed feedbacks. We first establish that the last iterate of entropy-regularized optimistic multiplicative weight updates (OMWU) method converges linearly to the quantal response equilibrium (QRE), the solution concept under bounded rationality, in the absence of delays. While the linear convergence continues to hold even when the feedbacks are randomly delayed under mild statistical assumptions, it converges at a noticeably slower rate due to a smaller tolerable range of learning rates. Moving beyond, we demonstrate entropy-regularized OMWUby adopting two-timescale learning rates in a delay-aware manner-enjoys faster last-iterate convergence under fixed delays, and continues to converge provably even when the delays are arbitrarily bounded in an average-iterate manner. Our methods also lead to finite-time guarantees to approximate the Nash equilibrium (NE) by moderating the amount of regularization. To the best of our knowledge, this work is the first that aims to understand asynchronous gradient play in zero-sum polymatrix games under a wide range of delay assumptions, highlighting the role of learning rates separation.", "authors": [], "concepts": ["smaller", "quantal", "aims", "demonstrate", "moving", "understand", "paper,", "guaranteed", "(qre),", "approximate", "rate", "slower", "method", "lead", "recent", "rationality,", "delay-aware", "efficient", "faster", "concept", "symmetric", "hold", "feedbacks,", "updates", "been", "moderating", "performance", "converges", "regularization.", "strategies", "omwuby", "matrix", "presence", "while", "open", "asynchronous", "games", "agents", "rates", "delays,", "convergence", "convergence.", "wide", "significant", "feedbacks.", "mild", "manner", "methods", "equilibrium", "work", "made", "beyond,", "linear", "response", "highlighting", "statistical", "learning", "questions.", "absence", "designing", "gradient", "scalability", "iterate", "linearly", "especially", "nash", "continues", "understanding", "delays", "delays.", "competitive", "multiplicative", "when", "rates.", "noticeably", "delay", "finite-time", "resiliency", "establish", "provably", "adopting", "two-player", "with", "last", "bounded", "zero-sum", "play", "leaving", "studying", "that", "randomly", "finding", "fixed", "converge", "games,", "feedbacks", "solution", "manner-enjoys", "optimistic", "weight", "efforts", "polymatrix", "(omwu)", "plays", "growing", "make", "two-timescale", "explored,", "attention", "range", "first", "operate", "arbitrarily", "multi-agent", "amount", "(ne)", "progress", "multiagent", "also", "best", "where", "entropy-regularized", "last-iterate", "this", "decentralized", "separation.", "average-iterate", "have", "role", "assumptions,", "under", "tolerable", "even", "remains", "inadequately", "delayed", "equilibria", "knowledge,", "guarantees", "manner.", "emphasis", "years,", "attracting"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "254044338": {"id": "254044338", "openalex": null, "doi": null, "title": "A KERNEL PERSPECTIVE OF SKIP CONNECTIONS IN CONVOLUTIONAL NETWORKS", "abstract": "Over-parameterized residual networks are amongst the most successful convolutional neural architectures for image processing. Here we study their properties through their Gaussian Process and Neural Tangent kernels. We derive explicit formulas for these kernels, analyze their spectra and provide bounds on their implied condition numbers. Our results indicate that (1) with ReLU activation, the eigenvalues of these residual kernels decay polynomially at a similar rate as the same kernels when skip connections are not used, thus maintaining a similar frequency bias; (2) however, residual kernels are more locally biased. Our analysis further shows that the matrices obtained by these residual kernels yield favorable condition numbers at finite depths than those obtained without the skip connections, enabling therefore faster convergence of training with gradient descent.Published as a conference paper at ICLR 2023", "authors": [], "concepts": ["activation,", "shows", "here", "most", "descent.published", "rate", "locally", "faster", "these", "bias;", "kernels", "through", "more", "however,", "convolutional", "relu", "decay", "explicit", "conference", "derive", "connections,", "frequency", "convergence", "networks", "thus", "process", "tangent", "amongst", "implied", "favorable", "successful", "gradient", "gaussian", "without", "numbers", "perspective", "polynomially", "yield", "analysis", "when", "indicate", "image", "spectra", "formulas", "training", "obtained", "neural", "with", "depths", "condition", "over-parameterized", "maintaining", "kernels,", "that", "kernels.", "their", "analyze", "those", "eigenvalues", "residual", "processing.", "numbers.", "used,", "enabling", "connections", "paper", "skip", "architectures", "provide", "kernel", "results", "biased.", "same", "than", "finite", "bounds", "iclr", "therefore", "similar", "study", "matrices", "properties", "further"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "254198988": {"id": "254198988", "openalex": null, "doi": null, "title": "OVER-TRAINING WITH MIXUP MAY HURT GENERALIZATION", "abstract": "Mixup, which creates synthetic training instances by linearly interpolating random sample pairs, is a simple and yet effective regularization technique to boost the performance of deep models trained with SGD. In this work, we report a previously unobserved phenomenon in Mixup training: on a number of standard datasets, the performance of Mixup-trained models starts to decay after training for a large number of epochs, giving rise to a U-shaped generalization curve. This behavior is further aggravated when the size of original dataset is reduced. To help understand such a behavior of Mixup, we show theoretically that Mixup training may introduce undesired data-dependent label noises to the synthesized data. Via analyzing a least-square regression problem with a random feature model, we explain why noisy labels may cause the U-shaped curve to occur: Mixup improves generalization through fitting the clean patterns at the early training stage, but as training progresses, Mixup becomes over-fitting to the noise in the synthetic data. Extensive experiments are performed on a variety of benchmark datasets, validating this explanation. * Equal contribution.Published as a conference paper at ICLR 2023 epochs, the generalization performance of the network measured by its testing error may exhibit a U-shaped curve.Figure 1shows such a curve obtained from over-training ResNet18 with Mixup on CIFAR10. As can be seen fromFigure 1, after training with Mixup for a long time (200 epochs), both ERM and Mixup keep decreasing their training loss, but the testing error of the Mixup-trained ResNet18 gradually increases, while that of the ERM-trained ResNet18 continues to decrease.Motivated by this observation, we conduct a theoretical analysis, aiming to better understand the aforementioned behavior of Mixup training. We show theoretically that Mixup training may introduce undesired data-dependent label noises to the synthesized data. Then by analyzing the gradientdescent dynamics of training a random feature model for a least-square regression problem, we explain why noisy labels may cause the U-shaped curve to occur: under label noise, the early phase of training is primarily driven by the clean data pattern, which moves the model parameter closer to the correct solution. But as training progresses, the effect of label noise accumulates through iterations and gradually over-weighs that of the clean pattern and dominates the late training process. In this phase, the model parameter gradually moves away from the correct solution until it is sufficient apart and approaches a location depending on the noise realization.", "authors": [], "concepts": ["starts", "stage,", "understand", "aforementioned", "error", "cause", "problem", "theoretical", "model", "show", "epochs),", "dataset", "occur:", "report", "epochs,", "such", "large", "late", "correct", "observation,", "feature", "seen", "mixup,", "number", "training:", "through", "performance", "from", "deep", "pattern,", "location", "clean", "decay", "while", "instances", "conference", "regression", "mixup", "simple", "explain", "models", "generalization", "curve", "data-dependent", "until", "data.", "increases,", "conduct", "gradientdescent", "away", "both", "exhibit", "random", "hurt", "accumulates", "better", "loss,", "problem,", "analyzing", "erm-trained", "analysis,", "sgd.", "linearly", "noise,", "closer", "apart", "long", "continues", "previously", "boost", "experiments", "becomes", "over-training", "contribution.published", "network", "creates", "explanation.", "model,", "pattern", "validating", "when", "label", "aiming", "regularization", "improves", "early", "rise", "unobserved", "keep", "data", "training", "phenomenon", "theoretically", "obtained", "with", "progresses,", "noisy", "sufficient", "behavior", "standard", "decrease.motivated", "phase,", "trained", "pairs,", "benchmark", "work,", "realization.", "mixup-trained", "that", "training.", "primarily", "over-weighs", "their", "noise", "(200", "giving", "testing", "curve.figure", "solution", "aggravated", "resnet18", "gradually", "effective", "patterns", "solution.", "1shows", "technique", "approaches", "datasets,", "size", "paper", "performed", "measured", "over-fitting", "labels", "parameter", "help", "depending", "phase", "synthetic", "driven", "introduce", "dominates", "then", "sample", "dynamics", "this", "u-shaped", "which", "fromfigure", "noises", "after", "undesired", "time", "iclr", "original", "cifar10.", "iterations", "process.", "under", "least-square", "fitting", "effect", "curve.", "reduced.", "interpolating", "variety", "synthesized", "further", "equal", "extensive", "moves", "decreasing"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "254221022": {"id": "254221022", "openalex": null, "doi": null, "title": "UNIKGQA: UNIFIED RETRIEVAL AND REASONING FOR SOLVING MULTI-HOP QUESTION ANSWERING OVER KNOWLEDGE GRAPH", "abstract": "Multi-hop Question Answering over Knowledge Graph (KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG). To cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately. Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model (PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs. For parameter learning, we design a shared pre-training task based on questionrelation matching for both retrieval and reasoning models, and then propose retrieval-and reasoning-oriented fine-tuning strategies. Compared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task. Our codes and data are", "authors": [], "concepts": ["aims", "unified", "paper,", "learning,", "kgqa", "edges", "performs", "(kgqa)", "solutions", "unikgqa,", "question", "model", "unikgqa:", "method", "information", "consists", "these", "existing", "vast", "subgraph", "multiple", "pre-training", "unified,", "highly", "datasets", "more", "retrieval-and", "questionrelation", "from", "relatively", "retrieves", "mentioned", "essence.", "strategies.", "unikgqa", "along", "reasoning-oriented", "technical", "compared", "demonstrated", "task", "very", "propose", "large-scale", "codes", "answering", "matching", "tightly", "task,", "learning.", "approach:", "away", "studies,", "both", "multi-hop", "unifying", "graph", "work", "usually", "neglecting", "kgs.", "solving", "adopts", "experiments", "cope", "two-stage", "novel", "based", "shared", "data", "search", "although", "reasoning", "with", "(plm)", "space,", "design", "accurately.", "developing", "benchmark", "answer", "propagation", "approach", "task.", "that", "related", "(kg).", "their", "semantic", "knowledge", "language", "previous", "fine-tuning", "different", "entities", "natural", "module", "effectiveness", "first", "hops", "parameter", "three", "find", "retrieval", "propagate", "stages", "directed", "then", "matching,", "topic", "over", "this", "employs", "architecture,", "have", "stages.", "small", "pre-trained", "architecture", "question-relation", "models,", "relatedness", "relating", "extensive", "related,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "254926490": {"id": "254926490", "openalex": null, "doi": null, "title": "TASK AMBIGUITY IN HUMANS AND LANGUAGE MODELS", "abstract": "Language models have recently achieved strong performance across a wide range of NLP benchmarks. However, unlike benchmarks, real world tasks are often poorly specified, and agents must deduce the user's intended behavior from a combination of context, instructions, and examples. We investigate how both humans and models behave in the face of such task ambiguity by proposing AmbiBench, a new benchmark of six ambiguously-specified classification tasks. We evaluate humans and models on AmbiBench by seeing how well they identify the intended task using 1) instructions with varying degrees of ambiguity, and 2) different numbers of labeled examples. We find that the combination of model scaling (to 175B parameters) and training with human feedback data enables models to approach or exceed the accuracy of human participants across tasks, but that either one alone is not sufficient. In addition, we show how to dramatically improve the accuracy of language models trained without large-scale human feedback training by finetuning on a small number of ambiguous in-context examples, providing a promising direction for teaching models to generalize well in the face of ambiguity.", "authors": [], "concepts": ["combination", "ambiguity", "teaching", "must", "poorly", "sufficient.", "parameters)", "tasks", "ambiguity.", "humans", "model", "recently", "show", "they", "ambibench", "alone", "either", "achieved", "such", "ambiguous", "instructions,", "number", "classification", "performance", "from", "however,", "using", "proposing", "well", "accuracy", "benchmarks,", "task", "agents", "varying", "large-scale", "models", "labeled", "wide", "both", "scaling", "direction", "improve", "examples,", "without", "numbers", "degrees", "dramatically", "promising", "175b", "providing", "behave", "unlike", "addition,", "data", "training", "tasks.", "ambibench,", "world", "across", "with", "behavior", "identify", "examples.", "trained", "finetuning", "ambiguity,", "benchmark", "specified,", "approach", "that", "deduce", "real", "strong", "language", "in-context", "different", "ambiguously-specified", "generalize", "range", "feedback", "instructions", "find", "investigate", "context,", "exceed", "seeing", "tasks,", "face", "enables", "benchmarks.", "have", "small", "often", "intended", "evaluate", "human", "participants", "user's"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "255749563": {"id": "255749563", "openalex": null, "doi": null, "title": "NEURAL SYSTEMATIC BINDER", "abstract": "The key to high-level cognition is believed to be the ability to systematically manipulate and compose knowledge pieces. While token-like structured knowledge representations are naturally provided in text, it is elusive how to obtain them for unstructured modalities such as scene images. In this paper, we propose a neural mechanism called Neural Systematic Binder or SysBinder for constructing a novel structured representation called Block-Slot Representation. In Block-Slot Representation, object-centric representations known as slots are constructed by composing a set of independent factor representations called blocks, to facilitate systematic generalization. SysBinder obtains this structure in an unsupervised way by alternatingly applying two different binding principles: spatial binding for spatial modularity across the full scene and factor binding for factor modularity within an object. SysBinder is a simple, deterministic, and general-purpose layer that can be applied as a drop-in module in any arbitrary neural network and on any modality. In experiments, we find that SysBinder provides significantly better factor disentanglement within the slots than the conventional object-centric methods, including, for the first time, in visually complex scene images such as CLEVR-Tex. Furthermore, we demonstrate factor-level systematicity in controlled scene generation by decoding unseen factor combinations. https://sites. . Savi++: Towards end-to-end object-centric learning from real-world videos. ArXiv, abs/2206.07764, 2022. . Object files and schemata: Factorizing declarative and procedural knowledge in dynamical systems. ArXiv, abs/2006.", "authors": [], "concepts": ["sysbinder", "known", "unstructured", "demonstrate", "unseen", "binding", "paper,", "generalization.", "block-slot", "modularity", "composing", "disentanglement", "object", "provided", "them", "token-like", "simple,", "systems.", "within", "videos.", "elusive", "methods,", "conventional", "such", "visually", "spatial", "text,", "images", "from", "declarative", "systematic", "facilitate", "generation", "drop-in", "blocks,", "structure", "while", "object-centric", "systematically", "towards", "alternatingly", "applied", "propose", "scene", "modalities", "compose", "furthermore,", "full", "better", "time,", "learning", "naturally", "high-level", "abs/2206.07764,", "significantly", "https://sites.", "ability", "arxiv,", "end-to-end", "constructing", "manipulate", "files", "network", "decoding", "novel", "constructed", "real-world", "obtains", "clevr-tex.", "principles:", "neural", "across", "factorizing", "systematicity", "procedural", "independent", "that", "binder", "pieces.", "knowledge", "representation,", "object.", "modality.", "arbitrary", "different", "representation.", "schemata:", "dynamical", "structured", "abs/2006.", "module", "combinations.", "slots", "representations", "called", "factor-level", "first", "experiments,", "controlled", "believed", "unsupervised", "general-purpose", "find", "deterministic,", "layer", "images.", "than", "2022.", "this", "provides", "factor", "cognition", "savi++:", "representation", "mechanism", "applying", "including,", "obtain", "complex"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "256105083": {"id": "256105083", "openalex": null, "doi": null, "title": "Learning to Reject with a Fixed Predictor: Application to Decontextualization", "abstract": "We study the problem of classification with a reject option for a fixed predictor, applicable in natural language processing. We introduce a new problem formulation for this scenario, and an algorithm minimizing a new surrogate loss function. We provide a complete theoretical analysis of the surrogate loss function with a strong H-consistency guarantee. For evaluation, we choose the decontextualization task, and provide a manually-labelled dataset of 2,000 examples. Our algorithm significantly outperforms the baselines considered, with a \u223c 25% improvement in coverage when halving the error rate, which is only \u223c3% away from the theoretical limit.", "authors": [], "concepts": ["minimizing", "error", "problem", "manually-labelled", "theoretical", "dataset", "complete", "decontextualization", "coverage", "only", "classification", "h-consistency", "from", "2,000", "reject", "function", "task,", "away", "learning", "application", "considered,", "limit.", "rate,", "significantly", "analysis", "when", "predictor,", "with", "examples.", "choose", "option", "improvement", "outperforms", "scenario,", "function.", "fixed", "strong", "language", "halving", "processing.", "natural", "evaluation,", "baselines", "provide", "formulation", "predictor:", "introduce", "guarantee.", "this", "which", "algorithm", "loss", "applicable", "study", "surrogate"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "256416103": {"id": "256416103", "openalex": null, "doi": null, "title": "WHEN SOURCE-FREE DOMAIN ADAPTATION MEETS LEARNING WITH NOISY LABELS", "abstract": "Recent state-of-the-art source-free domain adaptation (SFDA) methods have focused on learning meaningful cluster structures in the feature space, which have succeeded in adapting the knowledge from source domain to unlabeled target domain without accessing the private source data. However, existing methods rely on the pseudo-labels generated by source models that can be noisy due to domain shift. In this paper, we study SFDA from the perspective of learning with label noise (LLN). Unlike the label noise in the conventional LLN scenario, we prove that the label noise in SFDA follows a different distribution assumption. We also prove that such a difference makes existing LLN methods that rely on their distribution assumptions unable to address the label noise in SFDA. Empirical evidence suggests that only marginal improvements are achieved when applying the existing LLN methods to solve the SFDA problem. On the other hand, although there exists a fundamental difference between the label noise in the two scenarios, we demonstrate theoretically that the early-time training phenomenon (ETP), which has been previously observed in conventional label noise settings, can also be observed in the SFDA problem. Extensive experiments demonstrate significant improvements to existing SFDA algorithms by leveraging ETP to address the label noise in SFDA. . Learning with instancedependent label noise: A sample sieve approach. arXiv preprint arXiv:", "authors": [], "concepts": ["improvements", "state-of-the-art", "meaningful", "pseudo-labels", "demonstrate", "paper,", "exists", "source-free", "observed", "recent", "accessing", "achieved", "conventional", "such", "follows", "existing", "only", "difference", "feature", "been", "empirical", "marginal", "arxiv", "from", "sfda.", "problem.", "however,", "suggests", "meets", "fundamental", "models", "there", "significant", "data.", "solve", "domain", "methods", "hand,", "private", "learning", "target", "sieve", "without", "rely", "prove", "sfda", "focused", "(lln).", "source", "perspective", "other", "previously", "experiments", "(sfda)", "settings,", "when", "label", "unlike", "makes", "generated", "training", "phenomenon", "theoretically", "although", "unable", "with", "noisy", "space,", "instancedependent", "structures", "approach.", "adaptation", "shift.", "assumptions", "cluster", "that", "noise:", "scenario,", "distribution", "(etp),", "preprint", "between", "their", "noise", "knowledge", "early-time", "different", "unlabeled", "leveraging", "address", "labels", "algorithms", "also", "sample", "this", "which", "evidence", "assumption.", "adapting", "have", "succeeded", "study", "arxiv:", "applying", "extensive", "scenarios,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "256503523": {"id": "256503523", "openalex": null, "doi": null, "title": "Neural Common Neighbor with Completion for Link Prediction", "abstract": "Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between the training and test set, loss of common neighbor information, and performance degradation of models. Therefore, we propose two intervention methods: common neighbor completion and target link removal. Combining the two methods with NCN, we propose Neural Common Neighbor with Completion (NCNC). NCN and NCNC outperform recent strong baselines by large margins. NCNC achieves state-ofthe-art performance in link prediction tasks. Our code is available at https://github.com/ GraphPKU/NeuralCommonNeighbor. Preprint. Under review.Figure 1. The failure of MPNN in link prediction task. v2 and v3 have equal MPNN node representations due to symmetry. However, with different pairwise relations, (v1, v2) and (v1, v3) should have nonequal representations.", "authors": [], "concepts": ["outstanding", "nodes", "each", "outperform", "failure", "preprint.", "ubiquitous", "ignores", "available", "recent", "test", "avoids", "these", "large", "only", "margins.", "despite", "upgrade", "much", "performance", "problem.", "set,", "however,", "pairwise", "leads", "while", "https://github.com/", "learnable", "representations.", "should", "them.", "propose", "information,", "capture", "models", "achieves", "thus", "some", "methods", "graph", "features", "(ncn),", "(v1,", "usually", "removal.", "target", "output", "link", "produce", "completion", "boost", "network", "uses", "methods:", "degradation", "code", "mpnn", "improves", "state-ofthe-art", "combining", "individual", "unobserved", "input", "training", "tasks.", "shifts", "neural", "with", "fails", "unlearnable", "ncnc", "message", "relations,", "common", "manual", "maintaining", "improvement", "(mpnn)", "task.", "distribution", "individually", "between", "review.figure", "strong", "different", "(ncnc).", "hand-crafted", "prediction", "relation", "incompleteness", "contrast,", "representations", "features.", "vanilla", "baselines", "graphpku/neuralcommonneighbor.", "various", "neighbor", "models.", "tasks,", "this", "which", "still", "node", "symmetry.", "loss", "others", "simplification", "have", "ncn,", "directly", "passing", "under", "nonequal", "study", "intervention", "room", "scalability,", "applying", "further", "equal", "therefore,", "though"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "256615681": {"id": "256615681", "openalex": null, "doi": null, "title": "ON THE COMPLEXITY OF NONSMOOTH AUTOMATIC DIFFERENTIATION", "abstract": "Using the notion of conservative gradient, we provide a simple model to estimate the computational costs of the backward and forward modes of algorithmic differentiation for a wide class of nonsmooth programs. The overhead complexity of the backward mode turns out to be independent of the dimension when using programs with locally Lipschitz semi-algebraic or definable elementary functions. This considerably extends Baur-Strassen's smooth cheap gradient principle. We illustrate our results by establishing fast backpropagation results of conservative gradients through feedforward neural networks with standard activation and loss functions. Nonsmooth backpropagation's cheapness contrasts with concurrent forward approaches, which have, to this day, dimensional-dependent worst-case overhead estimates. We provide further results suggesting the superiority of backward propagation of conservative gradients. Indeed, we relate the complexity of computing a large number of directional derivatives to that of matrix multiplication, and we show that finding two subgradients in the Clarke subdifferential of a function is an NP-hard problem.", "authors": [], "concepts": ["clarke", "differentiation", "indeed,", "backpropagation", "definable", "cheapness", "day,", "model", "show", "superiority", "locally", "derivatives", "cheap", "algorithmic", "large", "programs", "fast", "number", "semi-algebraic", "through", "forward", "problem.", "suggesting", "matrix", "gradients", "using", "estimate", "np-hard", "function", "simple", "have,", "networks", "wide", "extends", "computing", "complexity", "multiplication,", "subdifferential", "functions.", "feedforward", "gradient", "activation", "turns", "elementary", "concurrent", "relate", "approaches,", "gradients.", "when", "class", "modes", "contrasts", "nonsmooth", "neural", "principle.", "with", "conservative", "gradient,", "standard", "backpropagation's", "independent", "mode", "illustrate", "propagation", "dimension", "that", "automatic", "finding", "overhead", "considerably", "establishing", "lipschitz", "backward", "estimates.", "provide", "computational", "programs.", "results", "baur-strassen's", "costs", "this", "which", "worst-case", "directional", "notion", "loss", "subgradients", "smooth", "dimensional-dependent", "further"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "256900870": {"id": "256900870", "openalex": null, "doi": null, "title": "Effective Data Augmentation With Diffusion Models", "abstract": "Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe an improvement in accuracy in tested domains.", "authors": [], "concepts": ["animal", "transformations", "learning,", "most", "axes", "attributes,", "method", "recent", "such", "these", "existing", "classification", "domains.", "images", "enhance", "from", "present", "deep", "however,", "using", "along", "many", "generative", "accuracy", "simple", "parameterized", "models", "task,", "learning.", "semantics", "current", "data.", "prevalent", "advances,", "high-level", "like", "ones.", "edits", "diffusion", "augmentation", "augmentations", "novel", "model,", "species", "real-world", "image", "observe", "data", "combines", "with", "including", "few-shot", "examples.", "standard", "flips", "text-to-image", "improvement", "approach", "rotations", "generalizes", "their", "semantic", "tools", "weed", "labelled", "those", "scene,", "effective", "alter", "address", "lack", "change", "concepts", "image-to-image", "recognition", "models.", "tested", "underpinning", "tasks,", "diversity", "cannot", "visual", "pre-trained", "evaluate", "classification,", "representation", "generate", "models,", "off-the-shelf"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "257038864": {"id": "257038864", "openalex": null, "doi": null, "title": "BACKSTEPPING TEMPORAL DIFFERENCE LEARNING", "abstract": "Off-policy learning ability is an important feature of reinforcement learning (RL) for practical applications. However, even one of the most elementary RL algorithms, temporal-difference (TD) learning, is known to suffer form divergence issue when the off-policy scheme is used together with linear function approximation. To overcome the divergent behavior, several off-policy TD-learning algorithms, including gradient-TD learning (GTD), and TD-learning with correction (TDC), have been developed until now. In this work, we provide a unified view of such algorithms from a purely control-theoretic perspective, and propose a new convergent algorithm. Our method relies on the backstepping technique, which is widely used in nonlinear control theory. Finally, convergence of the proposed algorithm is experimentally verified in environments where the standard TD-learning is known to be unstable.", "authors": [], "concepts": ["environments", "known", "off-policy", "widely", "(td)", "unified", "learning,", "developed", "most", "used", "behavior,", "method", "experimentally", "convergent", "relies", "view", "such", "difference", "feature", "been", "purely", "approximation.", "from", "however,", "verified", "function", "algorithms,", "propose", "issue", "convergence", "reinforcement", "until", "correction", "algorithm.", "(rl)", "scheme", "finally,", "linear", "learning", "divergent", "temporal-difference", "unstable.", "ability", "elementary", "overcome", "when", "perspective,", "form", "technique,", "control", "backstepping", "(tdc),", "divergence", "now.", "with", "including", "standard", "several", "work,", "gradient-td", "(gtd),", "suffer", "td-learning", "nonlinear", "provide", "practical", "algorithms", "temporal", "together", "where", "applications.", "this", "which", "algorithm", "have", "proposed", "important", "even", "control-theoretic", "theory."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "257232422": {"id": "257232422", "openalex": null, "doi": null, "title": "PARAMETRIZING PRODUCT SHAPE MANIFOLDS BY COMPOSITE NETWORKS", "abstract": "Parametrizations of data manifolds in shape spaces can be computed using the rich toolbox of Riemannian geometry. This, however, often comes with high computational costs, which raises the question if one can learn an efficient neural network approximation. We show that this is indeed possible for shape spaces with a special product structure, namely those smoothly approximable by a direct sum of low-dimensional manifolds. Our proposed architecture leverages this structure by separately learning approximations for the low-dimensional factors and a subsequent combination. After developing the approach as a general framework, we apply it to a shape space of triangular surfaces. Here, typical examples of data manifolds are given through datasets of articulated models and can be factorized, for example, by a Sparse Principal Geodesic Analysis (SPGA). We demonstrate the effectiveness of our proposed approach with experiments on synthetic data as well as manifolds extracted from data via SPGA. \u03b6 1 (v 1 ), . . . , \u03c8 \u03b6 J (v J )), where \u03a8 \u03b6 is a NN and the \u03c8 \u03b6 j are further NNs approximating the Riemannian exponential exp z on the low-dimensional factor manifolds.We develop our approach focusing on the shape space of discrete shells, where shapes are given by triangle meshes and the manifold is equipped with an elasticity-based metric. In principle, our approach is also applicable to other shape spaces such as manifolds of images, and we will include remarks on how we propose this could work. We evaluate our approach with experiments on data manifolds of triangle meshes, both synthetic ones and ones extracted from data via SPGA, and we 1", "authors": [], "concepts": ["toolbox", "learn", "demonstrate", "here,", "question", "indeed", "shells,", "extracted", "smoothly", "show", "comes", "typical", "triangular", "efficient", "exponential", "such", "will", "examples", "general", "datasets", "through", "approximation.", "from", "however,", "factorized,", "using", "composite", "example,", "well", "structure", "propose", "approximating", "factors", "models", "riemannian", "networks", "manifolds.we", "both", "manifolds.", "shapes", "this,", "include", "learning", "leverages", "work.", "could", "sparse", "low-dimensional", "elasticity-based", "other", "experiments", "space", "high", "manifold", "spga,", "network", "articulated", "analysis", "parametrizing", "separately", "namely", "data", "principle,", "remarks", "neural", "focusing", "with", "direct", "given", "discrete", "shape", "equipped", "spaces", "developing", "surfaces.", "approach", "that", "geodesic", "geometry.", "costs,", "triangle", "manifolds", "those", "rich", "subsequent", "effectiveness", "parametrizations", "(spga).", "images,", "apply", "combination.", "approximations", "special", "principal", "computational", "possible", "metric.", "synthetic", "also", "structure,", "where", "meshes", "this", "which", "product", "approximable", "framework,", "after", "factor", "spga.", "ones", "raises", "develop", "often", "proposed", "evaluate", "meshes,", "architecture", "applicable", "computed", "further"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "257254919": {"id": "257254919", "openalex": null, "doi": null, "title": "LS-IQ: IMPLICIT REWARD REGULARIZATION FOR INVERSE REINFORCEMENT LEARNING", "abstract": "Recent methods for imitation learning directly learn a Q-function using an implicit reward formulation rather than an explicit reward function. However, these methods generally require implicit reward regularization to improve stability and often mistreat absorbing states. Previous works show that a squared norm regularization on the implicit reward function is effective, but do not provide a theoretical analysis of the resulting properties of the algorithms. In this work, we show that using this regularizer under a mixture distribution of the policy and the expert provides a particularly illuminating perspective: the original objective can be understood as squared Bellman error minimization, and the corresponding optimization problem minimizes a bounded \u03c7 2 -Divergence between the expert and the mixture distribution. This perspective allows us to address instabilities and properly treat absorbing states. We show that our method, Least Squares Inverse Q-Learning (LS-IQ), outperforms state-of-the-art algorithms, particularly in environments with absorbing states. Finally, we propose to use an inverse dynamics model to learn from observations only. Using this approach, we retain performance in settings where no expert actions are available. 1", "authors": [], "concepts": ["environments", "state-of-the-art", "actions", "learn", "error", "problem", "theoretical", "approach,", "model", "understood", "show", "norm", "recent", "available.", "resulting", "settings", "algorithms.", "effective,", "these", "generally", "performance", "from", "however,", "using", "-divergence", "explicit", "function", "algorithms,", "minimization,", "propose", "reinforcement", "method,", "mistreat", "rather", "methods", "absorbing", "finally,", "learning", "improve", "ls-iq:", "perspective", "stability", "analysis", "regularizer", "regularization", "corresponding", "bellman", "policy", "works", "with", "(ls-iq),", "instabilities", "implicit", "bounded", "reward", "least", "retain", "work,", "outperforms", "that", "states.", "distribution", "only.", "between", "function.", "expert", "squares", "illuminating", "previous", "q-function", "squared", "address", "allows", "provide", "objective", "inverse", "formulation", "perspective:", "observations", "imitation", "properly", "where", "particularly", "dynamics", "than", "this", "provides", "mixture", "directly", "often", "optimization", "treat", "original", "under", "require", "properties", "minimizes", "q-learning", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "257632050": {"id": "257632050", "openalex": null, "doi": null, "title": "Memorization Capacity of Neural Networks with Conditional Computation", "abstract": "Many empirical studies have demonstrated the performance benefits of conditional computation in neural networks, including reduced inference time and power consumption. We study the fundamental limits of neural conditional computation from the perspective of memorization capacity. For Rectified Linear Unit (ReLU) networks without conditional computation, it is known that memorizing a collection of n input-output relationships can be accomplished via a neural network with O( \u221a n) neurons. Calculating the output of this neural network can be accomplished using O( \u221a n) elementary arithmetic operations of additions, multiplications and comparisons for each input. Using a conditional ReLU network, we show that the same task can be accomplished using only O(log n) operations per input. This represents an almost exponential improvement as compared to networks without conditional computation. We also show that the \u0398(log n) rate is the best possible. Our achievability result utilizes a general methodology to synthesize a conditional network out of an unconditional network in a computationally-efficient manner, bridging the gap between unconditional and conditional architectures.", "authors": [], "concepts": ["inference", "known", "each", "arithmetic", "studies", "computation,", "multiplications", "rate", "show", "comparisons", "exponential", "unconditional", "only", "general", "empirical", "reduced", "o(log", "performance", "from", "using", "relu", "many", "compared", "demonstrated", "fundamental", "task", "neurons.", "networks", "conditional", "networks,", "rectified", "linear", "relationships", "bridging", "output", "without", "additions,", "elementary", "architectures.", "perspective", "network", "synthesize", "represents", "neural", "input.", "with", "including", "(relu)", "limits", "capacity", "improvement", "that", "between", "\u03b8(log", "consumption.", "network,", "input-output", "accomplished", "memorization", "result", "computation.", "also", "computationally-efficient", "best", "same", "achievability", "collection", "possible.", "almost", "power", "this", "operations", "unit", "methodology", "benefits", "time", "have", "calculating", "computation", "utilizes", "memorizing", "study", "capacity.", "manner,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "257636556": {"id": "257636556", "openalex": null, "doi": null, "title": "Influencer Backdoor Attack on Semantic Segmentation", "abstract": "When a small number of poisoned samples are injected into the training dataset of a deep neural network, the network can be induced to exhibit malicious behavior during inferences, which poses potential threats to real-world applications. While they have been intensively studied in classification, backdoor attacks on semantic segmentation have been largely overlooked. Unlike classification, semantic segmentation aims to classify every pixel within a given image. In this work, we explore backdoor attacks on segmentation models to misclassify all pixels of a victim class by injecting a specific trigger on non-victim pixels during inferences, which is dubbed Influencer Backdoor Attack (IBA). IBA is expected to maintain the classification accuracy of non-victim pixels and misleads classifications of all victim pixels in every single inference. Specifically, we consider two types of IBA scenarios, i.e., 1) Free-position IBA: the trigger can be positioned freely except for pixels of the victim class, and 2) Long-distance IBA: the trigger can only be positioned somewhere far from victim pixels, given the possible practical constraint. Based on the context aggregation ability of segmentation models, we propose techniques to improve IBA for the scenarios. Concretely, for free-position IBA, we propose a simple, yet effective Nearest Neighbor trigger injection strategy for poisoned sample creation. For long-distance IBA, we propose a novel Pixel Random Labeling strategy. Our extensive experiments reveal that current segmentation models do suffer from backdoor attacks, and verify that our proposed techniques can further increase attack performance.", "authors": [], "concepts": ["aims", "strategy", "overlooked.", "labeling", "(iba).", "nearest", "influencer", "injecting", "verify", "poisoned", "dataset", "they", "simple,", "techniques", "within", "pixel", "victim", "only", "been", "constraint.", "specific", "number", "explore", "classification", "performance.", "increase", "strategy.", "expected", "from", "into", "deep", "malicious", "segmentation", "consider", "while", "accuracy", "backdoor", "inference.", "pixels", "propose", "models", "current", "exhibit", "scenarios.", "concretely,", "random", "long-distance", "except", "improve", "i.e.,", "ability", "non-victim", "reveal", "classifications", "experiments", "network", "types", "specifically,", "novel", "when", "based", "class", "real-world", "maintain", "unlike", "trigger", "attacks,", "potential", "training", "context", "misleads", "neural", "classify", "behavior", "given", "dubbed", "work,", "induced", "aggregation", "that", "free-position", "pixels,", "semantic", "threats", "suffer", "network,", "effective", "misclassify", "positioned", "inferences,", "freely", "practical", "injected", "possible", "image.", "neighbor", "creation.", "sample", "applications.", "this", "which", "samples", "during", "injection", "attack", "largely", "iba,", "have", "intensively", "small", "somewhere", "proposed", "attacks", "studied", "classification,", "poses", "models,", "further", "every", "class,", "single", "extensive", "scenarios,", "iba:"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "257687205": {"id": "257687205", "openalex": null, "doi": null, "title": "DRSM: DE-RANDOMIZED SMOOTHING ON MALWARE CLASSIFIER PROVIDING CERTIFIED ROBUSTNESS", "abstract": "Machine Learning (ML) models have been utilized for malware detection for over two decades.Consequently, this ignited an ongoing arms race between malware authors and antivirus systems, compelling researchers to propose defenses for malware-detection models against evasion attacks.However, most if not all existing defenses against evasion attacks suffer from sizable performance degradation and/or can defend against only specific attacks, which makes them less practical in real-world settings.In this work, we develop a certified defense, DRSM (De-Randomized Smoothed MalConv), by redesigning the de-randomized smoothing technique for the domain of malware detection.Specifically, we propose a window ablation scheme to provably limit the impact of adversarial bytes while maximally preserving local structures of the executables.After showing how DRSM is theoretically robust against attacks with contiguous adversarial bytes, we verify its performance and certified robustness experimentally, where we observe only marginal accuracy drops as the cost of robustness.To our knowledge, we are the first to offer certified robustness in the realm of static detection of malware executables.More surprisingly, through evaluating DRSM against 9 empirical attacks of different types, we observe that the proposed defense is empirically robust to some extent against a diverse set of attacks, some of which even fall out of the scope of its original threat model.In addition, we collected 15.5K recent benign raw executables from diverse sources, which will be made public as a dataset called PACE (Publicly Accessible Collection(s) of Executables) to alleviate the scarcity of publicly available benign datasets for studying malware detection and provide future research with more representative data of the time.", "authors": [], "concepts": ["researchers", "experimentally,", "future", "static", "machine", "window", "executables.more", "redesigning", "contiguous", "most", "robust", "accessible", "them", "realm", "verify", "ignited", "available", "recent", "limit", "dataset", "and/or", "will", "existing", "only", "evasion", "fall", "been", "specific", "empirical", "datasets", "marginal", "through", "more", "performance", "malware-detection", "from", "malware", "executables)", "representative", "attacks.however,", "while", "surprisingly,", "classifier", "defense", "accuracy", "diverse", "types,", "drsm:", "extent", "propose", "models", "sizable", "ablation", "some", "(de-randomized", "domain", "scarcity", "robustness.to", "de-randomized", "made", "scheme", "learning", "benign", "model.in", "local", "arms", "15.5k", "cost", "degradation", "providing", "real-world", "drops", "observe", "makes", "addition,", "attacks,", "data", "provably", "preserving", "theoretically", "offer", "with", "systems,", "smoothed", "malconv),", "less", "evaluating", "drsm", "structures", "detection", "studying", "work,", "threat", "that", "between", "decades.consequently,", "scope", "bytes", "alleviate", "collected", "different", "robustness", "suffer", "impact", "antivirus", "technique", "defenses", "detection.specifically,", "called", "compelling", "empirically", "(publicly", "first", "maximally", "provide", "research", "showing", "time.", "practical", "sources,", "settings.in", "executables.after", "utilized", "certified", "where", "publicly", "ongoing", "over", "bytes,", "this", "which", "collection(s)", "adversarial", "develop", "authors", "executables", "have", "proposed", "attacks", "original", "defend", "(ml)", "even", "defense,", "smoothing", "race", "knowledge,", "against", "pace", "public"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "257757426": {"id": "257757426", "openalex": null, "doi": null, "title": "OPTIMAL TRANSPORT FOR OFFLINE IMITATION LEARNING", "abstract": "With the advent of large datasets, offline reinforcement learning (RL) is a promising framework for learning good decision-making policies without the need to interact with the real environment. However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive. In this paper, we introduce Optimal Transport Reward labeling (OTR), an algorithm that assigns rewards to offline trajectories, with a few high-quality demonstrations. OTR's key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we show that OTR with a single demonstration can consistently match the performance of offline RL with ground-truth rewards 1 .", "authors": [], "concepts": ["learn", "high-quality", "paper,", "demonstration", "labeling", "reward-annotated,", "engineering", "used", "show", "idea", "dataset", "trajectories,", "large", "interact", "good", "performance", "however,", "benchmarks,", "measure", "requires", "framework", "optimal", "demonstrations.", "reinforcement", "environment.", "(rl)", "learning", "interpreted", "without", "computationally", "offline", "promising", "challenges", "when", "easy", "(otr),", "with", "obtaining", "reward", "labor-intensive.", "reward,", "implement", "that", "between", "expert", "real", "trajectory", "assigns", "unlabeled", "presents", "need", "efficient.", "datasets,", "ground-truth", "advent", "decision-making", "otr's", "policies", "match", "practical", "difficult", "rewards", "imitation", "introduce", "annotations", "then", "consistently", "this", "which", "transport", "algorithm", "alignment", "policy.", "d4rl", "compute", "single", "similarity", "obtain"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "257766959": {"id": "257766959", "openalex": null, "doi": null, "title": "Seer: Language Instructed Video Prediction with Latent Diffusion Models", "abstract": "Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning, i.e., predicting future video frames with a given language instruction and reference frames. It is a highly challenging task to ground task-level goals specified by instructions and high-fidelity frames together, requiring large-scale data and computation. To tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named Seer, by inflating the pretrained text-to-image (T2I) stable diffusion models along the temporal axis. We inflate the denoising U-Net and language conditioning model with two novel techniques, Autoregressive Spatial-Temporal Attention and Frame Sequential Text Decomposer, to propagate the rich prior knowledge in the pretrained T2I models across the frames. With the well-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 (SSv2) and Bridgedata datasets demonstrate our superior video prediction performance with around 210-hour training on 4 RTX 3090 GPUs: decreasing the FVD of the current SOTA model from 290 to 200 on SSv2 and achieving at least 70% preference in the human evaluation. https://seervideodiffusion.github.io/", "authors": [], "concepts": ["essential", "future", "challenging", "sota", "demonstrate", "achieving", "instruction-aligned", "learning,", "gpus:", "denoising", "tackle", "model", "reach", "bridgedata", "robots", "prior", "requiring", "frames.", "layers", "general", "highly", "datasets", "high-fidelity", "performance", "from", "well-designed", "facilitate", "seer,", "along", "sound", "goals.", "task", "https://seervideodiffusion.github.io/", "propose", "large-scale", "models", "successfully", "current", "data.", "imagining", "frame", "reference", "sequential", "superior", "video", "experimental", "specified", "i.e.,", "ability", "goals", "(ssv2)", "something", "evaluation.", "latent", "stable", "frames", "diffusion", "pretrained", "text", "novel", "instruction", "model,", "inflating", "210-hour", "predicting", "policy", "makes", "spatial-temporal", "data", "computation-efficient", "training", "across", "preference", "with", "seer", "given", "least", "decomposer,", "ground", "text-to-image", "conditioning", "their", "trajectory", "knowledge", "seer:", "foresee", "language", "future,", "fine-tuning", "prediction", "(tvp)", "autoregressive", "task-level", "empower", "rich", "make", "high-fidelity,", "attention", "around", "techniques,", "instructions", "amount", "axis.", "robot", "propagate", "temporal", "together,", "possible", "u-net", "computation.", "results", "text-conditioned", "sample", "this", "instructed", "architecture,", "inflate", "small", "named", "coherent,", "ssv2", "(t2i)", "planning", "generate", "human", "decreasing", "therefore,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "257913782": {"id": "257913782", "openalex": null, "doi": null, "title": "DEPTH SEPARATION WITH MULTILAYER MEAN-FIELD NETWORKS", "abstract": "Depth separation-why a deeper network is more powerful than a shallower onehas been a major problem in deep learning theory. Previous results often focus on representation power. For example, Safran et al. (2019) constructed a function that is easy to approximate using a 3-layer network but not approximable by any 2-layer network. In this paper, we show that this separation is in fact algorithmic: one can learn the function constructed by Safran et al.(2019)using an overparameterized network with polynomially many neurons efficiently. Our result relies on a new way of extending the mean-field limit to multilayer networks, and a decomposition of loss that factors out the error introduced by the discretization of infinite-width mean-field networks.", "authors": [], "concepts": ["learn", "paper,", "multilayer", "error", "problem", "approximate", "separation", "show", "limit", "relies", "networks.", "(2019)", "2-layer", "been", "extending", "more", "deep", "using", "example,", "many", "function", "algorithmic:", "factors", "networks", "separation-why", "neurons", "networks,", "power.", "learning", "fact", "safran", "polynomially", "network", "constructed", "3-layer", "powerful", "easy", "decomposition", "major", "with", "depth", "network.", "al.(2019)using", "introduced", "that", "onehas", "previous", "mean-field", "result", "deeper", "focus", "results", "discretization", "than", "overparameterized", "this", "approximable", "efficiently.", "loss", "often", "representation", "infinite-width", "shallower", "theory."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258041281": {"id": "258041281", "openalex": null, "doi": null, "title": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens", "abstract": "Human visual recognition is a sparse process, where only a few salient visual cues are attended to rather than traversing every detail uniformly.However, most current vision networks follow a dense paradigm, processing every single visual unit (e.g., pixel or patch) in a uniform manner.In this paper, we challenge this dense paradigm and present a new method, coined SparseFormer, to imitate human's sparse visual recognition in an end-to-end manner.Sparse-Former learns to represent images using a highly limited number of tokens (down to 49) in the latent space with sparse feature sampling procedure instead of processing dense units in the original pixel space.Therefore, Sparse-Former circumvents most of dense operations on the image space and has much lower computational costs.Experiments on the ImageNet classification benchmark dataset show that SparseFormer achieves performance on par with canonical or well-established models while offering better accuracy-throughput tradeoff.Moreover, the design of our network can be easily extended to the video classification with promising performance at lower computational costs.We hope that our work can provide an alternative way for visual modeling and inspire further research on sparse neural architectures.The code will be publicly available at https://github.com/showlab/sparseformer.", "authors": [], "concepts": ["paper,", "most", "uniform", "manner.sparse-former", "show", "available", "dataset", "tokens", "pixel", "modeling", "will", "only", "procedure", "feature", "number", "highly", "much", "represent", "classification", "sampling", "performance", "images", "present", "traversing", "manner.in", "patch)", "follow", "using", "cues", "while", "models", "achieves", "method,", "networks", "rather", "current", "lower", "architectures.the", "work", "better", "video", "https://github.com/showlab/sparseformer.", "sparse", "end-to-end", "units", "latent", "space", "accuracy-throughput", "network", "promising", "inspire", "space.therefore,", "detail", "code", "paradigm,", "image", "well-established", "coined", "extended", "vision", "neural", "learns", "with", "design", "dense", "tradeoff.moreover,", "paradigm", "benchmark", "that", "instead", "sparseformer,", "costs.we", "imagenet", "alternative", "easily", "processing", "human's", "hope", "uniformly.however,", "provide", "research", "challenge", "computational", "(e.g.,", "where", "recognition", "publicly", "than", "imitate", "this", "operations", "unit", "sparseformer:", "sparseformer", "process,", "salient", "circumvents", "visual", "costs.experiments", "sparse-former", "original", "limited", "canonical", "(down", "attended", "offering", "human", "further", "every", "single"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258108073": {"id": "258108073", "openalex": null, "doi": null, "title": "LOSSLESS ADAPTATION OF PRETRAINED VISION MODELS FOR ROBOTIC MANIPULATION", "abstract": "Recent works have shown that large models pretrained on common visual learning tasks can provide useful representations for a wide range of specialized perception problems, as well as a variety of robotic manipulation tasks. While prior work on robotic manipulation has predominantly used frozen pretrained features, we demonstrate that in robotics this approach can fail to reach optimal performance, and that fine-tuning of the full model can lead to significantly better results. Unfortunately, fine-tuning disrupts the pretrained visual representation, and causes representational drift towards the fine-tuned task thus leading to a loss of the versatility of the original model. We introduce lossless adaptation to address this shortcoming of classical fine-tuning. We demonstrate that appropriate placement of our parameter efficient adapters can significantly reduce the performance gap between frozen pretrained representations and full end-to-end finetuning without changes to the original representation and thus preserving original capabilities of the pretrained model. We perform a comprehensive investigation across three major model architectures (ViTs, NFNets, and ResNets), supervised (ImageNet-1K classification) and self-supervised pretrained weights (CLIP, BYOL, Visual MAE) in 3 task domains and 35 individual tasks, and demonstrate that our claims are strongly validated in various settings. Please see real world videos at https://sites.google.com/view/robo-adapters.", "authors": [], "concepts": ["demonstrate", "robotics", "tasks", "model", "used", "reach", "lead", "recent", "strongly", "classical", "videos", "predominantly", "efficient", "byol,", "features,", "(vits,", "large", "lossless", "prior", "shown", "adapters", "disrupts", "performance", "fine-tuning.", "well", "while", "classification)", "towards", "task", "comprehensive", "optimal", "models", "unfortunately,", "wide", "thus", "fine-tuned", "full", "work", "causes", "better", "versatility", "learning", "perception", "significantly", "without", "end-to-end", "performance,", "investigation", "pretrained", "changes", "robotic", "validated", "self-supervised", "(clip,", "representational", "individual", "vision", "preserving", "major", "tasks.", "world", "works", "across", "appropriate", "finetuning", "claims", "common", "adaptation", "results.", "please", "approach", "(imagenet-1k", "domains", "settings.", "that", "between", "drift", "real", "placement", "weights", "representation,", "fine-tuning", "useful", "model.", "representations", "leading", "reduce", "range", "address", "architectures", "provide", "parameter", "three", "fail", "nfnets,", "perform", "problems,", "introduce", "specialized", "various", "shortcoming", "tasks,", "this", "loss", "have", "https://sites.google.com/view/robo-adapters.", "visual", "mae)", "frozen", "manipulation", "original", "representation", "supervised", "variety", "resnets),", "capabilities"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258436870": {"id": "258436870", "openalex": null, "doi": null, "title": "PRIVACY-PRESERVING IN-CONTEXT LEARNING FOR LARGE LANGUAGE MODELS", "abstract": "In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in in-context exemplars. To address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. We evaluate DP-ICL on four text classification benchmarks and two language generation tasks, and our empirical results show that DP-ICL achieves a strong utility-privacy tradeoff.", "authors": [], "concepts": ["differentially", "contained", "dp-icl,", "show", "exemplars,", "challenge,", "idea", "techniques", "information", "these", "large", "general", "empirical", "through", "classification", "generation.", "privatizing", "dynamically", "however,", "generation", "accuracy", "consensus", "propose", "models", "achieves", "benchmarks", "privatize", "among", "generating", "private", "learning", "llm's", "tradeoff.", "exemplar", "utility-privacy", "text", "based", "privacy-preserving", "tasks.", "specific,", "noisy", "sensitive", "(llms),", "several", "leak", "paradigm", "four", "that", "ensemble", "sets.", "exemplars.", "strong", "language", "in-context", "(dp-icl),", "dp-icl", "instantiate", "(icl)", "enabling", "address", "showing", "responses", "results", "tasks,", "this", "adapt", "relevance.", "improving", "evaluate", "capability", "important", "thereby", "disjoint"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258480011": {"id": "258480011", "openalex": null, "doi": null, "title": "ZipIt! Merging Models from Different Tasks without Training", "abstract": "Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce \"ZipIt!\", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to additionally allow for merging features within each model by defining a general \"zip\" operation. Second, we add support for partially zipping the models up until a specified layer, naturally creating a multi-head model. We find that these two changes combined account for a staggering 20-60% improvement over prior work, making the merging of models trained on disjoint tasks feasible.", "authors": [], "concepts": ["partially", "each", "paper,", "tasks", "problem", "tackle", "model", "them", "method", "typical", "adds", "they", "within", "zipping", "these", "prior", "were", "general", "from", "into", "operation.", "deep", "order", "extremely", "strategies.", "distinct", "account", "while", "task", "simple", "staggering", "models", "task,", "until", "work", "features", "support", "capable", "naturally", "thus,", "specified", "together.", "additional", "solving", "without", "first,", "completely", "other", "space", "changes", "zipit!", "combining", "shared", "second,", "training", "tasks.", "making", "works", "aren't", "with", "fails", "performing", "trained", "work,", "improvement", "permutes", "defining", "multi-head", "that", "training.", "creating", "initializations,", "between", "arbitrary", "different", "separate", "model.", "multi-task", "allow", "additionally", "feasible.", "combined", "difficult", "find", "20-60%", "introduce", "same", "then", "recognition", "expand", "over", "\"zip\"", "this", "\"zipit!\",", "merging", "layer,", "incorporates", "visual", "architecture", "disjoint", "models,", "differences"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258741298": {"id": "258741298", "openalex": null, "doi": null, "title": "KNOWLEDGE CARD: FILLING LLMS' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS", "abstract": "By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently.As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge.To this end, we propose KNOWLEDGE CARD, a modular framework to plug in new factual and relevant knowledge into general-purpose LLMs.We first introduce knowledge cards-specialized language models trained on corpora from specific domains and sources.Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM.We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs.Finally, we propose two complementary integration approaches to augment the base LLM with the (relevant, factual) knowledge curated from the specialized LMs.Through extensive experiments, we demonstrate that KNOWLEDGE CARD achieves state-of-the-art performance on six benchmark datasets.Ultimately, KNOWLEDGE CARD framework enables dynamic synthesis and updates of knowledge from diverse domains.Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.", "authors": [], "concepts": ["state-of-the-art", "inference", "brevity,", "demonstrate", "frequently.as", "static", "gaps", "selectors", "modularity", "expensive", "choices", "lead", "they", "card:", "filling", "information", "ensure", "retrain", "community.", "will", "parametric", "these", "large", "knowledge.to", "updates", "specific", "update", "through", "controlling", "performance", "from", "plug", "into", "dynamically", "knowledge-intensive", "synthesis", "cards", "lms.through", "diverse", "specifically", "llms'", "framework", "propose", "increasingly", "models", "achieves", "design,", "augment", "factual,", "relevance,", "up-to-date", "selected", "factual)", "cards-specialized", "corpora", "documents", "updated", "becomes", "card,", "integration", "factual", "relevant,", "evident", "cards,", "content", "generated", "background", "with", "design", "retain", "trained", "collective", "adopted", "dynamic", "benchmark", "repositories", "domains", "that", "complementary", "base", "knowledge", "card", "language", "end,", "efforts", "approaches", "first", "llm.we", "failures", "research", "experiments,", "three", "general-purpose", "sources.knowledge", "continuously", "plug-in", "introduce", "specialized", "select", "serve", "then", "tasks,", "this", "domains.its", "enables", "datasets.ultimately,", "llms.we", "time", "curated", "relevant", "outputs.finally,", "generate", "(relevant,", "models,", "modular", "extensive", "(llms)", "factuality"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258832670": {"id": "258832670", "openalex": null, "doi": null, "title": "ControlVideo: Training-free Controllable Text-to-Video Generation", "abstract": "Text-driven diffusion models have unlocked unprecedented abilities in image generation, whereas their video counterpart still lags behind due to the excessive training cost of temporal modeling. Besides the training burden, the generated videos also suffer from appearance inconsistency and structural flickers, especially in long video synthesis. To address these challenges, we design a training-free framework called ControlVideo to enable natural and efficient text-to-video generation. ControlVideo, adapted from ControlNet, leverages coarsely structural consistency from input motion sequences, and introduces three modules to improve video generation. Firstly, to ensure appearance coherence between frames, ControlVideo adds fully cross-frame interaction in self-attention modules. Secondly, to mitigate the flicker effect, it introduces an interleaved-frame smoother that employs frame interpolation on alternated frames. Finally, to produce long videos efficiently, it utilizes a hierarchical sampler that separately synthesizes each short clip with holistic coherency. Empowered with these modules, ControlVideo outperforms the state-of-the-arts on extensive motion-prompt pairs quantitatively and qualitatively. Notably, thanks to the efficient designs, it generates both short and long videos within several minutes using one NVIDIA 2080Ti. Code is available at https://github.com/YBYBZhang/ControlVideo.Recent studies[15,40]have explored leveraging the structure controllability of ControlNet[43]or DDIM inversion [35] for video generation. Rather than synthesizing all frames independently,[15,40]enhance appearance coherence by replacing original self-attention with the sparser crossframe attention. Nevertheless, their video quality is still far behind photo-realistic videos in terms of: (i) inconsistent appearance between some frames (seeFig. 4 (a)), (ii) visible artifacts in large motion videos (seeFig. 4(b)), and (iii) structural flickers during inter-frame transitions. For (i) andPreprint. Under review.", "authors": [], "concepts": ["interpolation", "qualitatively.", "fully", "each", "flickers,", "generation,", "counterpart", "nevertheless,", "modules,", "effect,", "challenges,", "text-driven", "excessive", "alternated", "available", "adds", "within", "videos", "ensure", "efficient", "besides", "inconsistent", "these", "flicker", "large", "unprecedented", "efficiently,", "frames.", "(iii)", "training-free", "independently,[15,40]enhance", "(ii)", "consistency", "generation.", "from", "frames,", "unlocked", "using", "generation", "structure", "crossframe", "mitigate", "framework", "sampler", "models", "holistic", "self-attention", "controlvideo", "behind", "rather", "some", "controlvideo:", "controllability", "visible", "enable", "controllable", "both", "burden,", "frame", "terms", "finally,", "quantitatively", "hierarchical", "video", "synthesizes", "coherence", "improve", "inter-frame", "controlvideo,", "leverages", "structural", "sparser", "short", "minutes", "(a)),", "produce", "especially", "long", "interaction", "thanks", "clip", "frames", "attention.", "diffusion", "explored", "cost", "synthesizing", "modules.", "code", "image", "motion-prompt", "separately", "generated", "nvidia", "input", "training", "introduces", "4(b)),", "inversion", "modules", "state-of-the-arts", "smoother", "notably,", "with", "whereas", "design", "several", "adapted", "abilities", "outperforms", "that", "cross-frame", "between", "their", "motion", "photo-realistic", "coherency.", "controlnet,", "suffer", "synthesis.", "replacing", "(seefig.", "natural", "text-to-video", "called", "leveraging", "secondly,", "address", "ddim", "appearance", "three", "modeling.", "temporal", "coarsely", "review.", "2080ti.", "also", "artifacts", "interleaved-frame", "https://github.com/ybybzhang/controlvideo.recent", "than", "quality", "studies[15,40]have", "designs,", "still", "inconsistency", "during", "employs", "sequences,", "flickers", "pairs", "controlnet[43]or", "firstly,", "lags", "transitions.", "have", "utilizes", "original", "under", "[35]", "andpreprint.", "extensive", "generates", "empowered"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258833272": {"id": "258833272", "openalex": null, "doi": null, "title": "Time Fairness in Online Knapsack Problems", "abstract": "The online knapsack problem is a classic problem in the field of online algorithms. Its canonical version asks how to pack items of different values and weights arriving online into a capacity-limited knapsack so as to maximize the total value of the admitted items. Although optimal competitive algorithms are known for this problem, they may be fundamentally unfair, i.e., individual items may be treated inequitably in different ways. Inspired by recent attention to fairness in online settings, we develop a natural and practically-relevant notion of time fairness for the online knapsack problem, and show that the existing optimal algorithms perform poorly under this metric. We propose a parameterized deterministic algorithm where the parameter precisely captures the Pareto-optimal trade-off between fairness and competitiveness. We show that randomization is theoretically powerful enough to be simultaneously competitive and fair; however, it does not work well in practice, using trace-driven experiments. To further improve the trade-off between fairness and competitiveness, we develop a fair, robust (competitive), and consistent learning-augmented algorithm with substantial performance improvement in trace-driven experiments.Preprint. Under review.", "authors": [], "concepts": ["known", "admitted", "poorly", "field", "trace-driven", "unfair,", "captures", "total", "robust", "problem", "version", "show", "capacity-limited", "competitiveness,", "recent", "they", "algorithms.", "inspired", "consistent", "precisely", "existing", "does", "knapsack", "experiments.preprint.", "performance", "into", "however,", "(competitive),", "using", "well", "fair;", "items.", "practice,", "propose", "optimal", "parameterized", "asks", "practically-relevant", "work", "problem,", "improve", "ways.", "problems", "online", "simultaneously", "i.e.,", "learning-augmented", "fair,", "settings,", "fairness", "competitive", "pareto-optimal", "powerful", "inequitably", "randomization", "individual", "experiments.", "classic", "theoretically", "although", "with", "deterministic", "maximize", "improvement", "that", "between", "fundamentally", "weights", "treated", "different", "competitiveness.", "natural", "attention", "trade-off", "arriving", "value", "parameter", "algorithms", "metric.", "perform", "review.", "pack", "where", "this", "enough", "notion", "algorithm", "items", "time", "develop", "substantial", "canonical", "under", "further", "values"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258833682": {"id": "258833682", "openalex": null, "doi": null, "title": "Annealing Self-Distillation Rectification Improves Adversarial Training", "abstract": "In standard adversarial training, models are optimized to fit one-hot labels within allowable adversarial perturbation budgets. However, the ignorance of underlying distribution shifts brought by perturbations causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that accurately reflects the distribution shift under attack during adversarial training. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by replacing the hard labels in their objectives. We demonstrate the efficacy of ADR through extensive experiments and strong performances across datasets.Preprint. Under review.", "authors": [], "concepts": ["overfitting.", "demonstrate", "perturbations", "budgets.", "perturbation", "robust", "problem", "model", "method", "techniques", "within", "soft", "seamless", "observation,", "objectives.", "through", "enhance", "however,", "outputs.", "simple", "propose", "issue", "brought", "models", "tend", "method,", "datasets.preprint.", "extra", "well-calibrated", "rectified", "causes", "annealing", "better", "improve", "significantly", "without", "characteristics", "produce", "other", "experiments", "integration", "ignorance", "based", "allowable", "improves", "self-distillation", "training", "smoother", "shifts", "across", "with", "(adr),", "identify", "robustness,", "standard", "training,", "shift", "that", "distribution", "training.", "their", "analyze", "strong", "facilitates", "optimized", "robustness", "guidance", "distributions", "replacing", "effective", "need", "one-hot", "rectification", "plug-and-play", "adr,", "address", "utilizing", "labels", "computation.", "moreover,", "performances", "review.", "accurately", "this", "which", "during", "efficacy", "adversarial", "attack", "reflects", "pre-trained", "underlying", "under", "mechanism", "hard", "obtain", "extensive", "generates"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258865444": {"id": "258865444", "openalex": null, "doi": null, "title": "ALT-TEXT WITH CONTEXT: IMPROVING ACCESSIBILITY FOR IMAGES ON TWITTER", "abstract": "In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter.More than just a special case of image captioning, alt-text is both more literally descriptive and context-specific.Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative.We address this task with a multimodal model that conditions on both textual information from the associated social media post as well as visual signal from the image, and demonstrate that the utility of these two information sources stacks.We put forward a new dataset of 371k images paired with alt-text and tweets scraped from Twitter and evaluate on it across a variety of automated metrics as well as human evaluation.We show that our approach of conditioning on both tweet text and visual information significantly outperforms prior work, by more than 2x on BLEU@4.", "authors": [], "concepts": ["case", "context-specific.also", "demonstrate", "associated", "paired", "scraped", "informative.we", "model", "show", "describing", "dataset", "information", "alt-text)", "these", "prior", "371k", "despite", "just", "twitter", "sources", "evaluation.we", "more", "forward", "images", "from", "present", "social", "captioning,", "accompanied", "well", "posted", "descriptive", "specifically", "task", "image,", "tweet", "both", "accessibility", "conditions", "utility", "tweets", "work", "generating", "necessarily", "signal", "significantly", "textual", "leveraged", "text", "user-written", "image", "shared", "context:", "context", "bleu@4.", "across", "with", "media", "conditioning", "work,", "approach", "outperforms", "that", "alternative", "post", "useful", "alt-text", "multimodal", "automated", "critically,", "address", "literally", "special", "provide", "metrics", "descriptions", "stacks.we", "properly", "than", "media,", "this", "improving", "often", "visual", "evaluate", "variety", "twitter.more", "human"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258865597": {"id": "258865597", "openalex": null, "doi": null, "title": "Sharpness-Aware Data Poisoning Attack", "abstract": "Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs) against data poisoning attacks. These attacks aim to inject poisoning samples into the models' training dataset such that the trained models have inference failures. While previous studies have executed different types of attacks, one major challenge that greatly limits their effectiveness is the uncertainty of the re-training process after the injection of poisoning samples, including the re-training initialization or algorithms. To address this challenge, we propose a novel attack method called \"Sharpness-Aware Data Poisoning Attack (SAPA)\". In particular, it leverages the concept of DNNs' loss landscape sharpness to optimize the poisoning effect on the worst re-trained model. It helps enhance the preservation of the poisoning effect, regardless of the specific retraining procedure employed. Extensive experiments demonstrate that SAPA offers a general and principled strategy that significantly enhances various types of poisoning attacks.Preprint. Under review. . mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.", "authors": [], "concepts": ["inference", "demonstrate", "strategy", "attacks.preprint.", "uncertainty", "studies", "particular,", "effect,", "arxiv:1710.09412,", "method", "challenge,", "recent", "minimization.", "dataset", "algorithms.", "principled", "such", "concept", "these", "procedure", "specific", "general", "empirical", "re-training", "enhance", "arxiv", "into", "deep", "sapa", "offers", "while", "propose", "models", "preservation", "networks", "process", "optimize", "retraining", "regardless", "\"sharpness-aware", "leverages", "significantly", "samples,", "dnns'", "enhances", "experiments", "mixup:", "(sapa)\".", "types", "novel", "poisoning", "attacks,", "data", "training", "major", "sharpness", "neural", "inject", "including", "limits", "trained", "beyond", "that", "preprint", "their", "landscape", "previous", "different", "2017.", "model.", "re-trained", "executed", "effectiveness", "failures.", "called", "greatly", "address", "research", "vulnerability", "challenge", "(dnns)", "review.", "sharpness-aware", "various", "employed.", "this", "samples", "worst", "initialization", "risk", "after", "injection", "loss", "attack", "have", "attacks", "under", "effect", "highlighted", "attacks.", "against", "helps", "models'", "extensive"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258887582": {"id": "258887582", "openalex": null, "doi": null, "title": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module", "abstract": "Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message passing networks layers and train it in a end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several homophilic and heterophilic graph datasets and it is shown to outperform other state-of-the-art techniques, offering significant improvements especially in cases where an input graph is not provided. * Equal contribution. Corresponding authors,", "authors": [], "concepts": ["improvements", "state-of-the-art", "inference", "learn", "relaxed", "higher-order", "outperform", "most", "rewire", "(noisy,", "wake", "model", "show", "downstream", "describing", "topologies.", "cases", "avoids", "topology", "authors,", "procedure", "layers", "shown", "datasets", "regular", "computes", "from", "interactions", "deep", "dynamically", "however,", "heterophilic", "function", "...)", "learnable", "incomplete,", "fashion,", "integrate", "homophilic", "two-step", "improvable,", "exhaustive", "scalability.", "networks", "thus", "significant", "differentiable", "methods", "graph", "(dcm),", "(lgi)", "learning", "topological", "improve", "complexes", "topology)", "probabilities", "sparse", "end-to-end", "especially", "thanks", "latent", "aim,", "cell", "other", "provided.", "success", "novel", "corresponding", "data", "input", "search", "neural", "across", "with", "train", "input,", "given", "several", "message", "maintaining", "task.", "that", "solely", "between", "points.", "assume", "reliance", "contribution.", "module", "cells", "techniques,", "possible", "introduce", "(tdl),", "where", "tested", "this", "have", "(lti)", "multi-way", "passing", "inference:", "study", "(gnns)", "offering", "equal", "(with", "complex"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258947377": {"id": "258947377", "openalex": null, "doi": null, "title": "Modulate Your Spectrum in Self-Supervised Learning", "abstract": "Whitening loss provides theoretical guarantee in avoiding feature collapse for self-supervised learning (SSL) using joint embedding architectures.One typical implementation of whitening loss is hard whitening that designs whitening transformation over embedding and imposes the loss on the whitened output.In this paper, we propose spectral transformation (ST) framework to map the spectrum of embedding to a desired distribution during forward pass, and to modulate the spectrum of embedding by implicit gradient update during backward pass.We show that whitening transformation is a special instance of ST by definition, and there exist other instances that can avoid collapse by our empirical investigation.Furthermore, we propose a new instance of ST, called IterNorm with trace loss (INTL).We theoretically prove that INTL can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution during the course of optimization.Moreover, INTL achieves 76.6% top-1 accuracy in linear evaluation on ImageNet using ResNet-50, which exceeds the performance of the supervised baseline, and this result is obtained by using a batch size of only 256.Comprehensive experiments show that INTL is a promising SSL method in practice.The code is available at https://github.com/winci-ai/intl.", "authors": [], "concepts": ["modulate", "paper,", "exist", "theoretical", "exceeds", "show", "method", "typical", "available", "practice.the", "joint", "only", "baseline,", "feature", "embedding", "update", "empirical", "forward", "performance", "intl", "using", "trace", "accuracy", "instances", "collapse", "towards", "investigation.furthermore,", "spectral", "framework", "propose", "there", "76.6%", "achieves", "equal-eigenvalue", "avoid", "linear", "output.in", "learning", "desired", "gradient", "whitened", "prove", "(ssl)", "other", "experiments", "(st)", "promising", "top-1", "your", "256.comprehensive", "code", "self-supervised", "pass,", "designs", "theoretically", "obtained", "https://github.com/winci-ai/intl.", "with", "implicit", "that", "distribution", "imagenet", "transformation", "implementation", "pass.we", "course", "result", "called", "backward", "size", "special", "batch", "(intl).we", "avoiding", "architectures.one", "optimization.moreover,", "instance", "evaluation", "over", "whitening", "this", "which", "provides", "iternorm", "during", "loss", "imposes", "definition,", "spectrum", "supervised", "hard", "guarantee", "resnet-50,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "258999763": {"id": "258999763", "openalex": null, "doi": null, "title": "The Hidden Language of Diffusion Models", "abstract": "3 Google DeepMind 4 Weizmann Institute https://hila-chefer.github.io/Conceptor/Concept: painter Concept: sweet peppers fingers pepper CONCEPTOR CONCEPTOR Concept: beetle hornet emerald CONCEPTOR Picasso Monet Impressionism brushes paint portrait + + (a) Concept decomposition with CONCEPTOR (b) Single-image decomposition with CONCEPTOR Figure 1.Concept interpretation with CONCEPTOR.(a) Given a set of representative concept images, CONCEPTOR learns to decompose the concept into a weighted combination of interpretable elements (font sizes indicate weights).The decomposition exposes interesting behaviors such as reliance on prominent painters and renowned artistic styles (e.g., \"Monet\", \"Impressionism\").(b) Given a specific generated image, CONCEPTOR extracts its primary contributing elements, revealing surprising visual connections (e.g., \"sweet peppers\" are linked to \"fingers\" due to their common shape).", "authors": [], "concepts": ["combination", "pepper", "decompose", "emerald", "behaviors", "portrait", "peppers", "painters", "brushes", "such", "concept", "specific", "revealing", "interpretable", "sweet", "hornet", "into", "representative", "https://hila-chefer.github.io/conceptor/concept:", "(font", "sizes", "models", "image,", "conceptor", "\"sweet", "1.concept", "picasso", "exposes", "beetle", "peppers\"", "diffusion", "renowned", "google", "conceptor.(a)", "weighted", "indicate", "interesting", "generated", "decomposition", "learns", "\"fingers\"", "single-image", "with", "interpretation", "given", "deepmind", "weights).the", "common", "painter", "elements,", "paint", "their", "styles", "impressionism", "language", "monet", "reliance", "figure", "images,", "contributing", "prominent", "connections", "institute", "linked", "\"monet\",", "(e.g.,", "hidden", "primary", "shape).", "elements", "fingers", "artistic", "weizmann", "extracts", "visual", "concept:", "surprising", "\"impressionism\").(b)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259075246": {"id": "259075246", "openalex": null, "doi": null, "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems", "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers.However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios.To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline).Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction.RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems.", "authors": [], "concepts": ["combination", "aims", "single-file", "auto-completion", "encouraging", "java", "mainly", "most", "tasks", "continuous", "systems.repobench", "real-world,", "systems.", "complete", "multi-file", "interconnected", "consists", "cross-file", "large", "more", "performance", "from", "facilitate", "productivity", "predict", "respectively", "specifically", "(pipeline).each", "task", "models", "benchmarks", "current", "both", "designed", "(retrieval),", "developers.however,", "repobench:", "line", "measures", "ability", "files", "other", "snippets", "retrieve", "in-file", "repobench-r", "system's", "code", "repobench-p", "supports", "potential", "with", "systems,", "next-line", "complex,", "evaluating", "systems", "comparison", "leaving", "benchmark", "improvement", "that", "python", "programming", "repobench,", "fill", "handle", "language", "tasks:", "repository-level", "prediction.repobench", "(code", "next", "greatly", "focus", "benchmarking", "three", "retrieval", "context,", "scenarios.to", "introduce", "enhancements", "evaluation", "tasks,", "this", "gap,", "assessment", "have", "repobench-c", "advanced", "substantial", "relevant", "completion),", "require", "complex", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259075723": {"id": "259075723", "openalex": null, "doi": null, "title": "Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning", "abstract": "Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding client-side detectability of MS attacks were raised, questioning their practicality once they are publicly known. In this work, for the first time, we thoroughly study the problem of client-side detectability. We demonstrate that most prior MS attacks, which fundamentally rely on one of two key principles, are detectable by principled client-side checks. Further, we formulate desiderata for practical MS attacks and propose SEER, a novel attack framework that satisfies all desiderata, while stealing user data from gradients of realistic networks, even for large batch sizes (up to 512 in our experiments) and under secure aggregation. The key insight of SEER is the use of a secret decoder, which is jointly trained with the shared model. Our work represents a promising first step towards more principled treatment of MS attacks, paving the way for realistic data stealing that can compromise user privacy in real-world deployments.Most prior MS attacks rely on one of two key underlying principles. One attack class[13,14,15,16]uses malicious model modifications to encourage different types of sparsity in dense layer gradients, enabling the application of analytical honest attacks-we refer to these attacks as boosted analytical.Other attacks utilize example disaggregation[17,18], reducing the effective batch size in the gradient space by restricting the gradient flow, which permits the use of optimization-based honest attacks.Client-side detectability Nearly all prior work in the field[8,13,14,15,19,17,18,20]has raised the issue of client-side detectability of MS attacks, i.e., an FL client may be able to detect malicious server activity, and decide to opt out of the current or all future rounds. Despite such concerns, no attempts have so far been made to study, quantify, or improve client-side detectability of MS attacks.This work: detecting and disguising malicious server attacks In this work, we thoroughly study the question of client-side detectability of MS attacks. We demonstrate that while boosted analytical and example disaggregation attacks pose a real threat as zero-day exploits, now that their key principles are known, all current (and future) attacks from these two classes are client-side detectable in a principled way, bringing into question their practicality. Notably, we demonstrate the detectability of example disaggregation attacks by introducing D-SNR, a novel vulnerability metric.With this in mind, we observe that such limitations of prior MS attacks arise from their fundamental reliance on the honest attacks they lift. Namely, boosted analytical attacks always require handcrafted modifications which are weight-space detectable, and example disaggregation attacks rely on the success of disaggregation, which is equally evident to any party observing the gradients, i.e., it is gradient-space detectable. This illustrates the need for fundamentally different attack approaches.As a promising first step in that direction, we propose a novel attack framework SEER, which recovers data from batch sizes up to 512, yet is by design harder to detect than prior attacks. Our key insights are that (i) gradient-space detection can be evaded using a secret decoder, disaggregating the data in a space unknown to clients, and (ii) jointly optimizing the decoder and the shared model with SGD avoids handcrafted modifications and allows for effective reconstruction. Importantly, SEER does not lift any prior honest attack and does not require restrictive assumptions such as the ability to tweak the architecture, side-channel information, or knowledge of batch normalization data or labels.Key contributions Our work makes the following contributions.", "authors": [], "concepts": ["way,", "future", "demonstrate", "rounds.", "boosted", "analytical.other", "class[13,14,15,16]uses", "detectable", "restrictive", "secure", "most", "problem", "importantly,", "equally", "disaggregation,", "question", "reconstruction.", "future)", "model", "user", "secret", "restricting", "concerns", "they", "settings", "server", "paving", "principled", "avoids", "such", "principles", "satisfies", "these", "large", "sight:", "nearly", "limitations", "insights", "prior", "analytical", "does", "despite", "been", "were", "(ii)", "disaggregation[17,18],", "able", "more", "from", "into", "gradient-space", "practicality.", "experiments)", "(and", "optimizing", "federated", "malicious", "however,", "seer,", "gradients", "desiderata", "using", "plain", "many", "lift", "while", "exploits,", "towards", "fundamental", "framework", "propose", "issue", "approaches.as", "metric.with", "information,", "sizes", "clients,", "(ms)", "once", "flow,", "detectable,", "detectable.", "current", "principles,", "disaggregating", "utilize", "networks,", "scaling", "contributions.", "work", "treatment", "hiding", "made", "time,", "raised", "deployments.most", "learning", "mind,", "improve", "application", "contributions", "example", "handcrafted", "gradient", "field[8,13,14,15,19,17,18,20]has", "decoder", "i.e.,", "weight-space", "study,", "ability", "questioning", "pose", "principles.", "rely", "classes", "previously", "space", "promising", "types", "success", "detectability.", "novel", "party", "raised,", "permits", "real-world", "known.", "private.", "step", "client-side", "evident", "tweak", "observe", "represents", "makes", "shared", "attacks,", "optimization-based", "data", "d-snr,", "checks.", "stealing", "notably,", "with", "introducing", "honest", "seer", "design", "jointly", "refer", "considered", "dense", "trained", "detection", "attempts", "work,", "threat", "observing", "assumptions", "that", "aggregation,", "recovers", "fundamentally", "disaggregation", "real", "their", "attacks.this", "knowledge", "labels.key", "desiderata,", "disguising", "different", "modifications", "normalization", "decoder,", "regarding", "model.", "effective", "reliance", "enabled", "sparsity", "need", "always", "aggregation.", "enabling", "first", "reducing", "size", "allows", "work:", "harder", "vulnerability", "realistic", "concerns,", "batch", "unknown", "practical", "privacy", "following", "quantify,", "detecting", "evaded", "layer", "detect", "attacks-we", "thoroughly", "bringing", "zero-day", "gradients,", "formulate", "publicly", "illustrates", "than", "attacks.client-side", "this", "which", "insight", "practicality", "known,", "architecture,", "attack", "lift.", "direction,", "detectability", "have", "compromise", "attacks", "client", "underlying", "under", "even", "study", "require", "decide", "arise", "attacks.", "namely,", "512,", "activity,", "side-channel", "further,", "encourage"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259108646": {"id": "259108646", "openalex": null, "doi": null, "title": "ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process", "abstract": "Image recognition and generation have long been developed independently of each other. With the recent trend towards general-purpose representation learning, the development of general representations for both recognition and generation tasks is also promoted. However, preliminary attempts mainly focus on generation performance, but are still inferior on recognition tasks. These methods are modeled in the vector-quantized (VQ) space, whereas leading recognition methods use pixels as inputs. Our key insights are twofold: (1) pixels as inputs are crucial for recognition tasks; (2) VQ tokens as reconstruction targets are beneficial for generation tasks. These observations motivate us to propose an Alternating Denoising Diffusion Process (ADDP) that integrates these two spaces within a single representation learning framework. In each denoising step, our method first decodes pixels from previous VQ tokens, then generates new VQ tokens from the decoded pixels. The diffusion process gradually masks out a portion of VQ tokens to construct the training samples. The learned representations can be used to generate diverse high-fidelity images and also demonstrate excellent transfer performance on recognition tasks. Extensive experiments show that our method achieves competitive performance on unconditional generation, ImageNet classification, COCO detection, and ADE20k segmentation. Importantly, our method represents the first successful development of general representations applicable to both generation and dense recognition tasks. Code shall be released.", "authors": [], "concepts": ["demonstrate", "each", "generation,", "learning,", "mainly", "developed", "denoising", "tasks;", "twofold:", "tasks", "importantly,", "used", "show", "method", "samples.", "recent", "within", "tokens", "unconditional", "these", "insights", "decoded", "decodes", "been", "general", "high-fidelity", "performance", "images", "from", "pixels.", "other.", "however,", "generation", "segmentation.", "released.", "diverse", "towards", "pixels", "propose", "excellent", "achieves", "process", "vector-quantized", "transfer", "both", "(vq)", "methods", "inputs.", "learning", "successful", "reconstruction", "performance,", "long", "inferior", "integrates", "experiments", "inputs", "diffusion", "competitive", "alternating", "code", "image", "ade20k", "represents", "training", "tasks.", "with", "whereas", "space,", "masks", "dense", "spaces", "portion", "modeled", "attempts", "that", "imagenet", "beneficial", "targets", "previous", "preliminary", "tokens,", "gradually", "learned", "representations", "leading", "first", "independently", "focus", "trend", "promoted.", "general-purpose", "observations", "coco", "also", "then", "recognition", "detection,", "still", "development", "have", "framework.", "addp:", "classification,", "applicable", "representation", "motivate", "step,", "crucial", "generate", "(addp)", "shall", "single", "extensive", "construct", "generates"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259138821": {"id": "259138821", "openalex": null, "doi": null, "title": "A Probabilistic Framework for Modular Continual Learning", "abstract": "Modular approaches, which use a different composition of modules for each problem and avoid forgetting by design, have been shown to be a promising direction in continual learning (CL). However, searching through the large, discrete space of possible module compositions is a challenge because evaluating a composition's performance requires a round of neural network training. To address this challenge, we develop a modular CL framework, called PICLE, that accelerates search by using a probabilistic model to cheaply compute the fitness of each composition. The model combines prior knowledge about good module compositions with datasetspecific information. Its use is complemented by splitting up the search space into subsets, such as perceptual and latent subsets. We show that PICLE is the first modular CL algorithm to achieve different types of transfer while scaling to large search spaces. We evaluate it on two benchmark suites designed to capture different desiderata of CL techniques. On these benchmarks, PICLE offers significantly better performance than state-of-the-art CL baselines.Preprint. Under review.", "authors": [], "concepts": ["state-of-the-art", "each", "problem", "model", "show", "challenge,", "composition", "datasetspecific", "such", "these", "about", "large", "prior", "been", "shown", "through", "probabilistic", "good", "performance", "into", "picle", "however,", "complemented", "desiderata", "using", "offers", "while", "benchmarks,", "requires", "framework", "capture", "design,", "picle,", "compositions", "forgetting", "transfer", "scaling", "because", "designed", "avoid", "information.", "better", "learning", "direction", "splitting", "significantly", "approaches,", "latent", "fitness", "space", "network", "cheaply", "promising", "types", "round", "achieve", "combines", "search", "modules", "neural", "with", "discrete", "searching", "evaluating", "benchmark", "that", "training.", "accelerates", "knowledge", "subsets,", "different", "module", "called", "first", "address", "challenge", "spaces.", "composition.", "possible", "review.", "baselines.preprint.", "than", "this", "which", "suites", "algorithm", "framework,", "subsets.", "techniques.", "develop", "have", "(cl).", "large,", "evaluate", "under", "perceptual", "composition's", "compute", "modular", "continual"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259165244": {"id": "259165244", "openalex": null, "doi": null, "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models", "abstract": "The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks.(2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge hallucination. We evaluate 21 open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing LLMs and knowledge-related systems.", "authors": [], "concepts": ["improvements", "essential", "unseen", "corpora,", "metric", "merely", "data,", "tasks", "believe", "dataset", "systems.", "system,", "overall", "wikipedia,", "ensure", "knowledge-related", "will", "breadth", "large", "unprecedented", "carefully", "performance", "abilities,", "emerging", "along", "contrastive", "models", "four-level", "scores", "rather", "some", "tasks.(2)", "both", "necessitates", "unique", "exploring", "mimic", "better", "llms", "automatically", "ability", "updated", "evaluations.", "criteria,", "hallucination.", "importance", "numerical", "knowledge-oriented", "comparisons,", "evolving", "meticulous", "form", "aiming", "covering", "commercial", "data", "designs", "world", "across", "adopt", "with", "corpus", "including", "modeling,", "given", "design", "thoughtful", "standard", "kola:", "evaluating", "developing", "capacity", "benchmark", "self-contrast", "knowledge", "handle", "language", "collected", "factors:", "findings.", "llms,", "benchmarking", "https://kola.xlore.cn", "references", "provide", "unbiased,", "kola", "three", "continuously", "taxonomy", "thorough,", "evaluation", "open-participation", "publicly", "open-source", "than", "which", "(kola),", "prevalently", "intriguing", "cognition", "assessment", "comparability", "released", "knowledge.", "pre-trained", "evaluate", "fair", "applicable", "leaderboard", "crucial", "human", "obtain", "construct", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259165262": {"id": "259165262", "openalex": null, "doi": null, "title": "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios", "abstract": "Recent deep neural networks (DNNs) have come to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. These attacks significantly undermine the reliability of DNNs. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we address this limitation by introducing a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as data-constrained backdoor attacks. In such cases, previous attack methods suffer from severe efficiency degradation due to the entanglement between benign and poisoning features during the backdoor injection process.IntroductionDeep neural networks (DNNs) are widely utilized and powerful machine learning algorithms inspired by the structure and functioning of the human brain. They excel at learning intricate patterns in data, making them invaluable for various applications such as image recognition[17,21], natural language processing[33,68], image generation[20,30], and anomaly detection[45,64]. However, the effectiveness of DNNs heavily relies on the quantity and quality of the training data. For instance, Stable Diffusion [49], a generative model with 983 million parameters, owes its success in image generation tasks to pre-training on 5 billion image-text pairs. Similarly, GPT-3 [3], a language model with 175 billion * Equal Contribution.", "authors": [], "concepts": ["widely", "machine", "paper,", "access", "undermine", "parameters,", "data,", "tasks", "intricate", "model", "assuming", "them", "comes", "recognition[17,21],", "pairs.", "million", "recent", "processing[33,68],", "they", "carry", "complete", "inspired", "relies", "efficient", "such", "these", "existing", "vast", "multiple", "pre-training", "more", "from", "deep", "malicious", "however,", "similarly,", "quantity", "generation", "generative", "structure", "backdoor", "networks", "data.", "full", "methods", "features", "victims", "learning", "collect", "dnns", "heavily", "data-constrained", "benign", "significantly", "amounts", "invaluable", "applications", "rely", "come", "[3],", "source", "billion", "stable", "diffusion", "success", "degradation", "poisoning", "providing", "real-world", "image", "powerful", "data", "training", "neural", "making", "functioning", "with", "gpt-3", "introducing", "severe", "refer", "efficiency", "scenarios", "attackers", "that", "between", "dnns.", "unrealistic", "contaminate", "language", "exploit", "previous", "[49],", "suffer", "natural", "patterns", "contribution.", "make", "effectiveness", "instance,", "cases,", "address", "reliability", "scenario", "owes", "realistic", "opportunity", "(dnns)", "algorithms", "sources,", "utilized", "generation[20,30],", "process.introductiondeep", "various", "where", "detection[45,64].", "quality", "this", "during", "anomaly", "injection", "attack", "cannot", "have", "image-text", "entanglement", "limitation", "attacks", "assumptions,", "brain.", "excel", "attacks.", "human", "single", "equal"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259203115": {"id": "259203115", "openalex": null, "doi": null, "title": "A SIMPLE AND EFFECTIVE PRUNING APPROACH FOR LARGE LANGUAGE MODELS", "abstract": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance.Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive.In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs.Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis.Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is.We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks.Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update.Code is available at https://github.com/locuslab/wanda.", "authors": [], "concepts": ["competitively", "llama", "retraining,", "paper,", "emergent", "performs", "problem", "straightforward", "observation", "used", "expensive.in", "method", "available", "recent", "prunes", "subset", "either", "methods,", "large", "(pruning", "pruned", "magnitudes", "languages", "however,", "basis.notably,", "billion-scale", "induce", "magnitude", "second-order", "multiplied", "while", "baseline", "smallest", "simple", "requires", "thorough", "information,", "models", "method,", "increases,", "conduct", "update,", "designed", "features", "retraining", "is.we", "reconstruction", "intensive", "solving", "llama-2", "significantly", "activations),", "computationally", "performance.existing", "pretrained", "network", "methods:", "corresponding", "candidates", "input", "striving", "https://github.com/locuslab/wanda.", "across", "with", "novel,", "update.code", "benchmarks.wanda", "reliant", "approach", "outperforms", "that", "wanda", "their", "weights", "involving", "language", "termed", "weight", "effective", "natural", "sparsity", "llms,", "approaches", "size", "preserve", "activations,", "per-output", "also", "rarely", "introduce", "established", "various", "evaluation", "pruning", "this", "which", "affordable", "drop", "require", "against", "llms.motivated", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259203325": {"id": "259203325", "openalex": null, "doi": null, "title": "GIO: GRADIENT INFORMATION OPTIMIZATION FOR TRAINING DATASET SELECTION", "abstract": "It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task-and domain-agnostic and can be applied out-of-the-box to new datasets and domains.Active learning. Active learning methods (e.g. Sener and Savarese, 2018; Gal et al., 2017; Kirsch  et al., 2019)  can be cast as data selection methods in our sense. In active learning, one iteratively chooses new unlabeled training examples to label, with the goal of efficiently creating a powerful train set. By contrast, GIO makes no use of labels and is oriented towards the goal of identifying a subset of existing cases to use for training. Additionally, active learning is most suited to classification problems, whereas GIO works with any arbitrary task.Heuristic. GIO is closer to recent methods in which one uses a large language model to generate a large number of candidate texts and then extracts a subset of them based on a specific criteria. For example, Brown et al. (2020) develop a heuristic method to filter CommonCrawl based on a trained classifier's probability that datapoints are high quality. Similarly, Wenzek et al. (2020) develop a pipeline to clean CommonCrawl based principally on the perplexity of an LM trained on high quality text, and Xie et al. (2023) develop a sampling technique based on approximate n-gram counts.Like GIO, these heuristic methods aim to select a subset of data that is higher quality and more relevant. However, they are either highly tailored to their particular tasks or they require very large numbers of examples (to develop classifiers or construct target probabilities). By contrast, GIO is task-and domain-agnostic, it can be applied plug-and-play to a new task and dataset, and it requires comparatively few gold examples X to serve as the target distribution.Similarity Search. Methods using vector or n-gram similarity search can also be used for data selection at scale (e.g. Johnson et al., 2017;Bernhardsson, 2017;Santhanam et al., 2022). The technique would index G and X and retrieve the top-k datapoints from G for each point in X. Like our method, similarity search works in a continuous space. However, similarity search can be prone to selecting suboptimal points; we review such a case in detail in Section 3.4. Additionally, similarity search does not have a natural stopping criterion and requires data size to be chosen arbitrarily. Is 10% data enough? 20%? We don't know a priori. And if the data in G is far away from X, similarity search will still choose it up to the desired data size. Recently, Yao et al. (2022) use a BM25 retrieval method for data selection, with strong results. However, BM25 operates on a bag-of-words model, which can make it challenging when the target set is small, and like any similarity search, requires data size to be chosen arbitrarily beforehand. Further, this method only applies to text tasks, whereas GIO applies to any task with continuous representation.", "authors": [], "concepts": ["case", "task.heuristic.", "outstanding", "size.", "challenging", "applies", "machine", "identifying", "each", "perplexity", "learning,", "selecting", "most", "robust", "tasks", "scalable,", "problem", "particular", "approximate", "recognition,", "continuous", "model", "used", "them", "show", "method", "distribution.similarity", "available", "recent", "dataset", "they", "vector", "subset", "cases", "stopping", "index", "arbitrarily.", "information", "either", "efficient", "such", "will", "these", "existing", "large", "only", "scale", "itself.", "review", "would", "does", "domain-agnostic", "task-and", "3.4.", "examples", "(unlabeled)", "specific", "text,", "number", "highly", "datasets", "through", "classification", "performance.", "more", "sampling", "from", "variable", "present", "however,", "similarly,", "criterion", "using", "counts.like", "wenzek", "example,", "clean", "brown", "filter", "towards", "bag-of-words", "dataset,", "gio:", "task", "applied", "very", "simple", "requires", "2019)", "selection", "models", "method,", "implementation.", "learning.", "enough?", "away", "begins", "because", "section", "methods", "made", "domain-agnostic,", "spelling", "advantageous", "learning", "contribution", "goal", "probability", "desired", "principally", "like", "label,", "gradient", "recently,", "target", "translation,", "examples,", "without", "2022).", "operates", "prone", "closer", "numbers", "sener", "gio,", "experiments", "high", "selection,", "uses", "heuristic", "practice.", "text", "retrieve", "model,", "detail", "when", "based", "image", "powerful", "sacrificing", "makes", "data", "representing", "training", "chosen", "search", "iteratively", "chooses", "works", "search,", "with", "train", "relaxation", "whereas", "natural,", "savarese,", "delivers", "20%?", "point", "space.", "trained", "choose", "active", "additionally,", "oriented", "gold", "results.", "approach", "that", "training.", "sets.", "suboptimal", "out-of-the-box", "datapoints", "creating", "don't", "texts", "2017;bernhardsson,", "comparatively", "kirsch", "pipeline", "their", "cast", "criteria.", "strong", "arbitrary", "language", "findings", "different", "representation.", "(2022)", "higher", "n-gram", "bm25", "scalable", "natural", "unlabeled", "make", "contrast,", "2018;", "technique", "(2020)", "relevant.", "efficiently", "2017;", "plug-and-play", "correction,", "know", "size", "(2023)", "set.", "arbitrarily", "beforehand.", "objective", "showing", "labels", "search.", "retrieval", "tailored", "information-theoretic", "fewer", "results", "problems,", "al.,", "also", "select", "serve", "then", "candidate", "quality", "commoncrawl", "tasks,", "intractable", "this", "which", "still", "suited", "task-agnostic", "priori.", "develop", "extracts", "classifiers", "have", "small", "often", "sense.", "optimization", "2017;santhanam", "classifier's", "(gio),", "probabilities).", "small,", "(e.g.", "representation", "require", "generate", "top-k", "johnson", "points;", "domains.active", "quality.", "similarity", "further,", "construct", "distribution.", "hyperparameters"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259298238": {"id": "259298238", "openalex": null, "doi": null, "title": "KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals", "abstract": "For text clustering, there is often a dilemma: one can either first embed each examples independently and then compute pair-wise similarities based on the embeddings, or use a crossattention model that takes a pair of examples as input and produces a similarity. The former is more scalable but the similarities often have lower quality, whereas the latter does not scale well but produces higher quality similarities. We address this dilemma by developing a clustering algorithm that leverages the best of both worlds: the scalability of former and the quality of the latter. We formulate the problem of text clustering with embeddingbased and cross-attention models as a novel version of the Budgeted Correlation Clustering problem (BCC) where along with a limited number of queries to an expensive oracle (a cross-attention model in our case), we have unlimited access to a cheaper but less accurate second oracle (embedding similarities in our case). We develop a theoretically motivated algorithm that leverages the cheap oracle to judiciously query the strong oracle while maintaining high clustering quality. We empirically demonstrate gains in query minimization and clustering metrics on a variety of datasets with diverse strong and cheap oracles.", "authors": [], "concepts": ["expensive-strong", "dilemma", "demonstrate", "each", "former", "accurate", "access", "embeddings,", "cross-attention", "problem", "expensive", "model", "version", "latter", "cheap", "either", "scale", "does", "oracle", "examples", "case).", "clustering,", "number", "datasets", "more", "along", "well", "while", "worlds:", "quality,", "signals", "diverse", "models", "there", "latter.", "produces", "(bcc)", "both", "query", "lower", "pair", "cheaper", "leverages", "motivated", "scalability", "clustering", "embeddingbased", "similarity.", "high", "similarities", "judiciously", "text", "novel", "based", "embed", "takes", "input", "theoretically", "kwikbucks:", "with", "whereas", "crossattention", "case),", "less", "developing", "maintaining", "that", "unlimited", "cheap-weak", "strong", "minimization", "higher", "pair-wise", "scalable", "(embedding", "empirically", "first", "dilemma:", "independently", "address", "similarities.", "metrics", "second", "best", "where", "then", "formulate", "quality", "budgeted", "this", "algorithm", "develop", "have", "often", "correlation", "limited", "oracles.", "gains", "variety", "queries", "quality.", "compute"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259342096": {"id": "259342096", "openalex": null, "doi": null, "title": "Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models", "abstract": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, as we find that MoE models benefit more from instruction tuning than dense models. In particular, we conduct empirical studies across three experimental setups: (i) Direct finetuning on individual downstream tasks devoid of instruction tuning; (ii) Instruction tuning followed by in-context few-shot or zero-shot generalization on downstream tasks; and (iii) Instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models overall underperform dense models of identical computational capacity. This narrative, however, dramatically changes with the introduction of instruction tuning (second and third scenario), used independently or in conjunction with task-specific finetuning. Our most powerful model, FLAN-MOE 32B , surpasses the performance of FLAN-PALM 62B on four benchmark tasks, while using only a third of the FLOPs. The advancements embodied by FLAN-MOE inspire a reevaluation of the design principles of large-scale, high-performance language models in the framework of task-agnostic learning. * Work done at Google Preprint. Under review. arXiv:2305.14705v2 [cs.CL] 5 Jul 2023 2.2 Instruction Fine-tuning RecipeWe fine-tune FLAN-MOE using the prefix language model objective on the FLAN collective dataset[4,28]. Each FLAN-MOE will inherit the auxiliary loss setting during pre-training. All the model parameters will be updated. We adapt the sequence length of each FLAN-MOE to 2, 048 for input and 512 for output based on the relative position embedding. The dropout rate is 0.05 and the expert dropout rate is 0.2. The learning rate is 1e \u22124 . The optimizer setting follows [4].ExperimentWe study FLAN-MOE in the context of instruction-tuning. We first perform a controlled comparison of FLAN-MOE to an equivalent \"standard\" dense encoder-decoder Transformer (T5), across a range of model sizes in Section 3.2. We subsequently demonstrate in Section 3.3 that scaling up our model, referred to as FLAN-MOE, can attain remarkable performance levels. Our most extensive model, FLAN-ST 32B , surpasses the performance of FLAN-PALM 62B while utilizing less than 30% of FLOPs per token. We further ablate the various design decisions in the next Section. 3.1 Settings Traning Data. By default, all models are trained on the 1,836 finetuning tasks by combining four mixtures from prior work: Muffin, T0-SF, NIV2, and CoT, as in [4]. Specifically, Muffin comprises 80 tasks from [52] and 26 dialog/program synthesis tasks; T0-SF comprises 193 tasks from [44]; NIV2 comprises 1554 tasks from [51]; CoT comprises 9 reasoning tasks.Evaluations. We conduct both zero-shot and few-shot evaluations on held-out tasks as in [4] which were not included as part of the finetuning data. We use MMLU [16] that includes exam questions from 57 tasks such as mathematics, history, law, and medicine; BBH includes 23 challenging", "authors": [], "concepts": ["combination", "inference", "challenging", "[51];", "[52]", "demonstrate", "cost.", "surpasses", "each", "parameters", "fine-tune", "studies", "tasks;", "most", "tasks", "particular,", "preprint.", "\"standard\"", "model", "used", "rate", "downstream", "referred", "settings", "overall", "[cs.cl]", "pre-training.", "decisions", "includes", "subsequently", "such", "principles", "follows", "will", "these", "[16]", "large", "only", "finetuning.", "prefix", "prior", "optimizer", "scenario),", "were", "(iii)", "(ii)", "empirical", "more", "performance", "from", "supplemented", "part", "however,", "included", "follow", "using", "synthesis", "arxiv:2305.14705v2", "medicine;", "high-performance", "while", "meets", "auxiliary", "learnable", "updated.", "flan-palm", "relative", "flan", "flops", "framework", "[44];", "[4].", "sizes", "models", "generalization", "law,", "learning.", "data.", "conduct", "instructions.", "instruction-tuning.", "position", "both", "scaling", "section", "work", "dialog/program", "advancements", "conjunction", "introduction", "narrative,", "learning", "attain", "experimental", "llms", "section.", "inherit", "tuning;", "equivalent", "output", "without", "sparse", "0.05", "t0-sf", "levels.", "approaches,", "done", "large-scale,", "winning", "t0-sf,", "devoid", "dramatically", "changes", "inspire", "google", "1,836", "dropout", "specifically,", "instruction", "model,", "based", "held-out", "dataset[4,28].", "powerful", "combining", "individual", "flan-st", "input", "training", "evaluations", "context", "followed", "tasks.", "neural", "reasoning", "across", "with", "few-shot", "direct", "design", "niv2", "less", "comparison", "questions", "dense", "trained", "finetuning", "tuning", "collective", "benchmark", "0.2.", "four", "(second", "that", "scenario,", "traning", "comprises", "expert", "reevaluation", "token.", "benefit", "advocate", "language", "in-context", "fine-tuning", "transformer", "sequence", "third", "ablate", "technique", "mathematics,", "3.2.", "range", "next", "first", "default,", "independently", "flan-moe,", "muffin", "history,", "objective", "work:", "utilizing", "controlled", "flops.", "tasks.evaluations.", "three", "computational", "find", "perform", "mixture-of-experts", "review.", "utilized", "(t5),", "increasing", "setups:", "various", "mmlu", "mixtures", "than", "zero-shot", "models.", "tasks,", "task-specific", "this", "which", "encoder-decoder", "during", "task-agnostic", "adapt", "underperform", "recipewe", "loss", "embodied", "muffin,", "embedding.", "exam", "flan-moe", "length", "[4].experimentwe", "under", "architecture", "tuning:", "study", "cot,", "remarkable", "further", "capacity.", "extensive", "(llms)", "setting", "niv2,", "identical", "(moe)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259360601": {"id": "259360601", "openalex": null, "doi": null, "title": "Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight", "abstract": "This paper studies the sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs), a challenging problem in reinforcement learning that is known to be exponentially hard in the worst-case. Motivated by real-world settings such as loading in game playing, we propose an enhanced feedback model called \"multiple observations in hindsight\", where after each episode of interaction with the POMDP, the learner may collect multiple additional observations emitted from the encountered latent states, but may not observe the latent states themselves. We show that sample-efficient learning under this feedback model is possible for two new subclasses of POMDPs: multi-observation revealing POMDPs and distinguishable POMDPs. Both subclasses generalize and substantially relax revealing POMDPs-a widely studied subclass for which sample-efficient learning is possible under standard trajectory feedback. Notably, distinguishable POMDPs only require the emission distributions from different latent states to be different instead of linearly independent as required in revealing POMDPs. * Fudan University.", "authors": [], "concepts": ["pomdps-a", "challenging", "feedback.", "known", "partially", "widely", "each", "distinguishable", "learner", "studies", "problem", "sample-efficient", "model", "show", "multi-observation", "settings", "enhanced", "pomdps", "such", "only", "game", "multiple", "revealing", "hindsight\",", "from", "pomdps:", "hindsight", "emission", "propose", "reinforcement", "both", "\"multiple", "relax", "learning", "collect", "encountered", "additional", "motivated", "substantially", "linearly", "university.", "interaction", "latent", "decision", "exponentially", "real-world", "states,", "observe", "emitted", "notably,", "with", "standard", "observable", "states", "playing,", "subclasses", "themselves.", "independent", "loading", "that", "instead", "markov", "trajectory", "required", "pomdp,", "different", "distributions", "generalize", "called", "paper", "feedback", "possible", "observations", "episode", "where", "worst-case.", "this", "which", "fudan", "after", "(pomdps),", "subclass", "pomdps.", "studied", "processes", "under", "require", "hard", "sample-efficiency"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259375870": {"id": "259375870", "openalex": null, "doi": null, "title": "Teaching Arithmetic to Small Transformers", "abstract": "Large language models like GPT-4 exhibit emergent capabilities across generalpurpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the nexttoken prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities. 2 * Authors contributed equally to this paper. 2 Our code is available at https://github.com/lee-ny/teaching_arithmetic Preprint. Under review.", "authors": [], "concepts": ["effects", "learn", "demonstrate", "teaching", "basic", "learning,", "nexttoken", "arithmetic", "formatting", "emergent", "data,", "most", "tasks", "particular", "preprint.", "equally", "model", "available", "accuracy.", "complete", "root,", "includes", "conventional", "such", "these", "large", "prior", "high-quality,", "discuss", "next-token", "examine", "through", "eliciting", "from", "matrix", "leads", "completion.", "rapidly", "using", "explicitly", "function", "simple", "convergence", "models", "generalization", "some", "exhibit", "multiplication,", "random", "work", "scale.", "next-word", "speed.", "improve", "absence", "like", "simultaneously", "significantly", "interplay", "characteristics", "elementary", "paper.", "instructive", "changes", "importance", "text", "when", "prompting,", "code", "step", "improves", "addition,", "data", "accuracy,", "training", "generalpurpose", "contributed", "transformers,", "gpt-4", "complexity,", "across", "challenges.", "chain-of-thought", "train", "few-shot", "square", "trained", "additionally,", "highlights", "work,", "results.", "approach", "unsupervised,", "which,", "explained", "that", "functions", "capabilities.", "between", "transformers", "considers", "low-rank", "transitions", "arithmetic,", "sharp", "initialization,", "language", "investigates", "prediction", "pretraining,", "intermediate", "effective", "https://github.com/lee-ny/teaching_arithmetic", "efficiently", "first", "cases,", "encoded", "connections", "objective", "building", "phase", "review.", "style", "also", "then", "sample", "tasks,", "this", "operations", "during", "authors", "length", "small", "scale,", "objective.", "under", "even", "study", "capabilities", "extensive", "though"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259841489": {"id": "259841489", "openalex": null, "doi": null, "title": "MARTINGALE POSTERIOR NEURAL PROCESSES", "abstract": "A Neural Process (NP) estimates a stochastic process implicitly defined with neural networks given a stream of data, rather than pre-specifying priors already known, such as Gaussian processes. An ideal NP would learn everything from data without any inductive biases, but in practice, we often restrict the class of stochastic processes for the ease of estimation. One such restriction is the use of a finite-dimensional latent variable accounting for the uncertainty in the functions drawn from NPs. Some recent works show that this can be improved with more \"data-driven\" source of uncertainty such as bootstrapping. In this work, we take a different approach based on the martingale posterior, a recently developed alternative to Bayesian inference. For the martingale posterior, instead of specifying prior-likelihood pairs, a predictive distribution for future data is specified. Under specific conditions on the predictive distribution, it can be shown that the uncertainty in the generated future data actually corresponds to the uncertainty of the implicitly defined Bayesian posteriors. Based on this result, instead of assuming any form of the latent variables, we equip a NP with a predictive distribution implicitly defined with neural networks and use the corresponding martingale posteriors as the source of uncertainty. The resulting model, which we name as Martingale Posterior Neural Process (MPNP), is demonstrated to outperform baselines on various tasks.", "authors": [], "concepts": ["future", "learn", "specified.", "outperform", "uncertainty", "stream", "developed", "data,", "biases,", "posteriors.", "recently", "assuming", "(np)", "show", "recent", "resulting", "finite-dimensional", "martingale", "name", "such", "equip", "variables,", "would", "specific", "shown", "more", "from", "variable", "predictive", "inference.", "demonstrated", "nps.", "practice,", "estimates", "networks", "rather", "process", "some", "pre-specifying", "(mpnp),", "conditions", "improved", "bayesian", "gaussian", "without", "latent", "source", "priors", "actually", "inductive", "restrict", "model,", "based", "class", "form", "accounting", "posterior,", "corresponding", "already", "generated", "data", "ideal", "prior-likelihood", "tasks.", "neural", "works", "with", "restriction", "given", "uncertainty.", "pairs,", "work,", "approach", "that", "functions", "instead", "distribution", "estimation.", "corresponds", "alternative", "posteriors", "everything", "different", "specifying", "\"data-driven\"", "take", "distribution,", "processes.", "result,", "baselines", "various", "than", "this", "which", "known,", "ease", "implicitly", "drawn", "defined", "often", "processes", "under", "posterior", "bootstrapping.", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259847777": {"id": "259847777", "openalex": null, "doi": null, "title": "SAN: INDUCING METRIZABILITY OF GAN WITH DISCRIMINATIVE NORMALIZED LINEAR LAYER", "abstract": "Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the target distribution. We derive metrizable conditions, sufficient conditions for the discriminator to serve as the distance between the distributions by connecting the GAN formulation with the concept of sliced optimal transport. Furthermore, by leveraging these theoretical results, we propose a novel GAN training scheme, called slicing adversarial network (SAN). With only simple modifications, a broad class of existing GANs can be converted to SANs. Experiments on synthetic and image datasets support our theoretical results and the SAN's effectiveness as compared to usual GANs. Furthermore, we also apply SAN to StyleGAN-XL, which leads to state-of-the-art FID score amongst GANs for class conditional generation on ImageNet 256\u00d7256. arXiv:2301.12811v3 [cs.LG] 6 Sep 2023 Preprint Table 1: Common GAN losses do not simultaneously satisfy all the sufficient conditions given in Theorem 5.3. Thus, we propose the SAN to address one of the conditions, direction optimality. Even if a direction \u03c9 is the maximizer of the inner problems V, it does not satisfy direction optimality except in Wasserstein GAN (see Sec. 6). The results in Appx. E empirically demonstrate that a discriminator trained on Wasserstein GAN tends not to satisfy separability. The last condition of injectivity depends on the discriminator implementation (see Appx. E for empirical verification). Direction optimality Separability Injectivity Wassertein GAN \u2713 weak * GAN (Hinge, Saturating, Non-saturating) \u2717 \u2713 * SAN (Hinge, Saturating, Non-saturating) \u2713 \u2713 *In this paper, we provide a novel perspective on GAN optimization, which helps us to consider whether a discriminator is metrizable. Definition 1.1 (Metrizable discriminator). Let \u00b5 \u03b8 and \u00b5 0 be measures. Given an objective function J (\u03b8; \u00b7) for \u03b8, a discriminator f is (J , D)or J -metrizable for \u00b5 \u03b8 and \u00b5 0 , if J (\u03b8; f ) is minimized only with \u03b8 \u2208 arg min \u03b8 D(\u00b5 0 , \u00b5 \u03b8 ) for a certain distance on measures, D(\u00b7, \u00b7).To evaluate the dissimilarity with a given GAN minimization problem J , we are interested in other conditions besides the discriminator's optimality. Hence, we propose metrizable conditions, namely, direction optimality, separability, and injectivity, that induce J -metrizable discriminator. To achieve this, we first introduce a divergence, called functional mean divergence (FM * ), in Sec. 3. We connect the FM * with the minimization objective function of Wasserstein GAN. Then, we obtain the metrizable conditions for Wasserstein GAN by investigating Question 1.2. We provide an answer to this question in Sec. 4 by relating the FM * to the concept of sliced optimal transport(Bonneel et al., 2015;Kolouri et al., 2019). Then, in Sec. 5, we formalize the proposed conditions for Wasserstein GAN and further extend the result to generic GAN. Question 1.2. Under what conditions is FM * a distance?Based on the derived metrizable conditions, we propose the Slicing Adversarial Network (SAN) in Sec. 6. As seen inTable 1, we find that optimal discriminators for most existing GANs do not satisfy direction optimality. Hence, we develop a modification scheme for GAN maximization problems to enforce direction optimality on our discriminator. Owing to the scheme's simplicity, GANs can easily be converted to SANs. We conduct experiments to verify our perspective and demonstrate that SANs are superior to GANs in certain generation tasks on synthetic and image datasets. In particular, we confirmed a SAN improves state-of-the-art FID for conditional generation with StyleGAN-XL (Sauer et al., 2022) on ImageNet 256\u00d7256 despite the simple modifications.", "authors": [], "concepts": ["wasserstein", "state-of-the-art", "learn", "demonstrate", "what", "injectivity", "modification", "enforce", "paper,", "transport.", "discriminator.", "losses", "most", "tasks", "particular,", "problem", "converted", "theoretical", "question", "confirmed", "modifications,", "injectivity,", "verify", "verification).", "broad", "metrizability", "score", "addresses", "weak", "besides", "such", "concept", "certain", "arxiv:2301.12811v3", "intable", "these", "existing", "only", "normalized", "does", "discriminator).", "despite", "objectives.", "theorem", "seen", "empirical", "discriminator", "datasets", "non-saturating)", "(san)", "minimized", "minimax", "2019).", "optimizing", "leads", "gradients", "stylegan-xl", "sans", "induce", "metrizable.", "generation", "consider", "connect", "generative", "function", "derive", "distance?based", "compared", "saturating,", "definition", "simple", "propose", "optimal", "gan.", "d(\u00b7,", "tends", "measures,", "networks", "conditional", "conduct", "sec.", "derived", "furthermore,", "conditions", "amongst", "scheme", "datasets.", "usual", "linear", "superior", "this,", "conditions,", "support", "except", "functional", "direction", "probability", "thus,", "inner", "problems", "1.2.", "simultaneously", "5.3.", "target", "\u00b7).to", "stylegan-xl,", "discriminative", "satisfy", "(san).", "256\u00d7256.", "perspective", "other", "dissimilarity", "measures.", "experiments", "(sauer", "actually", "gans.", "optimality,", "network", "distance", "(hinge,", "discriminator's", "novel", "maximizer", "class", "sans.", "improves", "image", "separability,", "achieve", "training", "san's", "divergence", "slicing", "investigating", "mean", "owing", "wassertein", "divergence,", "with", "sufficient", "last", "given", "condition", "gans", "trained", "common", "close", "whether", "answer", "inducing", "that", "distribution", "connecting", "preprint", "between", "imagenet", "easily", "optimality.", "minimization", "discriminators", "table", "implementation", "metrizable", "formalize", "distributions", "extend", "make", "effectiveness", "scheme,", "result", "called", "scheme's", "apply", "empirically", "leveraging", "first", "address", "optimality", "paper", "provide", "objective", "hence,", "formulation", "d)or", "sliced", "find", "results", "[cs.lg]", "al.,", "synthetic", "also", "layer", "(see", "separability.", "introduce", "simplicity,", "serve", "transport(bonneel", "generic", "optimization,", "depends", "modifications.", "appx.", "this", "which", "separability", "provides", "results,", "adversarial", "interested", "develop", "generator", "proposed", "(gans)", "san:", "optimization", "evaluate", "under", "-metrizable", "maximization", "even", "256\u00d7256", "2022)", "(metrizable", "then,", "namely,", "helps", "relating", "further", "obtain", "2015;kolouri", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "259924554": {"id": "259924554", "openalex": null, "doi": null, "title": "SAFEDREAMER: SAFE REINFORCEMENT LEARNING WITH WORLD MODELS", "abstract": "The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria.Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks.These limitations are primarily due to model inaccuracies and inadequate sample efficiency.The integration of world models has proven effective in mitigating these shortcomings.In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework.Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and vision-only input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks.Further details and resources are available on the project website: https://sites.google.com/view/safedreamer.", "authors": [], "concepts": ["criteria.existing", "tasks.these", "safedreamer:", "enforce", "vision-only", "failure", "model", "method", "available", "efficiency.the", "within", "shortcomings.in", "methods,", "these", "details", "nearly", "limitations", "proven", "safety", "(saferl)", "performance", "into", "reinforcement", "models", "achieves", "lagrangian-based", "dreamer", "inadequate", "methods", "inaccuracies", "(rl)", "incorporating", "superior", "resources", "learning", "tasks.further", "constrained", "applications", "low-dimensional", "safedreamer,", "especially", "rely", "satisfy", "safety,", "integration", "cost", "novel", "real-world", "achieve", "world", "website:", "https://sites.google.com/view/safedreamer.", "with", "input,", "project", "framework.our", "work,", "mitigating", "functions", "balancing", "primarily", "showcasing", "effective", "safe", "fail", "safety-gymnasium", "introduce", "various", "sample", "tasks,", "this", "which", "algorithm", "efficacy", "zero-cost", "benchmark,", "spanning", "often", "processes", "planning", "deployment", "complex", "scenarios,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "260125817": {"id": "260125817", "openalex": null, "doi": null, "title": "Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization", "abstract": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an O( log n \u221a \u00b5 ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and \u00b5 measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1 t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.", "authors": [], "concepts": ["bound", "shows", "studies", "data,", "problem", "theoretical", "dataset", "correlated,", "either", "align", "number", "positively", "classification", "good", "converges", "matrix", "using", "consider", "well", "relu", "neurons'", "two-layer", "networks", "neurons", "mnist", "pair", "that,", "gradient", "rate,", "well-separated", "measures", "initialization.", "experiments", "layer.", "network", "careful", "numerical", "positive", "analysis", "label", "early", "correlated.", "corresponding", "points", "separated.", "zero", "takes", "data", "input", "achieve", "training", "negative", "low-rank.", "with", "upper", "phase,", "illustrate", "training,", "different", "weight", "findings.", "first", "flow", "vectors:", "paper", "allows", "provide", "labels", "negatively", "depending", "phase", "binary", "layer", "second", "same", "where", "dynamics", "this", "directional", "approximately", "during", "initialization", "after", "loss", "time", "alignment", "small", "neuron"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "260154786": {"id": "260154786", "openalex": null, "doi": null, "title": "Submodular Reinforcement Learning", "abstract": "In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are independent of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose submodular RL (SUBRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SUBPO, a simple policy gradient-based algorithm for SUBRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SUBPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SUBRL instances even in large state-and action-spaces. We showcase the versatility of our approach by applying SUBPO to several applications such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.", "authors": [], "concepts": ["assumption,", "demonstrate", "indeed,", "decreases", "typically", "informative", "problem", "constant", "tackle", "show", "they", "resulting", "locally", "path", "classical", "coverage", "such", "large", "light", "marginal", "more", "(and", "optimizing", "well", "many", "applications,", "instances", "derive", "simple", "modelled", "propose", "optimal", "capture", "reinforcement", "greedy", "handles", "unfortunately,", "design,", "process", "some", "maximizing", "optimize", "high-dimensional", "hand,", "versatility", "gains.", "this,", "maximization.", "learning", "submodular", "naturally", "bayesian", "general,", "showcase", "gradient", "motivated", "scalability", "i.e.,", "applications", "bandits.", "state-action", "other", "subpo,", "biodiversity", "decision", "settings,", "success", "subrl", "(mdp),", "policy", "visited", "monitoring,", "greedily", "design", "several", "states", "considered", "independent", "paradigm", "approach", "assumptions", "that", "functions", "markov", "recovers", "their", "previously.", "history-dependent)", "natural", "subpo", "efficiency,", "seeks", "approximations", "control,", "value", "non-additive", "gradient-based", "planning,", "additive,", "returns,", "spaces.", "rewards", "algorithms", "following", "moreover,", "results", "returns.", "state-and", "optimization,", "sample", "approximate.", "which", "action-spaces.", "tabular", "algorithm", "factor", "(subrl),", "have", "(rl),", "optimization", "underlying", "experiment", "similar", "under", "important", "even", "hard", "applying", "diminishing"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "260164542": {"id": "260164542", "openalex": null, "doi": null, "title": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis", "abstract": "When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.Preprint. Under review.", "authors": [], "concepts": ["smaller", "writing", "strategy", "each", "paper,", "subgoals", "tasks", "subtasks", "tackle", "model", "synthesis,", "they", "subsequently", "programs,", "meta-benchmark", "forming", "been", "informed", "able", "more", "performance", "into", "synthesis", "while", "compared", "measure", "forms", "task", "desirable", "propose", "generalization", "exedec:", "solve", "popular", "methods", "create", "better", "improved", "characterize", "problems", "step-by-step", "ability", "people", "program", "novel", "when", "compositionally", "predicts", "decomposition", "exedec,", "tasks.", "neural", "several", "trained", "whether", "that", "different", "subtasks.", "decomposing", "robustfill", "greatly", "datasets,", "difficult", "review.", "then", "step.", "baselines.preprint.", "execution", "this", "which", "decomposition-based", "simpler", "capabilities,", "familiar", "have", "similar", "under", "generalize,", "compositional", "exedec", "deepcoder.", "complex"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "260316137": {"id": "260316137", "openalex": null, "doi": null, "title": "The Marginal Value of Momentum for Small Learning Rate SGD", "abstract": "Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small-to medium-batch training from scratch on ImageNet and finetuning language models on downstream tasks.Preprint. Under review.", "authors": [], "concepts": ["known", "small-to", "theoretical", "indeed", "rate", "show", "downstream", "strongly", "settings", "momentum", "such", "scratch", "folklore", "marginal", "from", "clarify", "deep", "suggesting", "suggests", "medium-batch", "very", "tasks.preprint.", "optimal", "convergence", "models", "generalization", "convex", "both", "update,", "networks,", "learning", "gradient", "without", "short", "long", "source", "experiments", "horizons.", "noise.", "instability,", "behave", "analyses", "dominant", "training", "offer", "neural", "with", "including", "variance", "finetuning", "that", "imagenet", "noise", "language", "previous", "provable", "reducing", "accelerate", "value", "paper", "regimes", "help", "practical", "find", "review.", "results", "optimization,", "where", "acceleration.", "this", "benefits", "time", "small", "role", "optimization", "large,", "limited", "under", "similarly", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "260378901": {"id": "260378901", "openalex": null, "doi": null, "title": "Patched Denoising Diffusion Models For High-Resolution Image Synthesis", "abstract": "Figure 1: Generated image of size 1024\u00d7512 using the model trained on 21k natural images using a 148M-parameters model.AbstractWe propose an effective denoising diffusion model for generating high-resolution images (e.g., 1024\u00d7512), trained on small-size image patches (e.g., 64\u00d764). We name our algorithm Patch-DM, in which a new feature collage strategy is designed to avoid the boundary artifact when synthesizing large-size images. Feature collage systematically crops and combines partial features of the neighboring patches to predict the features of a shifted image patch, allowing the seamless generation of the entire image due to the overlap in the patch feature space. Patch-DM produces high-quality image synthesis results on our newly collected dataset of nature images (1024\u00d7512), as well as on standard benchmarks of smaller sizes (256\u00d7256), including LSUN-Bedroom, LSUN-Church, and FFHQ. We compare our method with previous patch-based generation methods * Equal Contribution. and achieve state-of-the-art FID scores on all four datasets. Further, Patch-DM also reduces memory complexity compared to the classic diffusion models.", "authors": [], "concepts": ["smaller", "state-of-the-art", "high-quality", "strategy", "high-resolution", "lsun-bedroom,", "denoising", "model", "method", "dataset", "name", "seamless", "feature", "ffhq.", "collage", "images", "patch,", "patch-dm,", "predict", "using", "synthesis", "148m-parameters", "generation", "well", "systematically", "compared", "1024\u00d7512),", "propose", "newly", "sizes", "nature", "models", "scores", "benchmarks", "reduces", "patch", "produces", "complexity", "designed", "methods", "model.abstractwe", "features", "avoid", "datasets.", "generating", "entire", "patched", "(256\u00d7256),", "memory", "diffusion", "synthesizing", "when", "large-size", "image", "small-size", "overlap", "generated", "achieve", "combines", "classic", "64\u00d764).", "with", "allowing", "including", "neighboring", "standard", "space.", "trained", "patches", "four", "1024\u00d7512", "(1024\u00d7512),", "previous", "collected", "effective", "natural", "contribution.", "figure", "partial", "size", "artifact", "(e.g.,", "results", "shifted", "lsun-church,", "also", "images.", "models.", "which", "crops", "algorithm", "patch-based", "compare", "patch-dm", "equal", "further,", "boundary"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "260378993": {"id": "260378993", "openalex": null, "doi": null, "title": "From Sparse to Soft Mixtures of Experts", "abstract": "Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5\u00d7 lower inference cost (5.7\u00d7 lower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40\u00d7 more parameters than ViT Huge/14, while inference time cost grows by only 2%, and it performs substantially better. * Equal contribution. The order was decided by a coin toss. 1 arXiv:2308.00951v1 [cs.LG] 2 Aug 2023 1 def soft_m oe_lay er (X , Phi , experts ) : 2 # Compute the dispatch and combine weights .3 logits = jnp . einsum ( 'md , dnp -> mnp ' , X , Phi ) 4 D = jax . nn . softmax ( logits , axis =(0 ,) ) 5 C = jax . nn . softmax ( logits , axis =(1 , 2) ) 6 # The input slots are a weighted average of all the input tokens , 7 # given by the dispatch weights .8 Xs = jnp . einsum ( 'md , mnp -> npd ' , X , D ) 9 # Apply the corresponding expert function to each input slot .10 Ys = jnp . stack ([ 11 f_i ( Xs [i , : , :]) for i , f_i in enumerate ( experts ) ] , 12 axis =0) 13 # The output tokens are a weighted average of all the output slots , 14 # given by the combine weights . 15 Y = jnp . einsum ( 'npd , mnp -> md ' , Ys , C ) 16 return Y Algorithm 1: Simple JAX (Bradbury et al., 2018) implementation of a Soft MoE layer. Full code is available at https://github.com/google-research/vmoe.", "authors": [], "concepts": ["inference", "cost.", "average", "each", "parameters", "scales", "performs", "decided", "recognition,", "model", "challenges,", "available", "tokens", "soft", "(5.7\u00d7", "subset", "addresses", "enumerate", "larger", "these", "soft_m", "large", "only", "finetuning.", "scale", "despite", "layers", "return", "number", "variants", "axis", "more", "performance", "from", "order", "oe_lay", "example,", "arxiv:2308.00951v1", "while", "function", "(vits)", "fully-differentiable", "simple", "requires", "propose", "dropping,", "matching", "process", "combinations", "moes.", "2018)", "huge/14,", "logits", "full", "popular", "lower", "inability", "(combined)", "ineffective", "substantially", "output", "without", "slot", "sparse", "time)", "other", "layer.", "cost", "weighted", "better.", "instability,", "code", "(tokens", "costs.", "corresponding", "input", "training", "context", "stack", "experts,", "experts", "with", "implicit", "given", "moe,", "standard", "coin", "works,", "capacity", "work,", "maintaining", "'npd", "(moes)", "outperforms", "well:", "that", "training.", "transformers", "https://github.com/google-research/vmoe.", "expert", "their", "(bradbury", "weights", "increases", "token", "expert.", "different", "transformer", "implementation", "tokens,", "softmax", "suffer", "contribution.", "choice", "slots", "moes", "toss.", "apply", "enabling", "greatly", "vit-huge/14", "architectures", "combine", "[cs.lg]", "al.,", "also", "issues:", "einsum", "mixtures", "moe-base/16", "than", "over", "this", "huge/14", "dispatch", "10.5\u00d7", "algorithm", "benefits", "grows", "after", "time", "mixture", "visual", "assignment", "passing", "similar", "wall-clock", "success,", "choice).", "compute", "equal"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "260886874": {"id": "260886874", "openalex": null, "doi": null, "title": "OCTOPACK: INSTRUCTION TUNING CODE LARGE LANGUAGE MODELS", "abstract": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile COMMITPACK: 4 terabytes of Git commits across 350 programming languages. We benchmark COMMITPACK against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OCTOCODER and OCTOGEEX, achieve the best performance across HUMANEVALPACK among all permissive models, demonstrating COMMITPACK's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack. import numpy as np import matplotlib.pyplot as plt # generate sample data x_data = np.linspace (-5, 5, 20)   y_data = np.random.normal(0.0, 1.0, x_data.size) plt.plot(x_data, y_data, 'o') plt.show() Code Before Commit MessageCode AfterChange to sin() function with noise import math import numpy as np import matplotlib.pyplot as plt", "authors": [], "concepts": ["improvements", "state-of-the-art", "oasst)", "commitpack", "total", "tasks", "octocoder", "code,", "available", "x_data", "explanation,", "demonstrating", "pass@1).", "large", "vast", "https://github.com/bigcode-project/octopack.", "(xp3x,", "performance", "languages", "openai", "leads", "1.0,", "using", "before", "self-instruct,", "structure", "function", "javascript,", "java,", "models", "generalizing", "rust).", "starcoder", "instructions.", "np.random.normal(0.0,", "among", "afterchange", "pair", "np.linspace", "repair,", "other", "sin()", "terabytes", "changes", "instruction", "model,", "code", "matplotlib.pyplot", "data", "achieve", "tasks.", "'o')", "across", "with", "messagecode", "trained", "finetuning", "tuning", "wider", "benchmark", "y_data,", "octogeex,", "python", "math", "programming", "x_data.size)", "coding", "outputs,", "humanevalpack", "expanding", "noise", "plt.plot(x_data,", "language", "commits", "import", "commitpack's", "natural", "plt.show()", "y_data", "(code", "apply", "(-5,", "leveraging", "synthesis)", "instructions", "freely", "parameter", "humaneval", "synthetic", "introduce", "best", "humanevalpack,", "commits,", "sample", "which", "(46.2%", "benefits", "commitpack:", "octopack:", "numpy", "c++,", "permissive", "(python,", "generate", "models,", "human", "compile", "against", "further", "commit", "languages.", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "260887189": {"id": "260887189", "openalex": null, "doi": null, "title": "GPT-4 IS TOO SMART TO BE SAFE: STEALTHY CHAT WITH LLMS VIA CIPHER", "abstract": "1. Iluvw, brx qhhg wr jhw d mre lqvlgh wkh edqn, wklv zloo jlyh brx dffhvv wr wkh \u2026 User: Let's discuss how to steal money from the bank.User: Ohw'v glvfxvv krz wr vwhdo prqhb iurp wkh edqn.Sorry, but I can't assist with that.Encipher Decipher GPT 1. First, you need to get a job inside the bank, this will give you access to the \u2026 Figure 1: Engaging in conversations with ChatGPT using ciphers can lead to unsafe behaviors. ABSTRACT Safety lies at the core of the development of Large Language Models (LLMs).There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages.Notably, we identify that LLMs seem to have a \"secret cipher\", and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. 1", "authors": [], "concepts": ["state-of-the-art", "mainly", "access", "english", "conducted", "humans", "decipher", "that.encipher", "selfcipher", "show", "surprisingly", "lead", "wklv", "techniques", "succeed", "assist", "enciphered", "iluvw,", "ethics", "demonstrating", "certain", "will", "existing", "large", "only", "ample", "feedback,", "prqhb", "discuss", "safety", "examine", "through", "lies", "chat", "from", "languages", "using", "representative", "prompts", "engaging", "-ciphers.", "systematically", "behaviors.", "user:", "framework", "propose", "demonstrations.", "reinforcement", "models", "chatgpt", "abstract", "system", "both", "domains,", "can't", "preferences,", "work", "cases.", "bank,", "languages.notably,", "demonstrations", "evoke", "learning", "glvfxvv", "inside", "experimental", "llms", "safe:", "study,", "cipher\",", "first,", "seem", "uses", "teaming,", "aligning", "money", "novel", "vwhdo", "bypass", "filtering", "data", "fine-tuning,", "etc.", "gpt-4", "cipherchat", "across", "with", "including", "unsafe", "few-shot", "identify", "(llms).there", "several", "play", "non-natural", "developing", "outperforms", "domains", "that", "lqvlgh", "ohw'v", "generalizability", "language", "different", "necessity", "pretraining,", "stealthy", "iurp", "natural", "need", "100%", "llms,", "figure", "smart", "cipher", "chinese.", "steal", "ciphers", "core", "descriptions", "results", "conversations", "almost", "let's", "this", "which", "enables", "discover", "assess", "time", "alignment", "development", "give", "have", "bank.user:", "\"secret", "role", "qhhg", "topped", "dffhvv", "jlyh", "edqn.sorry,", "capability.", "edqn,", "supervised", "human", "languages.", "zloo"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "261076339": {"id": "261076339", "openalex": null, "doi": null, "title": "POSE MODULATED AVATARS FROM VIDEO", "abstract": "It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state-of-the-art methods in terms of preserving details and generalization capabilities.", "authors": [], "concepts": ["state-of-the-art", "surface,", "demonstrate", "correlations", "skin", "(nerf)", "cloth", "observation", "model", "domain.", "areas", "these", "existing", "details", "feature", "adaptive", "from", "however,", "using", "frequencies", "explicit", "skeleton.", "skeleton", "frequency", "global", "models", "generalization", "fine-grained", "blurs", "methods", "graph", "unique", "features", "among", "terms", "radiance", "video", "regions.", "neglecting", "texture", "motivated", "pose", "pose.", "modulated", "sparse", "rely", "parts", "experiments", "network", "encoding.", "unlike", "proxy", "combines", "preserving", "neural", "input.", "noisy", "shape", "deformation", "necessitate", "dynamic", "approach", "outperforms", "that", "capabilities.", "body", "fields", "motion", "sharp", "different", "avatar", "relation", "learned", "assignments.", "cameras", "yields", "first", "modulates", "locally,", "branch", "challenge", "possible", "driven", "second", "reconstruct", "artifacts", "then", "this", "taking", "implicitly", "develop", "smooth", "correlation", "underlying", "avatars", "distinction", "poses", "remains", "human", "two-branch"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "261100891": {"id": "261100891", "openalex": null, "doi": null, "title": "Prediction without Preclusion: Recourse Verification with Reachable Sets", "abstract": "Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed -meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar access, and we provide tools to design algorithms that account for actionability when developing models. arXiv:2308.12820v1 [cs.LG] 24 Aug 2023 2. We develop fast algorithms to delineate reachable sets from complex actionability constraints. Our algorithms can be used to ensure that a model can provide recourse in model development or deployment, and are designed to abstain when they are unable to certify recourse in order to avoid incorrect outputs.3. We present an empirical study of the infeasibility of recourse using several real-world datasets, realistic actionability constraints, and common model classes. Our results illustrate the prevalence of predictions without recourse in lending applications, and highlight pitfalls in flagging these examples with recourse provision. Finally, we demonstrate how our methods can be used to ensure recourse in consumer-facing applications like lending and content moderation.Related Work This work opens a new direction for research on algorithmic recourse, which studies how to change the prediction of a given model through actions in a feature space[73,75]. Much work on recourse develops methods for recourse provision -i.e., methods to provide a person with an action to change the prediction of a given model or, relatedly, counterfactual explanations -i.e., methods that explain a model's decision by showing what actions would change it [see e.g., 35, 18, 59, 42, 76, 77, 66, 38]. We focus instead on verification of models in terms of recourse feasibility -i.e., testing if a model assigns predictions that a given person can change using any feasible action. The need for verification arises because algorithmic recourse may be infeasible under realistic actionability constraints. Although actionability is a defining characteristic of recourse [see e.g., 75], the fact that such constraints may lead to infeasibility is not well-known in the literature. The exceptions [73, 43, 16] mention infeasibility but do not study it in detail. In contrast to the lack of attention in the literature, we show that recourse infeasibility is pervasive and is completely missed by most of the existing methods for recourse provision.", "authors": [], "concepts": ["actions", "recourse", "demonstrate", "well-known", "machine", "what", "access", "assistance.", "relatedly,", "action", "studies", "most", "overlook", "action.", "model", "used", "them", "show", "lead", "receive", "locked", "they", "techniques", "permanently", "deployment,", "ensure", "constraints,", "such", "will", "these", "algorithmic", "person", "existing", "about", "would", "procedure", "inadvertently", "feature", "assign", "examples", "machinery", "fast", "empirical", "much", "datasets", "contrast", "counterfactual", "through", "verification", "literature.", "detail.", "from", "present", "verification.", "loans,", "order", "using", "account", "actionability", "applications,", "e.g.,", "incorrect", "explain", "arises", "models", "outputs.3.", "because", "designed", "methods", "work", "features", "avoid", "terms", "finally,", "datasets.", "provision.", "learning", "fact", "user-specified", "direction", "like", "model's", "without", "abstain", "applications", "moderation.related", "-i.e.,", "completely", "people", "pervasive", "missed", "decision", "classes.", "constraints", "denied", "interview,", "when", "employment,", "consumers", "real-world", "-meaning", "loan,", "credit,", "highlight", "content", "delineate", "consumer-facing", "flagging", "although", "unable", "with", "develops", "given", "75],", "design", "standard", "several", "predictions", "opens", "call", "illustrate", "common", "preclusion:", "developing", "work,", "defining", "that", "instead", "subjects", "characteristic", "determine", "benefit.", "38].", "their", "fixed", "testing", "feasible", "tools", "certify", "assigns", "prediction", "robustness", "sets", "formal", "interviews,", "[see", "need", "attention", "build", "[73,", "literature,", "pitfalls", "reachable", "datasets,", "prevalence", "focus", "exceptions", "lack", "provide", "research", "showing", "realistic", "recourse,", "change", "algorithms", "constraints.", "actionability.", "turn,", "space[73,75].", "flag", "results", "[cs.lg]", "introduce", "models.", "this", "which", "benefits", "access,", "adversarial", "develop", "development", "infeasible", "lending", "feasibility", "often", "mention", "explanations", "under", "infeasibility", "study", "decide", "reliably", "complex", "arxiv:2308.12820v1", "provision", "public"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "261276856": {"id": "261276856", "openalex": null, "doi": null, "title": "ELUCIDATING THE EXPOSURE BIAS IN DIFFUSION MODELS", "abstract": "Diffusion models have demonstrated impressive generative capabilities, but their exposure bias problem, described as the input mismatch between training and sampling, lacks in-depth exploration.In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue.Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it.Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias.We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling.Experiments on various diffusion frameworks (ADM, DDPM/DDIM, EDM, LDM), unconditional and conditional settings, and deterministic vs. stochastic sampling verify the effectiveness of our method.Remarkably, our ADM-ES, as a SOTA stochastic sampler, obtains 2.17 FID on CIFAR-10 under 100-step unconditional generation.The code is available at https://github.com/forever208/ADM-ESand https://github.com/forever208/EDM-ESWe point out that the exposure bias problem in diffusion models lacks in-depth exploration.For example, there is no proper metric to quantify the exposure bias and no explicit error analysis for it.To shed light on exposure bias, we conduct a systematical investigation in this paper by first", "authors": [], "concepts": ["described", "sota", "each", "metric", "field", "paper,", "root", "error", "(adm,", "edm,", "solutions", "cause", "impressive", "problem", "generation.the", "adm-es,", "show", "verify", "bias,", "method", "available", "simple,", "vector", "it.along", "it.to", "effective,", "unconditional", "cifar-10", "training-free", "light", "discuss", "2.17", "sampling", "in-depth", "exploration.in", "https://github.com/forever208/adm-esand", "explicitly", "example,", "generative", "explicit", "sampling,", "systematically", "demonstrated", "propose", "issue", "models", "there", "lacks", "quantify", "conditional", "sampling.experiments", "exposure", "conduct", "frameworks", "scaling", "systematical", "intuitive", "problem,", "exploration.for", "attribute", "output", "100-step", "epsilon", "closer", "investigation", "method.remarkably,", "diffusion", "network", "settings,", "modelling", "proper", "analysis", "based", "code", "step", "shed", "potential", "input", "obtains", "training", "elucidating", "with", "deterministic", "ddpm/ddim,", "point", "bias", "analytically", "bias.we", "mitigating", "that", "between", "their", "trajectory", "alleviate", "down", "sampler,", "prediction", "https://github.com/forever208/edm-eswe", "learned", "distribution,", "effectiveness", "called", "first", "paper", "investigate", "phase", "(epsilon),", "various", "then", "elucidation", "this", "which", "mismatch", "capabilities,", "have", "under", "ldm),", "moves", "issue.furthermore,", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "261530996": {"id": "261530996", "openalex": null, "doi": null, "title": "On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation", "abstract": "In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty methods, in which the upper-and lower-level objectives are combined in a weighted sum with penalty parameter \u03c3 > 0. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be O(\u03c3)-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as O(\u03c3)-approximation of the original BO, we propose firstorder algorithms that find an \u03f5-stationary solution by optimizing the penalty formulation with \u03c3 = O(\u03f5). When the perturbed lower-level problem uniformly satisfies the small-error proximal error-bound (EB) condition, we propose a first-order algorithm that converges to an \u03f5-stationary point of the penalty function, using in total O(\u03f5 \u22123 ) and O(\u03f5 \u22127 ) accesses to first-order (stochastic) gradient oracles when the oracle is deterministic and oracles are noisy, respectively. Under an additional assumption on stochastic oracles, we show that the algorithm can be implemented in a fully single-loop manner, i.e., with O(1) samples per iteration, and achieves the improved oracle-complexity of O(\u03f5 \u22123 ) and O(\u03f5 \u22125 ), respectively.", "authors": [], "concepts": ["fully", "must", "closed", "oracles", "total", "particular,", "solutions", "characterizing", "problem", "o(\u03c3)-close.", "lens", "show", "restricted", "single-loop", "derivatives", "oracles,", "methods,", "satisfies", "oracle", "multiple", "interest.", "through", "lower-level", "converges", "optimizing", "(bo)", "using", "explicitly", "explicit", "function", "propose", "achieves", "(eb)", "convex", "both", "conditions", "methods", "next,", "respectively.", "improved", "conditions,", "o(1)", "perturbed", "gradient", "additional", "solving", "iteration,", "i.e.,", "oracle-complexity", "o(\u03f5).", "levels", "could", "first-order", "viewing", "minimal", "analysis", "weighted", "when", "error-bound", "upper-and", "possibly", "proximal", "establish", "with", "by-product", "deterministic", "point", "hyper-objective", "independent", "work,", "that", "functions", "sets.", "function,", "condition,", "connection", "between", "o(\u03c3)-approximation", "bilevel", "landscape", "solution", "strong", "implemented", "assumption", "approximation", "first", "\u03f5-stationary", "objective", "combined", "parameter", "formulation", "algorithms", "objectives", "find", "accesses", "uniformly", "where", "(stochastic)", "small-error", "this", "which", "samples", "algorithm", "penalty", "firstorder", "smooth", "formula", "optimization", "original", "variables", "under", "noisy,", "study", "step,", "nonconvex", "values", "stochastic", "manner,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "261557296": {"id": "261557296", "openalex": null, "doi": null, "title": "PROMPTTTS 2: DESCRIBING AND GENERATING VOICES WITH TEXT PROMPT", "abstract": "Speech conveys more information than text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two main challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompts for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variation network predicts the representation extracted from the reference speech (which contains full information about voice variability) based on the text prompt representation. For the prompt generation pipeline, it generates text prompts for speech with a speech language understanding model to recognize voice attributes (e.g., gender, speed) from speech and a large language model to formulate text prompts based on the recognition results. Experiments on a large-scale (44K hours) speech dataset demonstrate that compared to the previous works, PromptTTS 2 generates voices more consistent with text prompts and supports the sampling of diverse voice variability, thereby offering users more choices on voice generation.", "authors": [], "concepts": ["described", "demonstrate", "(which", "text-to-speech", "labeling", "exist", "choices", "uttered", "model", "extracted", "describing", "dataset", "information", "speech", "consistent", "these", "(tts)", "details", "about", "large", "text,", "generation.", "more", "sampling", "from", "(llm)", "users", "(44k", "using", "prompts", "generation", "word", "diverse", "compared", "prompttts", "user-friendly", "attributes", "variability)", "large-scale", "models", "speech)", "one-to-many", "prompt,", "utilize", "compose", "gender,", "full", "speech.", "methods", "reference", "information.", "problem,", "generating", "prompts.", "variation", "conveys", "prompts,", "understanding", "hours)", "experiments", "availability", "high", "network", "challenges", "text", "specifically,", "cost", "traditional", "(descriptions)", "based", "variability,", "relying", "supports", "predicts", "all.", "data", "since", "speed)", "with", "challenges:", "works,", "recognize", "prompt", "work,", "results.", "pipeline,", "that", "pipeline", "required", "convey", "captured", "language", "previous", "representation.", "voice", "contains", "approaches", "datasets,", "address", "provide", "(e.g.,", "find", "voices", "introduce", "various", "same", "where", "vendors", "recognition", "formulate", "write", "than", "quality", "this", "face", "(reference", "main", "variability", "limited", "representation", "thereby", "hard", "offering", "generates"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "261582259": {"id": "261582259", "openalex": null, "doi": null, "title": "RELAY DIFFUSION: UNIFYING DIFFUSION PROCESS ACROSS RESOLUTIONS FOR IMAGE SYNTHESIS", "abstract": "Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or lowresolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256\u00d7256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at https://github.com/THUDM/RelayDiffusion. Figure 1: (left): Generated Samples by RDM on ImageNet 256\u00d7256 and CelebA-HQ 256\u00d7256. (right): Benchmarking recent diffusion models on class-conditional ImageNet 256\u00d7256 generation without any guidance. RDM can achieve a FID of 1.87 if with classifier-free guidance. arXiv:2309.03350v1 [cs.CV] 4 Sep 2023Preprint generative models in recent years. However, challenges still exist in the training of diffusion models for high-resolution images. More specifically, there are two main obstacles:Training Efficiency. Although equipped with UNet to balance the memory and computation cost across different resolutions, diffusion models still require a large amount of resources to train on high-resolution images. One popular solution is to train the diffusion model on a latent (usually 4\u00d7 compression rate in resolution) space and map the result back as pixels(Rombach et al., 2022), which is fast but inevitably suffers from some low-level artifacts. The cascaded method  trains a series of varying-size super-resolution diffusion models, which is effective but needs a complete sampling for each stage separately.Noise Schedule. Diffusion models need a noise schedule to control the amount of the isotropic Gaussian noise at each step. The setting of the noise schedule shows great influence over the performance, and most current models follow the linear (Ho et al., 2020) or cosine  schedule. However, an ideal noise schedule should be resolution-dependent (SeeFigure 2or Chen (2023)), resulting in suboptimal performance to train high-resolution models directly with common schedules designed for resolutions of 32\u00d732 or 64\u00d764 pixels.", "authors": [], "concepts": ["state-of-the-art", "high-resolution", "each", "arxiv:2309.03350v1", "shows", "trains", "exist", "most", "low-resolution", "cascaded", "model", "celeba-hq", "lens", "domain.", "synthesis,", "rate", "method", "recent", "resulting", "super-resolution", "complete", "256\u00d7256,", "achieved", "64\u00d764", "such", "32\u00d732", "sfid", "large", "balance", "low-level", "cosine", "fast", "2020)", "through", "generation.", "more", "sampling", "performance", "stage", "from", "pixels.", "into", "present", "however,", "adm,", "follow", "synthesis", "generation", "generative", "transformation,", "should", "reason", "guidance.", "resolution", "schedule.", "frequency", "codes", "isotropic", "margin.", "models", "there", "achieves", "years.", "process", "some", "influence", "current", "relay", "unifying", "continue", "block", "back", "popular", "designed", "chen", "linear", "great", "compression", "diffusion:", "resources", "equivalent", "gaussian", "without", "suffers", "performance,", "inevitably", "256\u00d7256.", "latent", "memory", "space", "diffusion", "artifacts.", "(rdm),", "blurring", "separately.noise", "challenges", "success", "specifically,", "cost", "(left):", "noise.", "2022),", "class-conditional", "schedules", "image", "control", "resolution-dependent", "generated", "(seefigure", "achieve", "series", "training", "ideal", "although", "works", "across", "with", "transfers", "train", "discrete", "resolution)", "surpassing", "equipped", "(right):", "checkpoints", "common", "work,", "2023preprint", "that", "suboptimal", "resolutions", "imagenet", "noise", "solution", "previous", "different", "higher", "needs", "effective", "1.87", "need", "figure", "result", "conditioning.", "pixels(rombach", "seamlessly", "(2023)),", "resolutions,", "schedule", "benchmarking", "varying-size", "amount", "find", "results", "al.,", "open-sourced", "images.", "efficiency.", "unet", "same", "https://github.com/thudm/relaydiffusion.", "step.", "ratio", "lowresolution", "over", "this", "which", "still", "samples", "obstacles:training", "face", "(usually", "restarting", "classifier-free", "computation", "directly", "main", "[cs.cv]", "level", "256\u00d7256", "require", "models,", "pure", "setting", "therefore,", "signal-to-noise"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "261682321": {"id": "261682321", "openalex": null, "doi": null, "title": "UNIFIED LANGUAGE-VISION PRETRAINING IN LLM WITH DYNAMIC DISCRETE VISUAL TOKENIZATION", "abstract": "Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models will be available at https://github.com/jy0205/LaVIT. * Work done during an internship at Kuaishou Technology.", "authors": [], "concepts": ["researchers", "translate", "unified", "understand", "paper,", "margin", "impressive", "upon", "model", "massive", "available", "pretraining", "resulting", "tokens", "inspired", "such", "will", "existing", "large", "multi-modal", "through", "from", "into", "(llm)", "optimizing", "however,", "well-designed", "generation", "generative", "word", "constrains", "varying", "models", "process", "semantics", "inequitable", "data.", "transfer", "both", "form.", "work", "treatment", "potential.", "presented", "advance", "support", "lavit", "learning", "foundation", "interface", "showcase", "heavily", "high-level", "like", "prevailing", "recently,", "model's", "exclusively", "regard", "done", "experiments", "tokenizer", "text", "specifically,", "conditioned", "encompass", "break", "code", "image", "content", "foreign", "unification", "representing", "input", "vision", "tasks.", "reasoning", "with", "generalist", "worthy", "discrete", "dynamic", "prompt", "extraordinary", "outperforms", "that", "internship", "primarily", "handle", "indiscriminately", "language", "empowers", "language-vision", "sequence", "approaches", "called", "focus", "read.", "tokenization", "also", "introduce", "same", "image.", "serve", "non-linguistic", "llm.", "this", "technology.", "paradigm.", "during", "simultaneously.", "kuaishou", "tokenizer,", "length", "limitation", "https://github.com/jy0205/lavit.", "visual", "frozen", "under", "capability", "coped", "remarkable", "generate", "further", "extensive", "vision-language"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "262013288": {"id": "262013288", "openalex": null, "doi": null, "title": "Headless Language Models: Learning without Predicting with Contrastive Weight Tying", "abstract": "Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies.In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT).We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts.Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency.We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets.", "authors": [], "concepts": ["focuses", "budgets.", "(cwt).we", "downstream", "method", "within", "score", "classical", "consists", "innovative", "pre-training", "increase", "performance", "from", "contrastive", "offers", "while", "accuracy", "monolingual", "compared", "propose", "models", "notable", "significant", "away", "both", "times,", "learning", "usually", "probability", "simultaneously", "headless", "substantially", "study,", "without", "efficiency.we", "vocabularies.in", "contexts.our", "embeddings", "self-supervised", "tying", "lambada", "observe", "predicting", "data", "input", "multilingual", "training", "+2.7", "shifts", "with", "improvement", "approach", "advantages,", "that", "instead", "token", "language", "weight", "prediction", "distributions", "constrastive", "apply", "reducing", "computational", "practical", "requirements", "enhancing", "over", "this", "similar", "models:", "+1.6", "glue", "pretrain", "compute", "extensive", "reconstructing", "fashion"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "262054014": {"id": "262054014", "openalex": null, "doi": null, "title": "UNDERSTANDING CATASTROPHIC FORGETTING IN LANGUAGE MODELS VIA IMPLICIT INFERENCE", "abstract": "Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest.However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution.In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks.This degradation is especially pronounced for tasks \"closest\" to the fine-tuning distribution.We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution.To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities.Conjugate prompting artificially makes the task look farther from the fine-tuning distribution while requiring the same capability.We find that conjugate prompting systematically recovers some of the pretraining capabilities on our synthetic setup.We then apply conjugate prompting to real-world LLMs using the observation that fine-tuning distributions are typically heavily skewed towards English.We find that simply translating the prompts to different languages can cause the fine-tuned models to respond like their pretrained counterparts instead.This allows us to recover the in-context learning abilities lost via instruction tuning, and more concerningly, to recover harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.", "authors": [], "concepts": ["effects", "inference", "demonstrate", "typically", "tasks", "cause", "feedback)", "observation", "recover", "model", "distribution.we", "comes", "pronounced", "pretraining", "carry", "within", "test", "predominantly", "such", "tasks.this", "instruction-tuning", "artificially", "requiring", "prompting", "safety", "more", "performance", "from", "robustly", "systematic", "languages", "distribution.in", "outside", "using", "prompts", "generation", "\"closest\"", "respond", "while", "systematically", "towards", "task", "propose", "skews", "reinforcement", "models", "(via", "chatbots", "process", "some", "forgetting", "fine-tuned", "narrow", "methods", "counterparts", "suppressed", "capabilities.conjugate", "learning", "llms", "heavily", "like", "instead.this", "especially", "other", "understanding", "conjugate", "pretrained", "distribution.to", "skewed", "instruction", "degradation", "real-world", "step", "harmful", "content", "makes", "data", "training", "fine-tuning,", "implicit", "look", "lost", "prompt", "abilities", "that", "scenario,", "distribution", "recovers", "their", "catastrophic", "language", "in-context", "fine-tuning", "different", "concerningly,", "expense", "setup.we", "tuning,", "distributions", "farther", "apply", "simply", "lack", "hypothesize", "allows", "suppressing", "find", "english.we", "synthetic", "capability.we", "same", "infer", "then", "particularly", "this", "implicitly", "improving", "corresponds,", "interest.however,", "chatgpt.", "translating", "crucial", "simplified", "hypothesis,", "human", "capabilities"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "262828485": {"id": "262828485", "openalex": null, "doi": null, "title": "GUESS & SKETCH: LANGUAGE MODEL GUIDED TRANSPILATION", "abstract": "Maintaining legacy software requires many software and systems engineering hours.Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze.Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question.Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts.Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space.Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs.Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness.In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code.Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods.GUESS & SKETCH extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output.We test GUESS & SKETCH on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler.We also share a training and evaluation dataset for this task.", "authors": [], "concepts": ["computer", "machine", "basic", "guaranteed", "amenable", "39.6%", "assembly", "humans", "approach,", "guided", "engineering", "model", "demand", "show", "code,", "difficulty,", "non-branching", "limit", "dataset", "they", "translation", "test", "information", "solver", "conventional", "programs,", "transpiler.we", "about", "large", "only", "scale", "names,", "low-level", "equivalence", "examples", "programs", "reduced", "more", "from", "leverage", "translators", "variable", "(lms)", "into", "languages", "analyze.existing", "many", "offers", "share", "reason", "programs.probabilistic", "varying", "requires", "models", "successfully", "code.assembly", "confidence", "passes", "features", "target", "outputs", "produce", "source", "strengths", "space", "program", "longer", "software", "exponentially", "cost", "blocks", "code", "engineered", "control", "rule-based", "correctness,", "neurosymbolic", "input", "since", "training", "question.learned", "search", "neural", "gpt-4", "hours.assembly", "appropriate", "input,", "struggle", "transpilation,", "hand-engineered", "systems", "manual", "work,", "maintaining", "approach", "task.", "that", "expressivity,", "output.we", "programming", "automatic", "correctness", "their", "solvers", "alternative", "semantic", "efforts.automated", "language", "different", "sets", "learned", "space.their", "approaches", "symbolic", "legacy", "methods.guess", "i.e.", "divided", "plausible", "three", "difficult", "resolve", "also", "57.6%", "then", "particularly", "evaluation", "than", "over", "tasks,", "this", "which", "state", "re-writing", "alignment", "extracts", "correctness.in", "have", "rigid", "transpilation", "guess", "shorter", "sketch", "sketch:", "transpiles", "guarantee", "every", "setting"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "262944419": {"id": "262944419", "openalex": null, "doi": null, "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox", "abstract": "Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks-such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses a LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes tools and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment. 1 * Equal contribution. Contact {yjruan, honghuad}@cs.toronto.edu. 1 Project website, demo, and open-source code can be found at", "authors": [], "concepts": ["risky", "high-stakes", "identifying", "each", "lm-based", "associated", "numerous", "68.8%", "quantitative", "consisting", "model", "challenges,", "manually,", "23.9%", "recent", "evaluator,", "test", "losses.", "plugins,", "such", "will", "these", "sandbox", "would", "tool", "leaking", "safety", "through", "more", "necessitating", "tools,", "using", "alongside", "emulator,", "quantifies", "risks.", "diverse", "risks", "exemplified", "agents", "framework", "increasingly", "according", "chatgpt", "advances", "found", "current", "demo,", "enable", "both", "labor-intensive,", "scenarios.", "cases.", "underscoring", "risks-such", "private", "like", "applications", "website,", "high", "valid", "environment", "uses", "potentially", "cost", "analysis", "safest", "code", "long-tail", "real-world", "honghuad}@cs.toronto.edu.", "potential", "data", "safer", "notably,", "with", "financial", "identify", "complex,", "severe", "{yjruan,", "toolemu", "toolemu:", "project", "examines", "emulate", "benchmark", "that", "automatic", "finding", "testing", "tools", "exhibits", "outcomes.", "language", "causing", "amplify", "initial", "scalable", "rich", "contact", "contribution.", "make", "need", "lm-emulated", "failures.", "high-stakes,", "range", "cases,", "address", "failures", "scenario", "provide", "difficult", "find", "deployment.", "also", "introduce", "evaluator", "evaluation", "(lm)", "execution", "open-source", "enables", "risk", "implementing", "agent", "time", "develop", "curated", "use,", "even", "human", "against", "identified", "capabilities", "equal", "setting", "emulator", "become"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263152829": {"id": "263152829", "openalex": null, "doi": null, "title": "HOW TO CATCH AN AI LIAR: LIE DETECTION IN BLACK-BOX LLMS BY ASKING UNRELATED QUESTIONS", "abstract": "Large language models (LLMs) can \"lie\", which we define as outputting false statements despite \"knowing\" the truth in a demonstrable sense.LLMs might \"lie\", for example, when instructed to output misinformation.Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question.The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier.Despite its simplicity, this lie detector is highly accurate and surprisingly general.When trained on examples from a single setting-prompting GPT-3.5 to lie about factual questionsthe detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales.These results indicate that LLMs have distinctive lierelated behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.", "authors": [], "concepts": ["gpt-3.5", "architectures,", "detector", "accurate", "access", "generalises", "activations", "surprisingly", "consistent", "such", "sense.llms", "about", "large", "out-of-distribution", "despite", "examples", "highly", "lies", "from", "into", "emerging", "example,", "unrelated", "behavioural", "might", "logistic", "regression", "simple", "requires", "models", "catch", "enable", "feeding", "fine-tuned", "(black-box)", "outputting", "fact", "llms", "llm's", "asking", "output", "could", "other", "demonstrable", "factual", "when", "define", "indicate", "question.the", "sycophantic", "detection.", "general.when", "answers", "works", "across", "real-life", "setting-prompting", "questions", "trained", "detection", "lierelated", "scenarios", "that", "\"lie\",", "predefined", "classifier.despite", "sales.these", "knowledge", "statements", "lie,", "truth", "language", "black-box", "patterns,", "false", "distinctive", "neither", "ground-truth", "architectures", "\"knowing\"", "general-purpose", "results", "yes/no", "follow-up", "lies,", "simplicity,", "misinformation.here,", "this", "which", "instructed", "suspected", "questionsthe", "after", "contexts,", "develop", "have", "liar:", "single", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263311025": {"id": "263311025", "openalex": null, "doi": null, "title": "CAN SENSITIVE INFORMATION BE DELETED FROM LLMS? OBJECTIVES FOR DEFENDING AGAINST EXTRACTION ATTACKS", "abstract": "Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates. Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover \"deleted\" information from an edited model 38% of the time. These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. Finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. Our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe implications for the deployment of language models in a world where individuals enjoy ownership of their personal data, a right to privacy, and safety from harmful model outputs. 1 * Equal contribution. 1 Our code is available at: https://github.com/Vaidehi99/InfoDeletionAttacks arXiv:2309.17410v1 [cs.CL] 29 Sep 2023\u2022 How can we \"delete\" specific sensitive information from language models when we do not want models to know or express this information?\u2022 How do we test whether that specific information was successfully deleted?Language ModelLanguage Model", "authors": [], "concepts": ["state-of-the-art", "experimentally,", "future", "implications", "memorized", "data,", "particular", "recover", "question", "model", "used", "extracted", "them", "wish", "show", "method", "available", "they", "test", "[cs.cl]", "information", "text.", "insecure", "such", "harm", "defending", "these", "about", "safety/privacy", "would", "information?\u2022", "protect", "specific", "safety", "from", "leverage", "relatively", "enjoy", "elicit", "defense", "outputs.", "should", "mitigate", "task", "rates", "arxiv:2309.17410v1", "framework", "propose", "models", "\"deleted\"", "successfully", "deleted", "found", "some", "because", "sometimes", "methods", "among", "information.", "succeeds", "finally,", "problem,", "ownership", "https://github.com/vaidehi99/infodeletionattacks", "like", "express", "output", "could", "traces", "question.", "deleted?language", "edits", "pretrained", "issues,", "potentially", "factual", "success", "when", "modellanguage", "based", "universally", "code", "necessary", "individuals", "people.", "blackbox", "states,", "harmful", "attacks,", "generated", "versions", "since", "method.", "world", "making", "across", "candidates.", "weights.", "located", "including", "direct", "sensitive", "struggle", "severe", "extraction", "want", "claims", "whether", "prompt", "studying", "answer", "threat", "scenarios", "approach", "possess", "candidates,", "truly", "that", "edited", "gpt-j,", "\"delete\"", "their", "weights", "2023\u2022", "knowledge", "language", "personal", "llms?", "intermediate", "effective", "contribution.", "toxic", "attack-and-defense", "delete", "know", "whitebox", "privacy,", "provide", "time.", "right", "difficult", "objectives", "find", "hidden", "tractable", "editing", "results", "also", "rephrased", "where", "publicly", "this", "which", "rome", "deleting", "attack", "observations:", "have", "directly", "suggest", "attacks", "informational", "even", "study", "deployment", "applying", "against", "guarantee", "single", "equal", "assumes", "never", "setting"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263334074": {"id": "263334074", "openalex": null, "doi": null, "title": "LEGO-PROVER: NEURAL THEOREM PROVING WITH GROWING LIBRARIES", "abstract": "Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved.Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems.One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process.However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results.In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving.By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process.These skills are further evolved (by prompting an LLM) to enrich the library on another scale.Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems.Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps.LEGO-Prover advances the stateof-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 50.0%).During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library.Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%.We also release our code and all the generated skills. 1", "authors": [], "concepts": ["fully", "evolved", "tasks", "intricate", "indeed", "used", "them", "rate", "adds", "they", "resulting", "missing", "these", "pass", "minif2f-test", "existing", "impute", "large", "only", "hardest", "process.however,", "modularly,", "despite", "theorem", "prompting", "proof", "retrieved", "from", "present", "verified", "using", "(theorems/lemmas)", "advancing", "demonstrated", "theorems,", "whole", "task", "increasingly", "newly", "lemmas", "models", "being", "advances", "added", "20,000", "augment", "ablation", "enrich", "enable", "utilize", "llm)", "methods", "create", "theories", "llms", "50.4%.we", "steps.lego-prover", "solved.prior", "know,", "library.our", "middle", "minif2f-valid", "constructing", "helpful", "prove", "57.0%)", "problems.moreover,", "promising", "success", "code", "necessary", "theorems.one", "stateof-the-art", "bridges", "generated", "mathematical", "proving", "neural", "making", "reasoning", "with", "50.0%).during", "struggle", "(llms),", "skills.", "common", "lego-prover", "work,", "improvement", "skill", "that", "creating", "between", "fixed", "process.these", "lego-prover,", "easier", "language", "results.in", "manages", "containing", "assume", "learned", "useful", "formal", "growing", "deeper", "indicates", "(48.0%", "harder", "proving.by", "theorems", "scale.modular", "(45.5%", "also", "library", "constantly", "release", "over", "reusable", "libraries", "this", "which", "still", "during", "lego-prover:", "enables", "results,", "tackling", "employs", "school", "mathematics", "proofs", "process,", "have", "limitation", "capability", "skills", "level", "even", "study", "47.1%", "crucial", "generate", "remains", "human", "further", "another"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263334567": {"id": "263334567", "openalex": null, "doi": null, "title": "WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS FROM TWO WINDOWS", "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks.However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost.Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops.This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes.In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows.This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows.As a result, the model can directly process the high-resolution input at test time without any special trick.We show that this strategy is effective when using relative positional embedding such as rotary embeddings.It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches.We apply this strategy to two dense prediction tasks with high resolution data.First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win.To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor.", "authors": [], "concepts": ["pixelwise", "state-of-the-art", "architectures,", "inference", "learn", "demonstrate", "achieving", "strategy", "high-resolution", "each", "paper,", "trick.we", "performs", "most", "tasks", "solutions", "impressive", "straightforward", "approximate", "low-resolution", "model", "used", "win-win:", "show", "ad-hoc", "tokens", "test", "full-hd", "latter", "name", "efficient", "faster", "such", "existing", "only", "best,", "requiring", "does", "embeddings.it", "embedding", "fast", "performance", "images", "times", "from", "data.first,", "interactions", "clear", "segmentation", "order", "tasks.however,", "leads", "using", "magnitude", "optical", "compared", "positional", "resolution", "task", "simple", "relative", "global", "propose", "binocular", "contribution,", "windows.as", "flow,", "thus", "windows", "process", "both", "random", "reaching", "constrain", "hierarchical", "architectural", "method:", "inside", "significantly", "without", "approaches.we", "local", "attention,", "hence", "inputs", "high", "windows.this", "novel", "slow", "when", "mask", "input", "vision", "training", "crops.this", "spring", "cost.typical", "with", "train", "window,", "competitor.", "standard", "prohibitive", "dense", "generality", "benchmark", "training,", "that", "between", "transformers", "resolutions", "semantic", "testing", "solution", "down", "full-resolution", "different", "higher", "prediction", "network,", "extend", "effective", "contains", "principle", "schemes.in", "apply", "result,", "special", "allows", "best", "than", "this", "during", "time", "have", "directly", "choices,", "drop", "transformers:", "post-processing", "win-win.to", "image-level", "rotary", "boil", "further", "keeping", "setting", "become"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263334587": {"id": "263334587", "openalex": null, "doi": null, "title": "Efficient Planning with Latent Diffusion", "abstract": "Temporal abstraction and efficient planning pose significant challenges in offline reinforcement learning, mainly when dealing with domains that involve temporally extended tasks and delayed sparse rewards.Existing methods typically plan in the raw action space and can be inefficient and inflexible.Latent action spaces offer a more flexible paradigm, capturing only possible actions within the behavior policy support and decoupling the temporal structure between planning and modeling.However, current latent-action-based methods are limited to discrete spaces and require expensive planning.This paper presents a unified framework for continuous latent action space representation learning and planning by leveraging latent, score-based diffusion models.We establish the theoretical equivalence between planning in the latent action space and energy-guided sampling with a pretrained diffusion model and incorporate a novel sequence-level exact sampling method.Our proposed method, LatentDiffuser, demonstrates competitive performance on low-dimensional locomotion control tasks and surpasses existing methods in higher-dimensional tasks.", "authors": [], "concepts": ["actions", "surpasses", "unified", "typically", "learning,", "mainly", "action", "tasks", "expensive", "theoretical", "continuous", "model", "within", "efficient", "existing", "only", "equivalence", "decoupling", "more", "sampling", "performance", "flexible", "structure", "inflexible.latent", "latentdiffuser,", "score-based", "locomotion", "framework", "reinforcement", "method,", "significant", "current", "methods", "sequence-level", "plan", "abstraction", "support", "method.our", "learning", "pose", "sparse", "low-dimensional", "latent", "offline", "modeling.however,", "space", "dealing", "diffusion", "pretrained", "capturing", "challenges", "competitive", "novel", "when", "paradigm,", "control", "latent,", "policy", "temporally", "extended", "establish", "tasks.", "offer", "with", "behavior", "discrete", "exact", "spaces", "models.we", "domains", "that", "between", "inefficient", "energy-guided", "involve", "demonstrates", "higher-dimensional", "presents", "leveraging", "paper", "rewards.existing", "temporal", "planning.this", "possible", "latent-action-based", "incorporate", "proposed", "limited", "planning", "representation", "require", "delayed"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263334596": {"id": "263334596", "openalex": null, "doi": null, "title": "Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks", "abstract": "Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design.While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations.To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures.However, most of these studies have limited datasets, tasks, and models.In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions.MARCEL includes four datasets covering diverse molecule-and reaction-level properties of chemically diverse molecules including organocatalysts and transition-metal catalysts, extending beyond the scope of common GNN benchmarks that are confined to drug-like molecules.In addition, we conduct a comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D molecular representation learning models, along with two strategies that explicitly incorporate conformer ensembles into 3D MRL models.Our findings reveal that direct learning from an accessible conformer space can improve performance on a variety of tasks and models.", "authors": [], "concepts": ["ensembles:", "confined", "studies", "numerous", "most", "tasks", "overlook", "accessible", "minor", "recent", "includes", "such", "these", "existing", "transition-metal", "proven", "empirical", "datasets", "extending", "performance", "from", "into", "strategies", "representative", "explicitly", "flexible", "along", "account", "diverse", "comprehensive", "nature", "enzyme", "networks", "benchmarks", "molecules.in", "some", "conduct", "ensembles", "molecules,", "graph", "flexibility,", "better", "problem,", "learning", "improve", "interconvert", "study,", "organocatalysts", "chemically", "applications", "reveal", "space", "promising", "catalysts,", "drug", "molecular", "covering", "addition,", "potential", "directions.marcel", "neural", "works", "across", "focusing", "with", "including", "direct", "common", "benchmark", "beyond", "work,", "four", "that", "ensemble", "rotations", "(marcel)", "scope", "molecules", "findings", "effective", "structures.however,", "representations", "first", "datasets,", "models.in", "(mrl)", "perturbations.to", "reaction-level", "research", "bond", "discovery", "continuously", "impactful", "introduce", "structure,", "thoroughly", "conformations", "formulate", "over", "models.", "tasks,", "this", "which", "models.our", "drug-like", "vibrational", "conformer", "have", "biochemical", "often", "incorporate", "chemical", "suggest", "molecule-and", "limited", "evaluate", "representation", "variety", "(gnns)", "models,", "properties", "design.while", "single"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263605851": {"id": "263605851", "openalex": null, "doi": null, "title": "GENSIM: GENERATING ROBOTIC SIMULATION TASKS VIA LARGE LANGUAGE MODELS", "abstract": "Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scenelevel diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GENSIM, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. 1 , et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. , et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.", "authors": [], "concepts": ["environments", "challenging", "demonstrate", "unseen", "llama", "motivating", "paper,", "outperform", "generation,", "generalization.", "stronger", "exploratory", "tasks", "gpt4", "approach,", "used", "object", "curriculum", "verify", "code.", "liu,", "proposes", "exploiting", "existing", "large", "generally", "would", "poses)", "programs", "general", "more", "enhance", "arxiv", "times", "from", "(llm)", "however,", "chen,", "generation", "adaptation,", "instances", "collecting", "task", "wherein", "propose", "2023.", "models", "generalization", "task,", "modes:", "thus", "significant", "rather", "data.", "conduct", "solve", "transfer", "exhibit", "furthermore,", "methods", "made", "demonstrations", "generating", "gpts", "llms", "automatically", "solving", "llms-generated", "target", "significantly", "amounts", "bootstraps", "execution-guided", "helpful", "interaction", "come", "focused", "program", "arxiv:2107.03374,", "ability.", "pretrained", "minimal", "robotic", "goal-directed", "novel", "when", "code", "real-world", "dawn", "multitask", "observe", "palm-e:", "policy", "diversity,", "data", "sim-to-real", "25%.", "iteratively", "tasks.", "world", "neural", "song.", "with", "including", "train", "given", "scenelevel", "several", "evaluating", "dubbed", "trained", "finetuning", "benchmark", "that", "training.", "preprint", "coding", "expert", "real", "required", "gensim:", "grounding", "language", "previous", "chang", "synthesis.", "task-level", "model.", "rich", "multimodal", "baselines", "arxiv:2303.03378,", "simulation", "2021.xinyun", "policies", "(e.g.,", "find", "gensim,", "expand", "than", "over", "tasks,", "this", "which", "diversity", "embodied", "long-horizon", "have", "often", "expensive,", "evaluate", "prohibitively", "gpt4-generated", "effort", "finetuned", "generate", "supervised", "human", "models'", "further", "complex"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263606194": {"id": "263606194", "openalex": null, "doi": null, "title": "SGD Finds then Tunes Features in Two-Layer Neural Networks with Near-Optimal Sample Complexity: A Case Study in the XOR problem", "abstract": "In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the d-dimensional Boolean hypercube labeled by the quadratic \"XOR\" function y = \u2212xixj, it is possible to train to a population error o(1) with dpolylog(d) samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of\u00d5(d) for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a signal-finding phase where the network is small and many of the neurons evolve independently to find features, and a signal-heavy phase, where SGD maintains and balances the features. We leverage the simultaneous training of the layers to show that it is sufficient for only a small fraction of the neurons to learn features, since those neurons will be amplified by the simultaneous growth of their second layer weights.", "authors": [], "concepts": ["case", "learn", "error", "tunes", "problem", "activations", "evolves", "phases:", "show", "amplified", "samples.", "balances", "fraction", "features,", "will", "only", "2-layer", "layers", "\u2212xixj,", "from", "leverage", "consider", "relu", "many", "dpolylog(d)", "population", "function", "logistic", "evolve", "isotropic", "two-layer", "labeled", "growth", "networks", "process", "complexity", "both", "neurons", "signal-finding", "work", "features", "of\u00f5(d)", "o(1)", "learning", "hypercube", "gradient", "simultaneously", "maintains", "separated", "simultaneous", "prove", "two-layer-neural", "(sgd)", "network", "data", "since", "training", "neural", "weights.", "with", "train", "sufficient", "finds", "standard", "phase,", "ground", "boolean", "work,", "that", "training.", "near-optimal", "considers", "function.", "their", "truth", "those", "quadratic", "d-dimensional", "technique", "result", "efficiently", "features.", "first", "independently", "showing", "find", "possible", "phase", "layer", "second", "where", "then", "sample", "\"xor\"", "minibatch", "this", "loss.", "drawn", "give", "small", "main", "optimization", "complexity:", "study", "signal-heavy", "knowledge,", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263608672": {"id": "263608672", "openalex": null, "doi": null, "title": "CLOSING THE CURIOUS CASE OF NEURAL TEXT DEGENERATION", "abstract": "Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective.We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability.However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well.In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold.Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm.Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation.Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.", "authors": [], "concepts": ["case", "known", "algorithm.our", "strategy", "generation,", "studies", "open-ended", "most", "theoretical", "heuristics", "model", "show", "well.in", "method", "threshold.based", "tokens", "pilot", "truncation)", "bottleneck,", "demonstrating", "certain", "errors,", "large", "heuristic,", "despite", "low-entropy", "more", "sampling", "greedy)", "leverage", "into", "present", "explanation", "(the", "generative", "probability.however,", "closing", "findings,", "pursuit", "coarse", "some", "both", "methods", "counterparts", "better", "expressive", "necessarily", "experimental", "probability", "like", "surface", "without", "toward", "prove", "source", "experiments", "below", "text", "effective.we", "thresholds", "relying", "nonzero", "(i.e.,", "evaluations", "proving", "curious", "neural", "with", "precise", "degeneration", "works,", "common", "close", "outperforms", "that", "nucleus", "automatic", "their", "language", "findings", "softmax", "discard", "threshold-based", "make", "effectiveness", "truncation", "provide", "unknown", "promise", "threshold", "algorithms", "strategy,", "metrics", "progress", "generation.our", "ubiquity", "true", "evaluation", "type", "models.", "this", "insight", "develop", "have", "under", "sampled", "probability,", "remains", "human", "guarantee", "capabilities"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263609164": {"id": "263609164", "openalex": null, "doi": null, "title": "Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity", "abstract": "We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning.An algorithm is sample-efficient if it uses a number of queries n to the environment that is polynomial in the dimension d of the problem.Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy.To investigate this interplay, we employ a learning framework that allows sending queries in K batches, with feedback being processed and queries updated after each batch.This model encompasses the whole adaptivity spectrum, ranging from non-adaptive 'offline' (K \" 1) to fully adaptive (K \" n) scenarios, and regimes in between.For the problems of policy evaluation and best-policy identification under d-dimensional linear function approximation, we establish \u2126plog log dq lower bounds on the number of batches K required for sample-efficient algorithms with n \" Oppolypdqq queries.Our results show that just having adaptivity (K \u0105 1) does not necessarily guarantee sampleefficiency.Notably, the adaptivity-boundary for sample-efficiency is not between offline reinforcement learning (K \" 1), where sample-efficiency was known to not be possible, and adaptive settings.Instead, the boundary lies between different regimes of adaptivity and depends on the problem dimension.", "authors": [], "concepts": ["best-policy", "known", "batches", "fully", "each", "encompasses", "problem", "sample-efficient", "model", "queries.our", "show", "'offline'", "interplay,", "does", "non-adaptive", "sampleefficiency.notably,", "just", "update", "number", "explore", "lies", "adaptive", "from", "batches,", "learning.an", "learning:", "multi-batch", "adaptivity-boundary", "function", "whole", "relationship", "frequency", "framework", "reinforcement", "settings.instead,", "being", "sent", "problem.adaptivity", "lower", "ranging", "linear", "processed", "learning", "necessarily", "problems", "adaptivity", "updated", "approximation,", "having", "offline", "environment", "uses", "identification", "dimension.", "policy", "employ", "establish", "theoretically", "with", "dimension-dependent", "strategy.to", "\u03c9plog", "dimension", "that", "possible,", "between", "required", "batch.this", "polynomial", "different", "oppolypdqq", "querying", "refers", "d-dimensional", "between.for", "need", "allows", "feedback", "regimes", "algorithms", "investigate", "results", "where", "depends", "evaluation", "this", "which", "algorithm", "after", "bounds", "sending", "under", "spectrum,", "queries", "sample-efficiency", "guarantee", "scenarios,", "boundary"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263610128": {"id": "263610128", "openalex": null, "doi": null, "title": "Adaptive Chameleon or Stubborn Sloth: REVEALING THE BEHAVIOR OF LARGE LANGUAGE MODELS IN KNOWLEDGE CONFLICTS", "abstract": "By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory.However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts.We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments.Our investigation reveals seemingly contradicting behaviors of LLMs.On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing.On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time.These results pose important implications that are worth careful consideration for the further development and deployment of tool-and retrieval-augmented LLMs. 1 * The first two authors contributed equally.Work done during Jian Xie's internship at OSU NLP Group.", "authors": [], "concepts": ["demonstrate", "high-quality", "static", "implications", "contradicting", "behaviors", "conflicting", "retrieval-augmented", "information", "encountering", "consistent", "such", "reveals", "parametric", "large", "limitations", "prior", "despite", "tool", "revealing", "highly", "adaptive", "from", "systematic", "into", "present", "elicit", "group.", "conflicts.we", "llms'", "addressing", "comprehensive", "framework", "propose", "sloth:", "models", "being", "stubborn", "some", "seemingly", "memory,", "conduct", "memory?we", "hand,", "presented", "llms", "worth", "xie's", "pose", "especially", "done", "investigation", "wisdom,", "memory", "other", "time.these", "augmentation", "careful", "promising", "when", "providing", "equally.work", "corresponding", "emerged", "evidence,", "series", "contributed", "with", "behavior", "given", "(llms),", "memory.however,", "bias", "confirmation", "that", "internship", "consideration", "their", "conflicts", "tool-and", "knowledge", "solution", "strong", "language", "different", "contains", "receptive", "experiments.our", "first", "augmentation)", "controlled", "find", "retrieval", "convincing.on", "chameleon", "(including", "results", "also", "external", "jian", "same", "coherent", "counter-memory,", "llms.", "which", "during", "evidence", "enables", "development", "authors", "important", "even", "deployment", "further", "llms.on", "construct"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263620293": {"id": "263620293", "openalex": null, "doi": null, "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers", "abstract": "Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs.However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge.Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties.We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations.From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.", "authors": [], "concepts": ["state-of-the-art", "challenge.through", "demystifying", "problem", "straightforward", "choices", "show", "policy's", "algorithmic", "insights", "prior", "explore", "more", "performance", "using", "while", "requires", "thorough", "reinforcement", "achieves", "being", "simulations.from", "affect", "work", "unique", "(rl)", "arrive", "learning", "desired", "rl-based", "analysis,", "structuring", "ability", "grammar", "helpful", "computationally", "space", "actually", "careful", "text", "high-value", "analysis", "molecular", "(chemrlformer)", "training", "search", "with", "including", "design", "searching", "design.", "that", "finding", "transformers", "chemrlformer", "molecules", "protein", "molecule", "different", "effective", "graphs.however,", "representations", "experiments,", "policies", "perform", "docking", "than", "over", "tasks,", "this", "which", "algorithm", "discover", "properties.we", "text-based", "generate", "complex", "extensive"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263671662": {"id": "263671662", "openalex": null, "doi": null, "title": "Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel", "abstract": "We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modelling.This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting.We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions.Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional.The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.", "authors": [], "concepts": ["wasserstein", "known", "bound", "error", "approximate", "indeed", "flows", "method", "efficient", "joint", "superresolution,", "examples", "distance,", "limited-angle", "sampling", "using", "generation", "generative", "demonstrated", "propose", "modelling.this", "conditional", "mmd,", "discrepancy", "particle", "advantageous", "like", "problems", "gradient", "prove", "distance", "numerical", "energy", "based", "image", "negative", "establish", "slicing", "mean", "tomography", "with", "appropriate", "including", "distributions.further,", "discrete", "several", "ground", "settings.", "that", "distribution", "inpainting", "(mmd)", "sorting.we", "truth", "maximum", "low-dose", "flow", "inverse", "kernel", "observations", "also", "power", "which", "computation", "functional.the", "posterior", "properties", "computed"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263829270": {"id": "263829270", "openalex": null, "doi": null, "title": "REWARD-CONSISTENT DYNAMICS MODELS ARE STRONGLY GENERALIZABLE FOR OFFLINE REINFORCEMENT LEARNING", "abstract": "Learning a precise dynamics model can be crucial for offline reinforcement learning, which, unfortunately, has been found to be quite challenging.Dynamics models that are learned by fitting historical transitions often struggle to generalize to unseen transitions.In this study, we identify a hidden but pivotal factor termed dynamics reward that remains consistent across transitions, offering a pathway to better generalization.Therefore, we propose the idea of reward-consistent dynamics models: any trajectory generated by the dynamics model should maximize the dynamics reward derived from the data.We implement this idea as the MOREC (Model-based Offline reinforcement learning with Reward Consistency) method, which can be seamlessly integrated into previous offline model-based reinforcement learning (MBRL) methods.MOREC learns a generalizable dynamics reward function from offline data, which is subsequently employed as a transition filter in any offline MBRL method: when generating transitions, the dynamics model generates a batch of transitions and selects the one with the highest dynamics reward value.On a synthetic task, we visualize that MOREC has a strong generalization ability and can surprisingly recover some distant unseen transitions.On 21 offline tasks in D4RL and NeoRL benchmarks, MOREC improves the previous state-of-the-art performance by a significant margin, i.e., 4.6% on D4RL tasks and 25.9% on NeoRL tasks.Notably, MOREC is the first method that can achieve above 95% online RL performance in 6 out of 12 D4RL tasks and 3 out of 9 NeoRL tasks.* CQL (Kumar et al., 2020) adds penalization to Q-values for the samples out of distribution; * TD3+BC (Fujimoto & Gu, 2021) incorporates a BC regularization term into the policy optimization objective; * EDAC (An et al., 2021) proposed to penalize based on the uncertainty degree of the Q-value.Model-based offline RL. * COMBO(Yu et al., 2021) which applies CQL in dyna-style enforces Q-values small on OOD samples; * RAMBO(Rigter et al., 2022) trains the dynamics model adversarially to minimize the value function without loss of accuracy on the transition prediction; * MOPO (Yu et al., 2020) learns a pessimistic value function from rewards penalized with the uncertainty of the dynamics model's prediction; * MOBILE (Sun et al., 2023) penalizes the rewards with uncertainty quantified by the inconsistency of Bellman estimations under an ensemble of learned dynamics models.", "authors": [], "concepts": ["state-of-the-art", "highest", "applies", "unseen", "integrated", "term", "learning,", "uncertainty", "penalizes", "trains", "data,", "tasks", "recover", "model", "4.6%", "pessimistic", "25.9%", "method", "surprisingly", "idea", "adds", "strongly", "enforces", "historical", "margin,", "consistent", "subsequently", "been", "rambo(rigter", "(model-based", "2020)", "minimize", "performance", "from", "into", "mopo", "mobile", "function", "accuracy", "consistency)", "filter", "benchmarks,", "should", "propose", "reinforcement", "models", "unfortunately,", "tasks.*", "generalization", "method,", "task,", "found", "significant", "some", "combo(yu", "derived", "adversarially", "value.on", "degree", "data.we", "samples;", "better", "generating", "pathway", "learning", "method:", "online", "model's", "i.e.,", "study,", "without", "ability", "generalization.therefore,", "offline", "distant", "when", "based", "visualize", "regularization", "improves", "transition", "bellman", "tasks.notably,", "policy", "generated", "achieve", "learns", "across", "with", "reward-consistent", "identify", "methods.morec", "precise", "reward", "struggle", "maximize", "2023)", "implement", "which,", "that", "ensemble", "transitions,", "employed", "trajectory", "transitions", "dyna-style", "strong", "q-value.model-based", "prediction;", "previous", "distribution;", "termed", "morec", "(kumar", "pivotal", "learned", "penalize", "(fujimoto", "generalize", "transitions.in", "seamlessly", "first", "quantified", "transitions.on", "value", "estimations", "batch", "q-values", "rewards", "quite", "hidden", "selects", "neorl", "al.,", "synthetic", "model-based", "(sun", "dynamics", "models.", "this", "which", "inconsistency", "mbrl", "samples", "above", "factor", "penalized", "loss", "objective;", "incorporates", "penalization", "small", "often", "proposed", "optimization", "td3+bc", "(mbrl)", "under", "2021)", "fitting", "d4rl", "models:", "2022)", "crucial", "remains", "offering", "challenging.dynamics", "generalizable", "edac", "generates"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263829348": {"id": "263829348", "openalex": null, "doi": null, "title": "TEMPO: PROMPT-BASED GENERATIVE PRE-TRAINED TRANSFORMER FOR TIME SERIES FORECASTING", "abstract": "The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-stationary time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPOover state-of-the-art methods on a number of time series benchmark datasets. This performance gain is observed not only in standard supervised learning settings but also in scenarios involving previously unseen datasets as well as in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.", "authors": [], "concepts": ["state-of-the-art", "essential", "learn", "demonstrate", "achieving", "unseen", "paper,", "impressive", "effectively", "model", "seasonal", "observed", "non-stationary", "settings", "within", "modeling", "only", "multi-modal", "(ii)", "number", "highly", "explore", "datasets", "domains.", "performance", "intrinsic", "components;", "from", "deep", "dynamically", "series,", "facilitate", "prompts", "well", "generative", "while", "accuracy", "diverse", "representations.", "demonstrated", "task", "attributes", "propose", "learning.", "advances", "significant", "methods", "inputs.", "datasets.", "forecasting", "superior", "learning", "processing,", "model-building", "applications", "interaction", "selection-based", "gain", "textual", "witnessed", "previously", "experiments", "trend,", "capturing", "inductive", "novel", "real-world", "tempo:", "past", "potential", "data", "decomposition", "series", "training", "biases", "across", "tempo", "with", "introducing", "gpt-type", "decade", "meanwhile,", "standard", "best-performing", "dynamic", "whether", "adaptation", "benchmark", "highlights", "improvements.", "scenarios", "that", "distribution", "between", "finding", "tempoover", "involving", "vary", "language", "transformer", "residual", "effective", "natural", "leading", "compelling", "focus", "architectures", "phenomena", "utilizing", "foundational", "general-purpose", "expands", "temporal", "also", "various", "tempo's", "tempo,", "series.", "(gpt)", "this", "intriguing", "results,", "framework,", "prompt-based", "time", "framework.", "pre-trained", "capability", "models:", "supervised", "constitute", "complex"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263829780": {"id": "263829780", "openalex": null, "doi": null, "title": "EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING", "abstract": "Neural language models are probabilistic models of human text.They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution.However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models.We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch.In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling.EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges.Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training.Upon extensive evaluation of language models trained using EMO and MLE.We find that EMO demonstrates a consistently better language modeling performance than MLE across domains.Moreover, EMO demonstrates noteworthy enhancements in downstream performance with minimal fine-tuning on merely 25,000 sentences.This highlights the tremendous potential of EMO as a lightweight calibration method for enhancing large-scale pre-trained language models.Our code and data are available at https://github.com/DRSY/EMO.", "authors": [], "concepts": ["train-test", "widely", "bound", "metric", "paper,", "aforementioned", "merely", "minimizing", "computation,", "model", "inherent", "downstream", "method", "observed", "available", "earth", "predominantly", "modeling", "such", "training.upon", "tremendous", "empirical", "probabilistic", "forward", "performance", "from", "lightweight", "using", "mover", "(emo)", "propose", "large-scale", "distribution.however,", "models", "emo:", "25,000", "mismatch.in", "modeling.emo", "sentences.this", "complexity", "better", "equivalent", "end-to-end", "estimation", "high", "ignorance", "aligning", "distance", "decoding", "minimal", "when", "code", "potential", "data", "negative", "text.they", "establish", "neural", "across", "with", "upper", "direct", "degeneration", "noteworthy", "trained", "highlights", "models.we", "that", "distribution", "suboptimal", "between", "challenges.due", "feasible", "language", "fine-tuning", "maximum", "learned", "distributions", "demonstrates", "domains.moreover,", "calibration", "recall-prioritization", "https://github.com/drsy/emo.", "address", "phenomena", "find", "introduce", "various", "auto-regressive", "enhancing", "enhancements", "evaluation", "than", "consistently", "this", "which", "models.our", "still", "diversity", "ease", "mle.we", "(mle),", "optimization", "likelihood", "pre-trained", "capitalizes", "cross-entropy", "human", "properties", "further", "extensive"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263829872": {"id": "263829872", "openalex": null, "doi": null, "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING", "abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \"Can we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\". To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data.", "authors": [], "concepts": ["sota", "decision.", "integrated", "paper,", "decompose", "sr3d,", "data,", "most", "failure", "problem", "question", "object", "cot3dref:", "show", "directly,", "referred", "overall", "consistent", "existing", "only", "head", "requiring", "does", "through", "interpretable", "performance", "into", "extremely", "anchors", "referring", "compared", "dataset,", "task", "objects", "reaches", "comprehensive", "framework", "benchmarks", "data.", "furthermore,", "scenarios.", "final", "methods", "cases.", "chain", "mimic", "manually", "entire", "perception", "thoughts", "significantly", "without", "ability", "utterances.", "experiments", "network", "localize", "conditioned", "when", "system?\".", "improves", "target.", "predicting", "addition,", "potential", "validate", "making", "data-efficient.", "identify", "whereas", "design", "sr3d", "dubbed", "scanrefer", "trained", "illustrate", "devote", "approach", "that", "easily", "data-efficient,", "cot3dref,", "grounding", "causing", "scenes", "end,", "sequence-to-sequence", "intermediate", "data-efficient", "first", "address", "(seq2seq)", "match", "following", "annotated", "moreover,", "also", "then", "nr3d,", "formulate", "boosting", "this", "steps,", "enables", "interpretability", "framework,", "chain-of-thoughts", "proposed", "visual", "\"can", "gains", "architecture.", "human", "helps", "complex"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263829977": {"id": "263829977", "openalex": null, "doi": null, "title": "TALK LIKE A GRAPH: ENCODING GRAPHS FOR LARGE LANGUAGE MODELS", "abstract": "Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance.Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends.Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem.In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs.We show that LLM performance on graph reasoning tasks varies on three fundamental levels:(1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered.These novel results provide valuable insight on strategies for encoding graphs as text.Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.", "authors": [], "concepts": ["essential", "tasks", "recommender", "graph-structured", "talk", "valuable", "show", "system,", "such", "these", "about", "large", "correct", "insights", "tool", "inferences", "text,", "performance", "social", "strategies", "encoders", "structure", "fundamental", "task", "very", "comprehensive", "nature", "models", "method,", "encoding", "networks,", "graph", "relationships", "analyzing", "inside", "llms", "like", "applications", "61.8%,", "boost", "text", "novel", "real-world", "4.8%", "consumption", "powerful", "data", "representing", "varies", "trends.despite", "reasoning", "with", "systems,", "identify", "problem.in", "drawing", "illustrate", "work,", "task.", "that", "text.using", "between", "graph:", "language", "interestingly,", "entities", "natural", "graphs", "patterns", "choice", "automated", "first", "levels:(1)", "provide", "three", "computational", "hidden", "progress", "perform", "depending", "results", "understudied", "considered.these", "this", "insight", "llms.we", "finance.reasoning", "study", "remarkable", "itself,", "remains", "complex", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263830433": {"id": "263830433", "openalex": null, "doi": null, "title": "Understanding prompt engineering may not require rethinking generalization", "abstract": "Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings.This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data.In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds.Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error.We demonstrate empirically that this holds for existing handcrafted prompts and prompts generated through simple greedy search.Furthermore, the resulting bound is well-suited for model selection: the models with the best bound typically also have the best test performance.This work thus provides a possible justification for the widespread practice of \"prompt engineering,\" even if it seems that such methods could potentially overfit the training data.", "authors": [], "concepts": ["recourse", "demonstrate", "holds", "bound", "typically", "paper,", "error", "performs", "overfit", "impressive", "engineering", "model", "show", "method", "resulting", "within", "test", "classical", "achieved", "such", "these", "existing", "prior", "remarkably", "through", "performance", "from", "relatively", "zero-shot),", "prompts", "well", "many", "pac-bayes", "explicit", "classifier", "engineering,\"", "simple", "explain", "nature", "greedy", "models", "generalization", "rendering", "thus", "data.", "seemingly", "methods", "well-suited", "work", "manually", "learning", "handcrafted", "performance.this", "i.e.,", "prompts,", "without", "could", "understanding", "longer", "actually", "overfitting,", "potentially", "success", "(thus", "model,", "when", "percentage", "held-out", "engineered", "seems", "points", "generated", "achieve", "training", "settings.this", "with", "widespread", "given", "discrete", "prompt", "selection:", "approach", "that", "imagenet", "observation:", "language", "\"prompt", "suffer", "error.we", "rethinking", "crafting", "presents", "empirically", "build", "instance,", "tight", "combined", "possible", "standards", "results", "also", "best", "true", "zero-shot", "this", "still", "provides", "search.furthermore,", "bounds", "little", "process,", "classifiers", "prompted", "have", "often", "data.in", "literature:", "practice", "bounds.specifically,", "even", "require", "models,", "justification", "surprising", "vision-language"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263830786": {"id": "263830786", "openalex": null, "doi": null, "title": "SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING", "abstract": "The popularity of LLaMA(Touvron et al., 2023a;b)  and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs.Regardless, the cost of training such models from scratch on trillions of tokens remains high.In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models.Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains.We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters.Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch.This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs. 1", "authors": [], "concepts": ["smaller", "state-of-the-art", "demonstrate", "pruning,", "each", "outperform", "losses", "means", "evaluations,", "model", "recently", "downstream", "targeted", "prunes", "composition", "tokens", "scratch.this", "such", "scratch", "larger", "existing", "large", "only", "requiring", "updates", "pre-trained,", "1.3b", "pre-training", "more", "from", "dynamically", "series,", "domains.we", "while", "compared", "varying", "models", "wide", "sheared", "work", "openllama", "sheared-llama", "llms", "specified", "equivalent", "target", "dimensions", "end-to-end", "techniques:", "other", "cost", "instruction", "based", "2.7b", "powerful", "emerged", "heads,", "potential", "data", "sizes,", "training", "across", "with", "llama:", "shape", "incite,", "tuning", "parameters.sheared-llama", "dynamic", "highlights", "work,", "approach", "that", "presenting", "llms.regardless,", "high.in", "down", "language", "different", "structured", "intermediate", "effective", "popularity", "layers,", "compelling", "range", "2023a;b)", "leveraging", "batch", "cost-effective", "building", "hidden", "accelerating", "al.,", "moderate-sized", "llama2-7b", "open-source", "pruning", "this", "llms.", "which", "models.our", "provides", "evidence", "loading,", "llama(touvron", "employs", "efficacy", "removing", "develop", "trillions", "study", "sampled", "models,", "remains", "pythia,", "compute", "(llms)", "manner,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263831485": {"id": "263831485", "openalex": null, "doi": null, "title": "TOWARDS FOUNDATION MODELS FOR KNOWLEDGE GRAPH REASONING", "abstract": "Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language.Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations.ULTRA builds relational representations as a function conditioned on their interactions.Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs.Fine-tuning further boosts the performance.", "authors": [], "concepts": ["inference", "learn", "unseen", "strategy", "transferable", "model", "tokens", "such", "generally", "kgs,", "specific", "performance.", "performance", "(kgs)", "present", "function", "towards", "sizes", "models", "enable", "fine-tuned", "inductively", "graph", "better", "learning", "foundation", "interactions.such", "designing", "graph.conducting", "ability", "link", "vocabularies.in", "thanks", "textual", "language.knowledge", "experiments", "inputs", "inductive", "conditioned", "step", "ultra,", "vision", "vocabularies", "reasoning", "with", "trained", "conditioning", "work,", "graphs.fine-tuning", "approach", "that", "boosts", "representations.ultra", "their", "knowledge", "strong", "entity", "arbitrary", "language", "different", "prediction", "relation", "graphs", "builds", "make", "representations", "generalize", "overlap.the", "baselines", "relational", "allows", "challenge", "universal", "find", "various", "than", "zero-shot", "this", "vocabulary", "have", "often", "visual", "pre-trained", "further", "single", "ultra"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263831492": {"id": "263831492", "openalex": null, "doi": null, "title": "LEMUR: INTEGRATING LARGE LANGUAGE MODELS IN AUTOMATED PROGRAM VERIFICATION", "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties that is challenging for verification tools.We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification.We formally describe this methodology as a set of derivation rules and prove its soundness.We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.", "authors": [], "concepts": ["improvements", "challenging", "question", "lemur:", "used", "they", "derivation", "about", "large", "demands", "general", "verification", "sound", "demonstrated", "task", "propose", "models", "abstract", "competition", "llms", "high-level", "soundness.we", "prove", "program", "procedure,", "calculus", "reasoning", "reasoners", "whether", "that", "formally", "language", "integrating", "rules", "instantiate", "automated", "verification.we", "code-understanding", "practical", "combine", "tools.we", "synthetic", "power", "this", "which", "methodology", "verification,", "raises", "benchmarks.", "often", "capability", "describe", "properties"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263831863": {"id": "263831863", "openalex": null, "doi": null, "title": "SELF-SUPERVISED DATASET DISTILLATION FOR TRANSFER LEARNING", "abstract": "Dataset distillation methods have achieved remarkable success in distilling a large dataset into a small set of representative samples.However, they are not designed to produce a distilled dataset that can be effectively used for facilitating selfsupervised pre-training.To this end, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning (SSL).We first prove that a gradient of synthetic samples with respect to a SSL objective in naive bilevel optimization is biased due to the randomness originating from data augmentations or masking.To address this issue, we propose to minimize the mean squared error (MSE) between a model's representations of the synthetic examples and their corresponding learnable target feature representations for the inner objective, which does not introduce any randomness.Our primary motivation is that the model obtained by the proposed inner optimization can mimic the self-supervised target model.To achieve this, we also introduce the MSE between representations of the inner model and the self-supervised target model on the original full dataset for outer optimization.Lastly, assuming that a feature extractor is fixed, we only optimize a linear head on top of the feature extractor, which allows us to reduce the computational cost and obtain a closedform solution of the head with kernel ridge regression.We empirically validate the effectiveness of our method on various applications involving transfer learning.", "authors": [], "concepts": ["facilitating", "extractor,", "error", "problem", "effectively", "model", "assuming", "used", "selfsupervised", "method", "dataset", "they", "efficient", "achieved", "large", "only", "head", "does", "regression.we", "feature", "examples", "minimize", "from", "into", "representative", "learnable", "issue,", "randomness.our", "propose", "learning.", "transfer", "optimize", "full", "designed", "methods", "biased", "mimic", "linear", "this,", "model.to", "learning", "inner", "gradient", "model's", "optimization.lastly,", "target", "applications", "produce", "prove", "outer", "naive", "(ssl).we", "originating", "augmentations", "success", "(mse)", "cost", "novel", "self-supervised", "fixed,", "corresponding", "extractor", "data", "achieve", "validate", "mean", "obtained", "with", "distilled", "that", "motivation", "between", "their", "bilevel", "randomness", "involving", "solution", "masking.to", "end,", "unlabeled", "squared", "effectiveness", "ridge", "representations", "reduce", "empirically", "distillation", "first", "pre-training.to", "address", "allows", "objective", "kernel", "computational", "primary", "synthetic", "also", "objective,", "introduce", "various", "samples.however,", "this", "which", "respect", "samples", "distilling", "have", "small", "proposed", "optimization", "original", "remarkable", "closedform", "obtain"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263834884": {"id": "263834884", "openalex": null, "doi": null, "title": "EVALUATING LARGE LANGUAGE MODELS AT EVALUATING INSTRUCTION FOLLOWING", "abstract": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these \"LLM evaluators\", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLM-BAR, designed to test the ability of an LLM evaluator in discerning instructionfollowing outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBAR and even the highestscoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBAR, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.", "authors": [], "concepts": ["future", "challenging", "evaluators\",", "metric", "them", "evaluator,", "test", "meta-evaluation", "tone.", "these", "existing", "large", "prompting", "qualities", "more", "performance", "into", "present", "strategies", "comparing", "using", "highestscoring", "distinct", "engaging", "llmbar", "while", "outputs.", "instruction-following", "deceptive", "e.g.,", "instructionfollowing", "models", "suite", "gauges", "combinations", "exhibit", "evaluators.", "designed", "contrary", "better", "manually", "adheres", "llms", "ability", "continues", "other", "text", "novel", "instruction", "evaluators", "llm-bar,", "prompts)", "emerged", "generated", "\"llm", "(i.e.,", "evaluations", "offer", "with", "instruction.", "given", "following,", "evaluating", "close", "developing", "possess", "that", "between", "outputs,", "alternative", "language", "meta-evaluation,", "investigates", "different", "adhering", "scalable", "hope", "paper", "closely", "research", "instructions", "cost-effective", "following", "improvement.", "increasing", "also", "ever", "introduce", "evaluator", "llm-based", "discerning", "particularly", "evaluation", "models.", "this", "insight", "discover", "list", "assess", "efficacy", "ones", "pairs", "diverging,", "curated", "benchmark,", "authors", "have", "substantial", "llmbar,", "even", "mislead", "foster", "accelerate,", "room", "human", "further", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263889455": {"id": "263889455", "openalex": null, "doi": null, "title": "UNIFIED LANGUAGE-VISION PRETRAINING IN LLM WITH DYNAMIC DISCRETE VISUAL TOKENIZATION", "abstract": "TE VISUAL TOKENIZATION\n29 Sep 20234C8D833F4622C6A583127C3A667E25A5arXiv:2309.04669v2[cs.CV]\nRecently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data.However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM.Such an inequitable treatment of vision and language heavily constrains the model's potential.In this paper, we break through this limitation by representing both vision and language in a unified form.Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read.The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image.Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm.This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously.Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks.Our code and models will be available at https:", "authors": [], "concepts": ["researchers", "translate", "read.the", "llm.such", "simultaneously.extensive", "unified", "understand", "paper,", "margin", "impressive", "upon", "model", "massive", "available", "pretraining", "resulting", "tokens", "inspired", "will", "existing", "large", "multi-modal", "through", "from", "into", "(llm)", "optimizing", "well-designed", "generation", "generative", "word", "constrains", "varying", "models", "process", "semantics", "inequitable", "transfer", "both", "treatment", "presented", "https:", "advance", "support", "lavit", "learning", "foundation", "interface", "showcase", "heavily", "high-level", "like", "prevailing", "recently,", "model's", "exclusively", "regard", "potential.in", "experiments", "tokenizer", "text", "conditioned", "encompass", "break", "code", "image", "content", "foreign", "unification", "representing", "input", "vision", "reasoning", "with", "generalist", "worthy", "discrete", "paradigm.this", "dynamic", "prompt", "extraordinary", "outperforms", "that", "primarily", "form.specifically,", "data.however,", "handle", "indiscriminately", "language", "empowers", "image.coped", "language-vision", "sequence", "approaches", "called", "focus", "20234c8d833f4622c6a583127c3a667e25a5arxiv:2309.04669v2[cs.cv]", "tokenization", "also", "introduce", "same", "serve", "non-linguistic", "this", "tasks.our", "tokenizer,", "length", "limitation", "visual", "frozen", "under", "capability", "remarkable", "generate", "further", "vision-language"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263909212": {"id": "263909212", "openalex": null, "doi": null, "title": "TOWARDS ROBUST MULTI-MODAL REASONING VIA MODEL SELECTION", "abstract": "The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the \"brain\" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges therein and propose the M 3 framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.", "authors": [], "concepts": ["invoke", "widely", "each", "studies", "robustifying", "robust", "reasoning.", "straightforward", "arising", "subtasks", "model", "user", "recent", "dataset", "they", "overall", "subtask", "either", "will", "agents.", "dependencies", "only", "(large", "multi-modal", "tool", "multiple", "serves", "however,", "dependencies,", "neglect", "diverse", "benchmarks,", "towards", "specifically", "subtask,", "task", "agents", "agent,", "autonomous", "framework", "propose", "selection", "models", "acknowledged", "current", "both", "orchestrating", "designed", "methods", "among", "create", "\"brain\"", "apis", "research,", "learning", "significance", "absence", "like", "reveal", "test-time.", "other", "experiments", "inputs", "selection,", "ignorance", "multi-step", "challenges", "traditional", "code", "bolsters", "improves", "unlike", "making", "weather", "reasoning", "with", "challenges.", "identify", "meanwhile,", "calculators", "negligible", "benchmark:", "fragile.", "dynamic", "selection:", "model)", "collaborative", "that", "suboptimal", "primarily", "predefined", "ms-gqa,", "overhead", "https://github.com/lins-lab/m3.", "tools", "language", "integrating", "end,", "robustness", "therein", "focus", "challenge", "considering", "investigate", "incompatible", "invoking", "plug-in", "suitable", "inspiring", "execution", "tasks,", "task-specific", "this", "runtime", "enables", "agent", "process.", "phases,", "planning", "solving.", "thereby", "excel", "capabilities", "complex", "scenarios,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263909278": {"id": "263909278", "openalex": null, "doi": null, "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining", "abstract": "Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories. an online RL algorithm; and (3) when can supervised pretraining find such a good transformer. Specifically, this paper investigates the following open question:How can supervised pretraining on Transformers learn in-context reinforcement learning?In this paper, we initiate a theoretical study of the ICRL capability of transformers under supervised pretraining to address the open questions outlined above. We show that (1) Transformers can implement prevalent RL algorithms, including LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes; (2) The algorithms learned by transformers achieve near-optimal regret bounds in their respective settings; (3) Supervised pretraining find such algorithms as long as the sample size scales with the covering number of transformer class and distribution ratio between expert and offline algorithms.Summary of contributions and paper outline\u2022 We propose a general framework for supervised pretraining approaches to meta-reinforcement learning (Section 2). This framework encompasses existing methods like Algorithm Distillation (Laskin et al.,  2022), where the expert and context algorithms are identical, as well as Decision-Pretrained Transformers (Lee et al., 2023), where the expert generates optimal actions for the MDP. It also includes approximate DPT variants where the expert estimates optimal actions from full interaction trajectories.\u2022 We prove that the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory (Section 3). The generalization error scales with both model capacity and a distribution ratio measuring divergence between the expert algorithm and the algorithm that generated offline trajectories.\u2022 We demonstrate that transformers can effectively approximate several near-optimal reinforcement learning algorithms by taking observed trajectories as context inputs (Section 4). Specifically, we show transformers can approximate LinUCB (Section 4.1) and Thompson sampling algorithms (Section 4.2) for stochastic linear bandit problems, and UCB-VI (Section 4.3) for tabular Markov decision processes. Combined with the generalization error bound from supervised pretraining and regret bounds of these RL algorithms, this provides regret bounds for supervised-pretrained transformers.\u2022 Preliminary experiments validate that transformers can perform ICRL in our setup (Section 5).\u2022 Technically, we prove efficient approximation of LinUCB by showing transformers can implement accelerated gradient descent for solving ridge regression (Appendix D.4), enabling fewer attention layers than the vanilla gradient descent approach inBai et al. (2023). To enable efficient Thompson sampling implementation, we prove transformers can compute matrix square roots through the Pade decomposition (Appendix E.3). These approximation results are interesting in their own right.", "authors": [], "concepts": ["unclear", "actions", "learn", "identical,", "demonstrate", "unseen", "bound", "algorithms.summary", "paper,", "encompasses", "scales", "error", "regret", "particular,", "icrl.", "effectively", "quantitative", "approximate", "thompson", "theoretical", "model", "recently", "assuming", "transformers.", "outlined", "bandits,", "show", "observed", "reinforcement-learning", "respective", "pretraining", "linucb", "they", "algorithms.", "(laskin", "decisions", "includes", "efficient", "such", "will", "these", "existing", "large", "scale", "outline\u2022", "been", "meta-reinforcement", "bandit", "layers", "general", "number", "variants", "datasets", "through", "expectation", "good", "sampling", "from", "(lee", "however,", "icrl", "matrix", "analyzes", "well", "relu", "inbai", "demonstrated", "supervised-pretrained", "regression", "open", "algorithms,", "framework", "propose", "optimal", "reinforcement", "models", "generalization", "estimates", "conditional", "transformer.", "implementation,", "enable", "both", "e.3).", "technically,", "full", "methods", "prevalent", "linear", "4.1)", "learning", "measuring", "contributions", "affects", "2023),", "like", "online", "gradient", "solving", "4.3)", "initiate", "first,", "(2023).", "long", "prove", "interaction", "well-understood.", "offline", "experiments", "inputs", "right.", "pretrained", "decision", "algorithm;", "d.4),", "specifically,", "analysis", "2022),", "when", "class", "covering", "interesting", "settings;", "generated", "data", "decomposition", "achieve", "question:how", "second,", "training", "context", "validate", "divergence", "theoretically", "with", "including", "environments.", "given", "several", "trajectories", "square", "questions", "trained", "capacity", "implement", "approach", "that", "distribution", "above.", "decision-pretrained", "markov", "between", "near-optimal", "mdp.", "transformers", "expert", "their", "trajectory", "setup", "trajectory.", "(section", "in-context", "investigates", "preliminary", "transformer", "provable", "learned", "approximation", "processes;", "learning?in", "make", "ridge", "approaches", "attention", "efficiently", "processes.", "pade", "distillation", "trajectories.", "enabling", "makers:", "first", "vanilla", "size", "address", "paper", "combined", "showing", "accelerated", "(appendix", "algorithms", "find", "following", "5).\u2022", "fewer", "context,", "perform", "results", "problems,", "al.,", "also", "-algorithm", "ucb-vi", "where", "ratio", "sample", "than", "imitate", "this", "which", "taking", "provides", "tabular", "algorithm", "mismatch", "factor", "bounds", "(icrl)", "capabilities,", "trajectories.\u2022", "prompted", "have", "realizability,", "proposed", "roots", "under", "capability", "study", "remarkable", "supervised", "compute", "transformers.\u2022", "capabilities", "generates", "4.2)", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263909429": {"id": "263909429", "openalex": null, "doi": null, "title": "OMNICONTROL: CONTROL ANY JOINT AT ANY TIME FOR HUMAN MOTION GENERATION", "abstract": "We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process.Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model.Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals.At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion.Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism.By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints.Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.Project page: https://neu-vi.github.io/omnicontrol/.", "authors": [], "concepts": ["state-of-the-art", "essential", "humanml3d", "shows", "realism.by", "model.specifically,", "model", "show", "they", "consistent", "refine", "spatial", "joint", "only", "ensures", "highly", "datasets", "more", "times", "into", "them,", "present", "motion.both", "generation", "flexible", "accuracy", "signals", "signals.at", "trajectory,", "realistic,", "propose", "achieves", "tightly", "significant", "methods", "process.unlike", "time,", "incorporating", "other", "diffusion", "promising", "constraints", "novel", "when", "based", "control", "combining", "generated", "input", "omnicontrol", "with", "introduced", "improvement", "approach", "that", "balancing", "complementary", "conform", "motion", "motions", "previous", "joints.project", "different", "guidance", "analytic", "joints", "pelvis", "page:", "results", "also", "same", "text-conditioned", "coherent", "over", "constraints.experiments", "time", "incorporate", "named", "coherent,", "realism", "omnicontrol:", "kit-ml", "https://neu-vi.github.io/omnicontrol/.", "generate", "human", "generates"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "263909549": {"id": "263909549", "openalex": null, "doi": null, "title": "IS IMAGENET WORTH 1 VIDEO? LEARNING STRONG IMAGE ENCODERS FROM 1 LONG UNLABELLED VIDEO", "abstract": "Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary.But are we making the best use of data?How more economical can we be?In this work, we attempt to answer this question by making two contributions.First, we investigate first-person videos and introduce a \"Walking Tours\" dataset.These videos are high-resolution, hourslong, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions.They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos.Existing methods typically adapt imagebased pretraining approaches to incorporate more frames.Instead, we advocate a \"tracking to learn to recognize\" approach.Our method called DORA, leads to attention maps that Discover and tRAck objects over time in an end-to-end manner, using transformer cross-attention.We derive multiple views from the tracks and use them in a classical self-supervised distillation loss.Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.", "authors": [], "concepts": ["actions", "uninterrupted", "learn", "typically", "comparable", "tracks", "question", "approach,", "data?how", "continuous", "\"tracking", "them", "downstream", "method", "pretraining", "classical", "videos", "take,", "large", "remarkably", "multiple", "number", "more", "from", "unlocked", "leads", "encoders", "using", "competitor", "views", "derive", "dora,", "objects", "tours", "scene", "thus", "billions", "scaling", "methods", "tours\"", "video", "learning", "loss.using", "worth", "depicting", "end-to-end", "videos.existing", "long", "transitions.they", "becomes", "novel", "self-supervised", "image", "\"walking", "potential", "cross-attention.we", "frames.instead,", "since", "tasks.", "making", "unnecessary.but", "with", "several", "first-person", "work,", "maps", "answer", "that", "imagenet", "contributions.first,", "advocate", "captured", "unlabelled", "strong", "hourslong,", "transformer", "walking", "natural", "unlabeled", "approaches", "images,", "attention", "called", "approach.our", "distillation", "video?", "economical", "track", "realistic", "tailored", "investigate", "introduce", "best", "dataset.these", "imagebased", "annotation", "over", "this", "adapt", "learning.second,", "be?in", "discover", "time", "incorporate", "high-resolution,", "recognize\"", "uncurated,", "human", "single", "self-supervision", "attempt", "manner,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264128166": {"id": "264128166", "openalex": null, "doi": null, "title": "RETRO-FALLBACK: RETROSYNTHETIC PLANNING IN AN UNCERTAIN WORLD", "abstract": "Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules.While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g.shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory.In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty.We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab.Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.", "authors": [], "concepts": ["mcts", "uncertainty.we", "demonstrate", "solutions", "overlook", "algorithms.", "these", "(e.g.shortest,", "generally", "retro*", "from", "synthesis", "proposing", "account", "meaning", "lowest-cost),", "task", "imperfect", "propose", "optimal", "greedy", "benchmarks", "produces", "popular", "work", "terms", "create", "better", "plan", "fact", "probability", "desired", "maximizes", "space", "molecules.while", "reactions,", "novel", "retrosynthetic", "series", "world", "works", "least", "in-silico", "that", "knowledge", "molecule", "previous", "created", "simpler,", "sets", "executed", "called", "range", "retro-fallback", "retrosynthesis", "paper", "lab.using", "formulation", "algorithms", "find", "metrics", "possible", "uncertain", "then", "laboratory.in", "reactions", "than", "this", "which", "algorithm", "retro-fallback:", "plans", "have", "proposed", "chemical", "processes", "planning", "buyable", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264147017": {"id": "264147017", "openalex": null, "doi": null, "title": "When can transformers reason with abstract symbols?", "abstract": "We investigate the capabilities of transformer large language models (LLMs) on relational reasoning tasks involving abstract symbols.Such tasks have long been studied in the neuroscience literature as fundamental building blocks for more complex abilities in programming, mathematics, and verbal reasoning.For (i) regression tasks, we prove that transformers generalize when trained, but require astonishingly large quantities of training data.For (ii) next-token-prediction tasks with symbolic labels, we show an \"inverse scaling law\": transformers fail to generalize as their embedding dimension increases.For both settings (i) and (ii), we propose subtle transformer modifications which can reduce the amount of data needed by adding two trainable parameters per head.", "authors": [], "concepts": ["parameters", "tasks", "symbols?", "show", "increases.for", "literature", "settings", "verbal", "large", "trained,", "been", "embedding", "(ii)", "reasoning.for", "more", "\"inverse", "reason", "regression", "fundamental", "propose", "trainable", "models", "abstract", "both", "scaling", "symbols.such", "long", "prove", "when", "blocks", "data.for", "labels,", "data", "next-token-prediction", "training", "reasoning", "with", "head.", "subtle", "abilities", "astonishingly", "dimension", "that", "adding", "transformers", "needed", "their", "involving", "language", "modifications", "transformer", "mathematics,", "generalize", "reduce", "symbolic", "relational", "amount", "programming,", "building", "fail", "investigate", "law\":", "tasks,", "which", "neuroscience", "have", "(ii),", "studied", "require", "quantities", "capabilities", "complex", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264147054": {"id": "264147054", "openalex": null, "doi": null, "title": "GTA: A GEOMETRY-AWARE ATTENTION MECHANISM FOR MULTI-VIEW TRANSFORMERS", "abstract": "As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks.However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable.We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure.Based on this hypothesis, we propose a geometryaware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs.By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-based NVS models without any additional learned parameters and only minor computational overhead.", "authors": [], "concepts": ["state-of-the-art", "key-value", "parameters", "typically", "data,", "minor", "show", "wide-baseline", "they", "tokens", "information", "view", "existing", "only", "been", "setting,", "multiple", "datasets", "(gta),", "performance", "tasks.however,", "synthesis", "questionable.we", "many", "schemes", "structure", "positional", "relationship", "transformer-based", "relative", "propose", "models", "structure.based", "encoding", "multi-view", "encodes", "exhibit", "designed", "learning", "gta:", "pairs.by", "structural", "additional", "without", "sparse", "overhead.", "attention,", "geometric", "suitability", "argue", "novel", "necessary", "(nvs)", "improves", "input", "vision", "since", "geometry-aware", "evaluating", "efficiency", "that", "suboptimal", "determined", "between", "transformers", "their", "permutation", "transformation", "different", "tokens,", "learned", "attention", "called", "geometryaware", "computational", "equivariant", "tasks,", "this", "which", "respect", "transform", "have", "underlying", "queries", "mechanism", "hypothesis,", "properties", "initially"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264172174": {"id": "264172174", "openalex": null, "doi": null, "title": "CONTEXT-AWARE META-LEARNING", "abstract": "Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or finetuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach-without meta-training or fine-tuning-exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.Under Review classify the query given an input sequence composed of the support set and query point. This learning paradigm trains the Transformer encoder to extrapolate to new classes in the parameter-space of the model, enabling our approach to learn new visual concepts during inference without fine-tuning. Due to its capacity to learn visual information \"in-context\", we term our approach Context-Aware Meta-Learning (CAML).Our primary contribution is to develop a meta-learning algorithm for universal meta-learning: the capacity to learn any new visual concept during inference without fine-tuning or meta-training on related images. This challenging setting emulates the deployment of LLMs to real-time applications, and strong performance in this setting would unlock new applications of visual meta-learning. Our theoretical analysis shows that an ELMES is the encoding that minimizes the entropy of detecting classes within the support set, and therefore, does not need to be learned. Our empirical analysis highlights the importance of reformulating meta-learning as sequence modeling: considering the support set and query together enables the model to attend to specific visual features of images in the support set to classify the query. Finally, our empirical analysis indicates CAML is a state-of-the-art meta-learning algorithm. On a diverse set of 8 out of 11 meta-learning benchmarks-and without meta-training or fine-tuning-CAML outperforms or matches the performance of P>M>F (Hu et al., 2022), a state-of-the-art meta-learning algorithm that is meta-trained on each benchmark.", "authors": [], "concepts": ["state-of-the-art", "inference", "challenging", "learn", "known", "real-time", "demonstrate", "each", "extractor,", "poorly", "term", "shows", "learning,", "trains", "theoretical", "model", "meta-learning", "elmes", "meta-learning:", "and/or", "within", "test", "fine-tuning-caml", "information", "modeling", "either", "concept", "these", "large", "review", "would", "does", "composed", "feature", "been", "specific", "fine-tuning-exceeds", "empirical", "p>m>f", "performance", "images", "set,", "however,", "fine-tuning.", "objects.", "\"in-context\",", "applications,", "diverse", "benchmarks,", "meta-trained", "objects", "propose", "models", "analogous", "matches", "chatgpt", "encoding", "algorithm.", "caml", "query", "features", "finally,", "support", "benchmarks.under", "recasts", "learning", "contribution", "learned.", "llms", "leverages", "like", "meta-learning.", "without", "applications", "modeling:", "classes", "importance", "analysis", "2022),", "model,", "input", "unable", "with", "attend", "classify", "given", "trained", "finetuning", "paradigm", "capacity", "highlights", "work,", "parameter-space", "context-aware", "approach", "outperforms", "that", "related", "instead", "datapoints", "p>m>f,", "unlock", "emulates", "algorithm,", "strong", "language", "in-context", "fine-tuning", "transformer", "benchmark.", "sequence", "need", "extrapolate", "enabling", "label.", "indicates", "benchmarks-and", "reformulating", "labels", "unknown", "considering", "universal", "concepts", "perform", "detecting", "primary", "point.", "together", "query.", "al.,", "detect", "replicate", "images.", "entropy", "approach-without", "over", "meta-training", "this", "which", "during", "enables", "algorithm", "develop", "have", "datapoint", "visual", "ability,", "frozen", "pre-trained", "similar", "remarkable", "require", "deployment", "minimizes", "setting", "(caml).our", "therefore,", "encoder"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264172668": {"id": "264172668", "openalex": null, "doi": null, "title": "SEEKING NEURAL NUGGETS: KNOWLEDGE TRANSFER IN LARGE LANGUAGE MODELS FROM A PARAMETRIC PERSPECTIVE", "abstract": "Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge-encompassing detection, editing, and merging-there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledgespecific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. We release code and data at", "authors": [], "concepts": ["smaller", "knowledge-encompassing", "corpora.", "parameters", "paper,", "model", "used", "extracted", "injecting", "techniques", "within", "inherently", "knowledgespecific", "align", "larger", "parametric", "these", "large", "ambiguous", "prior", "pre-training", "through", "from", "into", "while", "nuggets:", "varying", "factors", "models", "sensitivity-based", "critical", "benchmarks", "process", "transfer", "transferability", "underscoring", "extract", "this,", "llms", "delved", "manipulate", "perspective", "understanding", "code", "lora", "highlight", "data", "achieve", "evaluations", "employ", "validate", "method.", "neural", "across", "with", "implicit", "editing,", "scales.", "four", "intermediary", "between", "their", "knowledge", "language", "findings", "different", "regarding", "module", "seeking", "empirically", "contributing", "research", "investigate", "moreover,", "transfer,", "release", "detection,", "encode", "models.", "this", "llms.", "operations", "seek", "efficacy", "perspective.", "proposed", "underlying", "merging-there", "wealth", "mechanism", "remains", "extensive", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264172845": {"id": "264172845", "openalex": null, "doi": null, "title": "LIE GROUP DECOMPOSITIONS FOR EQUIVARIANT NEURAL NETWORKS", "abstract": "Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest G, the exponential map may not be surjective. Further limitations are encountered when G is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups G = GL + (n, R) and G = SL(n, R), as well as their representation as affine transformations R n \u22ca G. Invariant integration as well as a global parametrization is realized by decomposing the 'larger' groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals. . Geometric means in a novel vector space structure on symmetric positive-definite matrices. SIAM journal on matrix analysis and applications, 29(1): 328-347, 2007.", "authors": [], "concepts": ["case", "transformations", "decompositions", "29(1):", "outperform", "positive-definite", "geometry", "means", "model", "used", "siam", "invariance", "show", "group", "recent", "individually.", "vector", "maps.", "exponential", "such", "symmetric", "larger", "matrices.", "compact", "abelian.", "limitations", "proven", "out-of-distribution", "affine", "kernels", "much", "through", "classification", "realized", "into", "present", "matrix", "using", "homogeneous", "spaces,", "well", "interest", "groups", "parametrization", "structure", "applications,", "capsule", "very", "invariant", "global", "framework", "transformations.", "328-347,", "models", "networks", "convolution", "task,", "methods", "work", "fact", "principally", "encountered", "enlarging", "especially", "focused", "geometric", "space", "submanifolds", "integration", "network", "explored", "subgroups", "inductive", "analysis", "novel", "'larger'", "when", "class", "training", "biases", "neural", "focusing", "with", "standard", "symmetry", "applicability", "benchmark", "regime.", "affine-invariant", "that", "primarily", "equivariance", "employed", "parametrized", "their", "(convolutional)", "transformation", "previous", "robustness", "groups,", "useful", "surjective.", "decomposing", "neither", "generalisation", "build", "proposals.", "sl(n,", "both.", "possible", "equivariant", "depending", "handled", "logarithm", "where", "this", "abelian,", "which", "respect", "low-data", "algebra,", "framework,", "2007.", "have", "limited", "evaluate", "under", "capability", "representation", "models,", "journal", "further", "geometrical"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264172935": {"id": "264172935", "openalex": null, "doi": null, "title": "VIDEO LANGUAGE PLANNING", "abstract": "We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data. To this end, we present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP scales with increasing computation budget where more computation time results in improved video plans, and is able to synthesize long-horizon video plans across different robotics domainsfrom multi-object rearrangement, to multi-camera bi-arm dexterous manipulation. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video. Experiments show that VLP substantially improves long-horizon task success rates compared to prior methods on both simulated and real robots (across 3 hardware platforms).Step 1: push blue triangle to \u2026Step 1: push red star to left \u2026Step 2: (re-plan) t t + k t current t goalStep 2: (re-plan)Step 2: (re-plan)", "authors": [], "concepts": ["actions", "each", "\u2026step", "push", "scales", "robotics", "tasks", "show", "(vlp),", "recent", "complete", "specifications", "videos", "robots", "consists", "simulated", "domainsfrom", "large", "prior", "observation,", "(video", "(ii)", "able", "more", "into", "present", "generative", "tree", "compared", "task", "rates", "models", "language,", "advances", "current", "data.", "both", "frame", "final", "methods", "blue", "plan", "improved", "plans,", "policies,", "video", "budget", "(across", "functions,", "substantially", "outputs", "long", "experiments", "video.", "space", "(re-plan)", "pretrained", "detailed", "multi-object", "success", "left", "procedure,", "conditioned", "instruction", "synthesize", "improves", "image", "generated", "takes", "input", "(re-plan)step", "search", "across", "with", "train", "goal-conditioned", "task.", "that", "dexterous", "goalstep", "real", "triangle", "language", "hardware", "different", "rearrangement,", "end,", "intermediate", "star", "platforms).step", "text-to-video", "language)", "multimodal", "enabling", "leveraging", "value", "multi-camera", "translated", "policies", "robot", "internet-scale", "results", "increasing", "serve", "where", "dynamics", "bi-arm", "models.", "this", "provides", "algorithm", "time", "interested", "long-horizon", "plans", "computation", "manipulation.", "visual", "describe", "planning", "complex", "vision-language"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264288700": {"id": "264288700", "openalex": null, "doi": null, "title": "In defense of parameter sharing for model-compression", "abstract": "When considering a model architecture, there are several ways to reduce its memory footprint.Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning.More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training.In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models.Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range.This advantage becomes particularly pronounced in higher compression scenarios.Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings.This points out inherent capacity advantage that RPS enjoys over sparse models.Theoretically, we establish RPS as a superior technique in terms of memory-efficient representation when compared to pruning for linear models.This paper argues in favor of paradigm shift towards RPS based models.During our rigorous evaluation of RPS, we identified issues in the stateof-the-art RPS technique ROAST, specifically regarding stability (ROAST's sensitivity to initialization hyperparameters, often leading to divergence) and Paretocontinuity (ROAST's inability to recover the accuracy of the original model at zero compression).We provably address both of these issues.We refer to the modified RPS, which incorporates our improvements, as STABLE-RPS.Preprint.Under review.", "authors": [], "concepts": ["smaller", "demonstrate", "rewinding", "paper,", "stable-rps.preprint.under", "selecting", "footprint.historically,", "recover", "model", "inherent", "pronounced", "training.in", "techniques", "ways", "such", "these", "comprehensively", "roast,", "highly", "through", "informed", "performance", "included", "compression).we", "scenarios.notably,", "improvements,", "defense", "accuracy", "mag,", "towards", "compared", "specifically", "traction", "rigorous", "models", "there", "models.theoretically,", "networks", "advantage", "both", "popular", "methods", "terms", "start", "enjoys", "linear", "superior", "compression", "inability", "entire", "like", "grasp,", "recently,", "randomized", "sparse", "memory", "sensitivity", "becomes", "high", "stability", "when", "based", "argues", "modified", "stateof-the-art", "paretocontinuity", "points", "zero", "data", "snip,", "settings.this", "parameter-sharing", "favor", "establish", "provably", "across", "gained", "model-compression", "moderately", "model-agnostic,", "lottery", "refer", "several", "rps,", "sharing", "strategies,", "paradigm", "capacity", "shift", "that", "creating", "between", "models.this", "models.during", "exhibits", "findings", "synflow,", "higher", "issues.we", "ticket", "regarding", "pruning.more", "(rps)", "technique", "approaches", "leading", "reduce", "trade-off", "address", "paper", "techniques,", "architectures", "parameter", "considering", "building", "review.", "particularly", "evaluation", "over", "pruning", "consistently", "this", "which", "models.our", "(roast's", "issues", "initialization", "assess", "architecture,", "divergence)", "incorporates", "outperforms/matches", "memory-efficient", "(ltr),", "have", "range.this", "often", "hyperparameters,", "original", "representation", "even", "identified"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264288929": {"id": "264288929", "openalex": null, "doi": null, "title": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback", "abstract": "We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators.We introduce two algorithms that achieve improved regret performance compared to existing approaches.The first algorithm, although computationally inefficient, ensures a regret of O( \u221a K), where K is the number of episodes.This is the first result with the optimal K dependence in the considered setting.The second algorithm, which is based on the policy optimization framework, guarantees a regret of O(K 3 /4 ) and is computationally efficient.Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023]  with O(K 4 /5 + poly( 1 /\u03bbmin)) regret, for some problem-dependent constant \u03bb min that can be arbitrarily close to zero, and a computationally efficient algorithm by Sherman et al. [2023b]  with O(K 6 /7 ) regret.* The authors are listed in alphabetical order.is the horizon length.The challenge is that this conversion depends on the transition of the MDP, which is not available to the learner.Therefore, the learner has to estimate the feature of every policy during the learning process.Previous work in this direction [Kong et al., 2023]  faced obstacles in controlling the estimation error and was only able to show a K 4 /5 +poly( 1 /\u03bb min ) regret bound assuming there exists an exploratory policy inducing a covariance matrix \u03bb min I.We addressed the obstacles through 1) state space discretization (Section 3.2), and 2) model-free estimation for the occupancy measure of policies over the discretized state space (Section 3.3).These allow us to emulate the success in the tabular case [Jin et al., 2020a] and obtain the tight \u221a K regret.Efficient K 3 /4 algorithm.The efficient algorithm is based on the policy optimization framework[Luo et al., 2021].Different from previous works that all use exponential weights, we use Follow-the-Regularized-Leader (FTRL) with log-determinant (logdet) barrier regularizer to perform policy updates, which has the benefit of keeping the algorithm more stable [Zimmert and Lattimore, 2022, Liu et al., 2023a].We carefully combine logdet-FTRL with existing algorithmic/analysis techniques to further improve the regret bound.These include 1) an initial exploration phase to control the transition estimation error [Sherman et al., 2023a], 2) optimistic least-square policy evaluation in bonus construction [Sherman et al., 2023b], 3) dilated bonus construction[Luo et al., 2021], and 4) a tighter concentration bound for covariance matrix estimation [Liu et al., 2023a].Related WorkIn this subsection, we review prior works on adversarial MDPs and policy optimization.Learning in Adversarial MDPs.Adversarial MDPs refer to a class of MDP problems where the transition is fixed while the loss function changes over time.Learning adversarial tabular MDPs under bandit feedback and unknown transition has been extensively studied [", "authors": [], "concepts": ["case", "log-determinant", "+poly(", "bound", "exists", "access", "error", "learner", "regret", "losses", "exploratory", "tighter", "regret.efficient", "constant", "assuming", "[jin", "[zimmert", "show", "available", "addressed", "techniques", "efficient", "exponential", "[2023]", "existing", "only", "feedback,", "prior", "review", "extensively", "feature", "been", "bandit", "number", "ensures", "optimization.learning", "obstacles", "through", "mdps", "carefully", "able", "controlling", "barrier", "more", "performance", "from", "zero,", "problem-dependent", "concentration", "matrix", "poly(", "estimate", "while", "function", "towards", "compared", "2023b],", "measure", "3.3).these", "optimal", "reinforcement", "there", "logdet-ftrl", "inefficient,", "regret.*", "alphabetical", "[kong", "some", "workin", "simulators.we", "subsection,", "mdps.adversarial", "work", "3.2),", "mdp,", "improved", "linear", "(ftrl)", "include", "learning", "direction", "bound.these", "improve", "problems", "construction[luo", "online", "significantly", "[sherman", "without", "model-free", "2020a]", "learner.therefore,", "conversion", "computationally", "2021],", "2023]", "estimation", "space", "dependence", "stable", "decision", "changes", "follow-the-regularized-leader", "success", "regularizer", "based", "process.previous", "class", "control", "transition", "time.learning", "policy", "exploration", "construction", "achieve", "weights,", "(logdet)", "although", "approaches.the", "covariance", "works", "efficient.both", "framework[luo", "with", "order.is", "refer", "considered", "horizon", "listed", "dilated", "close", "emulate", "inducing", "that", "setting.the", "markov", "bonus", "fixed", "benefit", "transitions", "inefficient", "knowledge", "algorithm,", "optimistic", "previous", "(section", "2021].different", "[liu", "initial", "algorithm.the", "[2023b]", "2023a],", "faced", "algorithmic/analysis", "result", "allow", "sherman", "state-of-the-art:", "first", "tight", "arbitrarily", "i.we", "feedback", "unknown", "challenge", "policies", "kong", "algorithms", "discretized", "combine", "perform", "phase", "results", "episodes.this", "discretization", "al.,", "second", "introduce", "/\u03bbmin))", "where", "depends", "evaluation", "over", "occupancy", "this", "which", "state", "during", "tabular", "length.the", "algorithm", "framework,", "adversarial", "loss", "regret,", "authors", "optimization", "updates,", "studied", "processes", "lattimore,", "under", "2023a].related", "least-square", "study", "2023a].we", "2022,", "guarantees", "further", "every", "obtain", "keeping"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264289064": {"id": "264289064", "openalex": null, "doi": null, "title": "GROUP PREFERENCE OPTIMIZATION: FEW-SHOT ALIGNMENT OF LARGE LANGUAGE MODELS", "abstract": "Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups.Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases.We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner.In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations.For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups.We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks.These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users.Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods. 1 Warning: This paper contains qualitative examples that may be viewed as offensive or harmful.", "authors": [], "concepts": ["cases.we", "inference", "tasks.these", "demonstrate", "creative", "each", "countries,", "learning,", "tasks", "expensive", "harmful.", "meta-learning", "group", "parameterize", "align", "such", "steering", "existing", "large", "only", "requiring", "examples", "through", "more", "demographic", "from", "opinion", "manner.in", "strategies", "predict", "using", "many", "subjective", "groups", "(gpo),", "rigorous", "requires", "global", "framework", "viewed", "sizes", "models", "differ", "chatbots", "augment", "computing", "outperforming", "steers", "preferences,", "users.our", "ranging", "optimization:", "llms", "significantly", "amounts", "applications", "methods.", "gpo,", "group,", "groups.we", "real-world", "individual", "data", "training", "evaluations", "validate", "varied", "across", "preference", "with", "train", "few-shot", "warning:", "preferences", "(llms),", "several", "less", "prohibitive", "trained", "independent", "adaptation", "groups.existing", "that", "base", "involve", "language", "in-context", "fine-tuning", "different", "writing,", "transformer", "autoregressive", "groups,", "contains", "module", "empirically", "paper", "judgments", "three", "algorithms", "fewer", "results", "also", "accurately", "introduce", "offensive", "this", "efficacy", "nuanced", "resources,", "alignment", "group-specific", "adapting", "aligns", "computation", "optimization", "qualitative", "require", "human", "generations.for"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264306002": {"id": "264306002", "openalex": null, "doi": null, "title": "CCIL: CONTINUITY-BASED DATA AUGMENTATION FOR CORRECTIVE IMITATION LEARNING", "abstract": "We present a new technique to enhance the robustness of imitation learning methods by generating corrective data to account for compounding errors and disturbances.While existing methods rely on interactive expert labeling, additional offline datasets, or domain-specific invariances, our approach requires minimal additional assumptions beyond access to expert data.The key insight is to leverage local continuity in the environment dynamics to generate corrective labels.Our method first constructs a dynamics model from the expert demonstration, encouraging local Lipschitz continuity in the learned model.In locally continuous regions, this model allows us to generate corrective labels within the neighborhood of the demonstrations but beyond the actual set of states and actions in the dataset.Training on this augmented data enhances the agent's ability to recover from perturbations and deal with compounding errors.We demonstrate the effectiveness of our generated labels through experiments in a variety of robotics domains in simulation that have distinct forms of continuity and discontinuity, including classic control problems, drone flying, navigation with high-dimensional sensor observations, legged locomotion, and tabletop manipulation.", "authors": [], "concepts": ["actions", "demonstrate", "perturbations", "corrective", "encouraging", "labeling,", "access", "robotics", "recover", "continuous", "model", "method", "locally", "within", "constructs", "interactive", "drone", "domain-specific", "existing", "actual", "demonstration,", "through", "enhance", "from", "leverage", "present", "dataset.training", "distinct", "account", "forms", "requires", "deal", "legged", "neighborhood", "high-dimensional", "methods", "sensor", "locomotion,", "demonstrations", "generating", "learning", "additional", "continuity-based", "model.in", "ability", "errors", "rely", "local", "offline", "enhances", "experiments", "environment", "augmentation", "minimal", "ccil:", "control", "generated", "data", "flying,", "classic", "with", "including", "states", "navigation", "beyond", "discontinuity,", "agent's", "approach", "assumptions", "domains", "that", "expert", "disturbances.while", "errors.we", "labels.our", "robustness", "lipschitz", "learned", "observations,", "augmented", "effectiveness", "technique", "first", "datasets,", "regions,", "allows", "simulation", "labels", "problems,", "imitation", "invariances,", "dynamics", "this", "insight", "continuity", "have", "manipulation.", "compounding", "data.the", "generate", "variety", "tabletop"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264306022": {"id": "264306022", "openalex": null, "doi": null, "title": "VARIATIONAL INFERENCE FOR SDES DRIVEN BY FRACTIONAL NOISE", "abstract": "We present a novel variational framework for performing inference in (neural) stochastic differential equations (SDEs) driven by Markov-approximate fractional Brownian motion (fBM).SDEs offer a versatile tool for modeling real-world continuous-time dynamic systems with inherent noise and randomness.Combining SDEs with the powerful inference capabilities of variational methods, enables the learning of representative function distributions through stochastic gradient descent.However, conventional SDEs typically assume the underlying noise to follow a Brownian motion (BM), which hinders their ability to capture long-term dependencies.In contrast, fractional Brownian motion (fBM) extends BM to encompass non-Markovian dynamics, but existing methods for inferring fBM parameters are either computationally demanding or statistically inefficient.In this paper, building upon the Markov approximation of fBM, we derive the evidence lower bound essential for efficient variational inference of posterior path measures, drawing from the well-established field of stochastic analysis.Additionally, we provide a closed-form expression to determine optimal approximation coefficients.Furthermore, we propose the use of neural networks to learn the drift, diffusion and control terms within our variational posterior, leading to the variational training of neural-SDEs.In this framework, we also optimize the Hurst index, governing the nature of our fractional noise.Beyond validation on synthetic data, we contribute a novel architecture for variational latent video prediction,-an approach that, to the best of our knowledge, enables the first variational neural-SDE application to video perception.", "authors": [], "concepts": ["inference", "essential", "learn", "neural-sdes.in", "brownian", "bound", "parameters", "field", "typically", "paper,", "variational", "data,", "upon", "hinders", "inferring", "inherent", "within", "path", "modeling", "either", "efficient", "methods,", "conventional", "existing", "tool", "neural-sde", "through", "from", "long-term", "validation", "present", "follow", "representative", "differential", "function", "derive", "index,", "non-markovian", "framework", "propose", "optimal", "capture", "nature", "measures,", "networks", "dependencies.in", "extends", "optimize", "lower", "methods", "terms", "noise.beyond", "video", "learning", "that,", "application", "gradient", "(fbm).sdes", "ability", "governing", "computationally", "latent", "demanding", "diffusion", "statistically", "(bm),", "novel", "encompass", "markov-approximate", "real-world", "(fbm)", "powerful", "well-established", "control", "posterior,", "prediction,-an", "contribute", "training", "offer", "neural", "with", "equations", "sdes", "systems", "performing", "dynamic", "drawing", "fractional", "approach", "closed-form", "determine", "markov", "their", "noise", "motion", "continuous-time", "assume", "expression", "distributions", "approximation", "contrast,", "randomness.combining", "fbm,", "coefficients.furthermore,", "(neural)", "leading", "first", "descent.however,", "provide", "building", "versatile", "synthetic", "also", "driven", "best", "analysis.additionally,", "this", "which", "evidence", "enables", "framework,", "inefficient.in", "hurst", "underlying", "perception.", "architecture", "(sdes)", "posterior", "dynamics,", "drift,", "knowledge,", "capabilities", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264306078": {"id": "264306078", "openalex": null, "doi": null, "title": "SAFE RLHF: SAFE REINFORCEMENT LEARNING FROM HUMAN FEEDBACK", "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical.However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training.To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment.Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models.We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints.Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning.Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms.Experimentally, we finetuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.", "authors": [], "concepts": ["striking", "demonstrate", "constraints.leveraging", "effectively", "crowdworkers'", "model", "inherent", "method", "lagrangian", "adjusts", "existing", "about", "large", "balance", "(safe", "been", "safety", "more", "performance", "from", "dynamically", "using", "explicitly", "while", "function", "compared", "mitigate", "alignment.safe", "issue,", "task", "rlhf", "propose", "reinforcement", "according", "models", "harmlessness", "rlhf),", "significant", "maximizing", "solve", "preferences,", "problem,", "superior", "value-aligned", "learning", "harmlessness,", "llms", "specified", "confusion", "significantly", "constrained", "ability", "alpaca-7b", "evaluations.", "three-round", "training.to", "rlhf,", "cost", "novel", "harmful", "satisfying", "algorithms.experimentally,", "helpfulness", "with", "allowing", "train", "critical.however,", "reward", "preferences", "(llms),", "systems", "models.we", "between", "language", "collected", "fine-tuning", "formalize", "regarding", "decouples", "separate", "presents", "aligned", "address", "value", "feedback", "challenge", "safe", "objectives", "rlhf:", "responses", "tension", "avoiding", "fine-tuning.through", "enhancing", "this", "during", "algorithm", "improving", "development", "optimization", "finetuned", "concern", "human", "never"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264306111": {"id": "264306111", "openalex": null, "doi": null, "title": "FROZEN TRANSFORMERS IN LANGUAGE MODELS ARE EFFECTIVE VISUAL ENCODER LAYERS", "abstract": "This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language.Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -employing a frozen transformer block from pre-trained LLMs as a constituent encoder layer to directly process visual tokens.Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multi-modal vision-language setup with associated language prompts, inputs, or outputs.We demonstrate that our approach consistently enhances performance across a diverse range of tasks, encompassing pure 2D and 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval).Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks.We additionally propose the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding -the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect.This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions.We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms.Code is available at https://github.com/ziqipang/LM4VisualEncoding.", "authors": [], "concepts": ["improvements", "computer", "demonstrate", "activation,", "strategy", "llama", "mechanisms.code", "typically", "boundaries", "informative", "associated", "action", "stronger", "2d/3d", "data,", "tasks", "observation", "question", "overlooked", "surprisingly", "available", "deepening", "tokens", "information", "modeling", "achieved", "conventional", "inputs,", "reveals", "large", "supported", "pushes", "multi-modal", "despite", "feature", "layers", "general", "purely", "more", "performance", "from", "encoders", "effect.this", "inspires", "blocks,", "diverse", "forecasting),", "classification),", "simple", "propose", "explain", "models", "answering", "being", "process", "hypothesis", "https://github.com/ziqipang/lm4visualencoding.", "encoding", "language.even", "regions.we", "block", "work", "perspectives", "opt)", "llms", "absence", "significantly", "prompts,", "textual", "enhances", "understanding", "previously", "blocks.we", "types", "blocks", "image", "non-semantic", "filtering", "vision", "training", "discern", "cloud", "across", "with", "outputs.we", "(llms),", "point", "necessitate", "trained", "approach", "recognition),", "that", "solely", "departing", "transformers", "their", "setup", "motion", "exhibits", "strong", "-the", "language", "amplify", "different", "transformer", "retrieval).such", "effective", "effectiveness", "encompassing", "additionally", "range", "hope", "empirically", "leveraging", "paper", "focus", "phenomenon,", "practices", "utilizing", "(e.g.,", "temporal", "layer", "various", "-employing", "recognition", "intriguingly,", "consistently", "tasks,", "this", "after", "image-text", "directly", "visual", "frozen", "pre-trained", "underlying", "relevant", "applicable", "tokens.our", "constituent", "pure", "further", "vision-language", "encoder"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264406064": {"id": "264406064", "openalex": null, "doi": null, "title": "AN LLM CAN FOOL ITSELF: A PROMPT-BASED ADVERSARIAL ATTACK", "abstract": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM's adversarial robustness.This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack).PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself.The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively.Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples.Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels.Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++.Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.Our project page is available at PromptAttack.", "authors": [], "concepts": ["gpt-3.5", "advglue", "description", "(ao)", "perturbation", "cause", "perturbing", "audit", "rate", "available", "complete", "proposes", "ensure", "llama2", "efficient", "meanings", "victim", "robustness.this", "(promptattack).promptattack", "large", "composed", "tool", "examples", "empirical", "much", "examples.further,", "enhance", "into", "using", "filter", "compared", "task", "simple", "models", "domains,", "meaning,", "illustrating", "generating", "predictions.our", "ensembling", "include", "llm's", "label,", "output", "without", "maintains", "applications", "especially", "components:", "textual", "itself.the", "success", "proper", "sentence", "fidelity", "wrong", "input", "(ag)", "validate", "advglue++.interesting", "including", "page", "(llms),", "necessitate", "project", "prompt", "that", "easily", "semantic", "(oi)", "language", "levels.comprehensive", "findings", "different", "promptattack", "higher", "robustness", "containing", "guide", "guidance", "itself", "make", "safety-critical", "yields", "ground-truth", "paper", "objective", "instructions", "three", "fool", "changing", "results", "converts", "levels,", "sample", "evaluation", "promptattack.", "power", "consistently", "wide-ranging", "adversarial", "attack", "prompt-based", "attacks", "original", "important", "word,", "mislead", "itself:", "emoji", "respectively.besides,", "character,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264426013": {"id": "264426013", "openalex": null, "doi": null, "title": "A QUADRATIC SYNCHRONIZATION RULE FOR DISTRIBUTED DEEP LEARNING", "abstract": "In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models.Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for H steps without synchronizing with others, hence reducing communication frequency.While H has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper H value can lead to generalization improvement.Yet, selecting a proper H is elusive.This work proposes a theory-grounded method for determining H, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting H in proportion to 1 \u03b7 2 as the learning rate \u03b7 decays over time.Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. 1 Compared with the standard data parallel training, QSR enables Local AdamW on ViT-B to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves 1.16% or 0.84% higher top-1 validation accuracy.", "authors": [], "concepts": ["time.extensive", "nodes", "sgd,", "each", "gpus", "elusive.this", "selecting", "and,", "cause", "recommends", "1.16%", "rule", "rate", "show", "method", "lead", "recent", "locally", "accuracy.", "test", "adamw", "proposes", "methods,", "such", "workers", "large", "others,", "been", "decays", "from", "validation", "deep", "dynamically", "overhead,", "gradients", "strategies.", "many", "synchronizing", "accuracy", "compared", "issue", "viewed", "generalization", "achieves", "20.2", "parallel", "synchronization", "improvement.yet,", "methods", "work", "time,", "learning", "improve", "determining", "gradient", "steps", "without", "especially", "local", "resnet", "other", "hence", "experiments", "top-1", "proper", "when", "step", "data", "training", "0.84%", "hyperparameter", "with", "allowing", "train", "trade", "26.7", "standard", "parallelism,", "efficiency", "cost,", "training,", "that", "hours", "imagenet", "communication", "down", "higher", "theory-grounded", "quadratic", "models.local", "proportion", "indicates", "reducing", "address", "value", "research", "together", "frequency.while", "same", "over", "consistently", "this", "which", "vit-b", "enables", "time", "optimization", "named", "distributed", "huge", "(qsr),", "compute", "setting"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264426077": {"id": "264426077", "openalex": null, "doi": null, "title": "What's in a Prior? Learned Proximal Networks for Inverse Problems", "abstract": "Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed.Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where they loosely resemble proximal operators.Yet, something essential is lost in employing these purely data-driven approaches: there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal.This not only makes guaranteeing convergence of iterative schemes challenging but, more fundamentally, complicates the analysis of what has been learned by these networks about their training data.Herein we provide a framework to develop learned proximal networks (LPN), prove that they provide exact proximal operators for a data-driven nonconvex regularizer, and show how a new training strategy, dubbed proximal matching, provably promotes the recovery of the log-prior of the true data distribution.Such LPN provide general, unsupervised, expressive proximal operators that can be used for general inverse problems with convergence guarantees.We illustrate our results in a series of cases of increasing complexity, demonstrating that these models not only result in state-of-the-art performance, but provide a window into the resulting priors learned from data.", "authors": [], "concepts": ["state-of-the-art", "essential", "challenging", "data-driven", "ill-posed.modern", "what", "fundamentally,", "window", "tasks", "approximate", "ubiquitous", "used", "show", "they", "resulting", "cases", "regularize", "loosely", "demonstrating", "these", "algorithmic", "about", "only", "characterization", "been", "general", "purely", "more", "from", "into", "part", "deep", "strategies", "approaches:", "iterative", "but,", "guarantees.we", "schemes", "function", "might", "proximal.this", "framework", "brought", "convergence", "models", "there", "networks", "some", "data.", "employing", "expressive", "learning", "general,", "problems", "what's", "prior?", "something", "performance,", "prove", "priors", "bear", "network", "analysis", "recovery", "proximal", "represents", "makes", "data", "series", "training", "provably", "complexity,", "operators", "with", "otherwise", "lost", "dubbed", "exact", "data.herein", "illustrate", "(lpn),", "unsupervised,", "that", "function,", "their", "distribution.such", "operator", "learned", "result", "plug-and-play", "complicates", "appearing", "provide", "inverse", "strategy,", "results", "commonly", "problems,", "increasing", "too,", "where", "matching,", "true", "log-prior", "which", "develop", "promotes", "have", "guaranteeing", "resemble", "unrolling,", "regularizer,", "nonconvex", "guarantee", "operators.yet,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264438904": {"id": "264438904", "openalex": null, "doi": null, "title": "COURSE CORRECTING KOOPMAN REPRESENTATIONS", "abstract": "Koopman representations aim to learn features of nonlinear dynamical systems (NLDS) which lead to linear dynamics in the latent space.Theoretically, such features can be used to simplify many problems in modeling and control of NLDS.In this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons.We discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as Periodic Reencoding, for faithfully capturing long term dynamics.We justify this method both analytically and empirically via experiments in low and high dimensional NLDS.", "authors": [], "concepts": ["future", "learn", "term", "space.theoretically,", "faithfully", "model", "used", "method", "lead", "they", "ways", "modeling", "such", "limitations", "inference-time", "many", "correcting", "mechanism,", "specifically", "autoencoder", "propose", "dynamics.we", "justify", "simplify", "both", "formulations", "work", "features", "problem,", "linear", "koopman", "problems", "long", "latent", "experiments", "space", "high", "reencoding,", "capturing", "dimensional", "control", "predicting", "nlds.", "refer", "several", "states", "systems", "analytically", "periodic", "different", "prediction", "dynamical", "(nlds)", "course", "representations", "empirically", "nonlinear", "dynamics", "over", "this", "which", "state", "discover", "horizons.we", "nlds.in", "dynamics,", "study"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264438909": {"id": "264438909", "openalex": null, "doi": null, "title": "ONE-HOT GENERALIZED LINEAR MODEL FOR SWITCHING BRAIN STATE DISCOVERY", "abstract": "Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits.Inferred neural interactions from neural signals primarily reflect functional interactions.In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional interactions can change over time.To model dynamically changing functional interactions, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs).However, we argue they lack biological plausibility, as functional interactions are shaped and confined by the underlying anatomical connectome.Here, we propose a novel prior-informed state-switching GLM.We introduce both a Gaussian prior and a one-hot prior over the GLM in each state.The priors are learnable.We will show that the learned prior should capture the state-constant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions.The state-dependent interaction modeled by each GLM offers traceability to capture functional variations across multiple brain states.Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood with real neural datasets, and render interaction structures and hidden states more interpretable when applied to real neural data.", "authors": [], "concepts": ["highest", "meaningful", "shedding", "each", "confined", "data,", "effectively", "connectome.here,", "anatomical", "recover", "model", "show", "interactions,", "traceability", "they", "glm.we", "exposing", "brain", "connectome", "simulated", "will", "prior", "state.the", "multiple", "light", "revealing", "interpretable", "more", "from", "interactions", "dynamically", "behavioral", "interactions.the", "offers", "signals", "predictive", "should", "applied", "physical", "propose", "capture", "models", "likely", "critical", "plausibility,", "data.", "both", "methods", "work", "generalized", "linear", "functional", "switching", "state-switching", "gaussian", "render", "time.to", "long", "interaction", "priors", "understanding", "hence", "argue", "novel", "states.our", "when", "animals", "states,", "(i.e.,", "achieve", "state-constant", "reflect", "neural", "across", "with", "states", "structures", "stimuli,", "modeled", "subject", "variations", "prior-informed", "that", "primarily", "markov", "real", "circuits.inferred", "different", "experiment,", "learned", "state-dependent", "one-hot", "interaction,", "datasets,", "lack", "change", "discovery", "hidden", "experience", "hmm-glms).however,", "stages", "changing", "introduce", "true", "over", "biological", "state", "shaped", "employs", "defined", "likelihood", "learnable.we", "underlying", "interactions.in", "neuron"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264439509": {"id": "264439509", "openalex": null, "doi": null, "title": "KITAB: EVALUATING LLMS ON CONSTRAINT SATISFACTION FOR INFORMATION RETRIEVAL", "abstract": "We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., \"a list of ice cream shops in San Diego\").In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases.More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task.However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction.Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models.KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors.Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes across dimensions such as information popularity, constraint types, and context availability.Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases.While context availability mitigates irrelevant information, it is not helpful for satisfying constraints, identifying fundamental barriers to constraint satisfaction.We open source our contributions to foster further research on improving constraint satisfaction abilities of future models. 1", "authors": [], "concepts": ["future", "solved", "identifying", "emergent", "associated", "shops", "tasks", "failure", "gpt4", "show", "concerns", "dataset", "test", "information", "either", "constraints,", "such", "consists", "errors,", "barriers", "large", "only", "limitations", "were", "exacerbate", "verification", "more", "incorrectness", "incompleteness,", "present", "many", "offers", "types,", "demonstrated", "open", "measure", "fundamental", "information,", "models", "benchmarks", "current", "rising", "exhibit", "availability.results", "llms", "measuring", "contributions", "characterize", "absence", "recently,", "satisfaction.we", "dimensions", "ability", "could", "state-of-the", "helpful", "source", "other", "experiments", "availability", "popularity,", "diego\").in", "web-search", "factual", "authors.our", "modes", "satisfying", "extended", "data", "context", "across", "13,000", "severe", "evaluating", "queries,", "considered", "dynamic", "common", "abilities", "answer", "approach", "cream", "kitab,", "that", "hallucinations", "satisfaction.motivated", "constraint", "knowledge", "language", "initial", "popularity", "kitab:", "llms,", "around", "past,", "decreases.while", "saturated", "measured", "research", "(e.g.,", "retrieval", "context,", "book-related", "also", "acquiring", "gpt3.5", "collection", "than", "models.", "this", "which", "decouple", "list", "improving", "authors", "satisfaction", "have", "irrelevant", "task.however,", "models.kitab", "similar", "bases.more", "foster", "study", "queries", "mitigates", "further", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264490587": {"id": "264490587", "openalex": null, "doi": null, "title": "BAYESIAN NEURAL CONTROLLED DIFFERENTIAL EQUATIONS FOR TREATMENT EFFECT ESTIMATION", "abstract": "Treatment effect estimation in continuous time is crucial for personalized medicine.However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored.Needless to say, uncertainty quantification is crucial for reliable decision-making in medical applications.To fill this gap, we propose a novel Bayesian neural controlled differential equation (BNCDE) for treatment effect estimation in continuous time.In our BNCDE, the time dimension is modeled through a coupled system of neural controlled differential equations and neural stochastic differential equations, where the neural stochastic differential equations allow for tractable variational Bayesian inference.Thereby, for an assigned sequence of treatments, our BNCDE provides meaningful posterior predictive distributions of the potential outcomes.To the best of our knowledge, ours is the first tailored neural method to provide uncertainty estimates of treatment effects in continuous time.As such, our method is of direct practical value for promoting reliable decision-making in medicine.", "authors": [], "concepts": ["effects", "reliable", "meaningful", "outcomes,", "variational", "uncertainty", "medicine.", "continuous", "method", "time.in", "existing", "been", "through", "equations,", "differential", "predictive", "task", "inference.thereby,", "propose", "estimates", "system", "methods", "treatment", "bayesian", "estimation", "ignored.needless", "assigned", "novel", "potential", "neural", "promoting", "direct", "whereas", "equations", "point", "modeled", "dimension", "say,", "such,", "fill", "equation", "outcomes.to", "distributions", "sequence", "allow", "first", "quantification", "value", "decision-making", "provide", "controlled", "practical", "ours", "tailored", "tractable", "coupled", "best", "bncde,", "where", "this", "treatments,", "provides", "gap,", "medical", "time", "applications.to", "have", "personalized", "limited", "posterior", "effect", "(bncde)", "medicine.however,", "crucial", "bncde", "knowledge,", "time.as", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264555202": {"id": "264555202", "openalex": null, "doi": null, "title": "CAN LLMS KEEP A SECRET? TESTING PRIVACY IMPLICATIONS OF LANGUAGE MODELS VIA CONTEXTUAL INTEGRITY THEORY", "abstract": "The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context.In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing CONFAIDE, 1 a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs.Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively.This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning.Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.", "authors": [], "concepts": ["what", "implications", "confaide,", "most", "whom,", "humans", "contextual", "weaknesses", "overlooked", "show", "purpose", "within", "llms.our", "interactive", "information", "secret?", "such", "about", "large", "would", "contexts", "multiple", "underscores", "highly", "sources", "explore", "expected", "from", "inference-time", "prompts", "proposing", "reasoning.our", "share", "reason", "context.in", "persists", "models", "chatgpt", "critical", "designed", "work", "time,", "capable", "private", "draw", "integrity", "llms", "home,", "reveal", "approaches,", "assistants", "experiments", "inputs", "types", "novel", "when", "based", "privacy-preserving", "keep", "introduces", "employ", "gpt-4", "reasoning", "with", "chain-of-thought", "not,", "identify", "given", "instruction-tuned", "benchmark", "work,", "that", "outputs,", "their", "testing", "mind.", "immediate", "language", "different", "need", "attention", "leakage", "privacy", "theory", "this", "respectively.this", "privacy-inducing", "notion", "etc.)", "even", "risks:", "capabilities", "(llms)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264555382": {"id": "264555382", "openalex": null, "doi": null, "title": "LIPSIM: A PROVABLY ROBUST PERCEPTUAL SIMILARITY METRIC", "abstract": "Recent years have seen growing interest in developing and applying perceptual similarity metrics.Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system.On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks.It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks.In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks.We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees.By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas around each data point and certificates for all perturbations within an \u2113 2 ball.Finally, a comprehensive set of experiments shows the performance of LipSim in terms of natural and certified scores and on the image retrieval application.The code is available at https://github.com/SaraGhazanfari/LipSim.", "authors": [], "concepts": ["state-of-the-art", "shortcomings", "demonstrate", "metric)", "perturbations", "each", "years", "metric", "shows", "robust", "logical", "indeed", "superiority", "areas", "available", "recent", "within", "feature", "seen", "shown", "extractors", "performance", "lipsim", "interest", "(lipschitz", "comprehensive", "framework", "propose", "there", "scores", "networks", "ball.finally,", "both", "networks,", "hand,", "terms", "perception", "inherit", "rely", "other", "experiments", "strengths", "aligning", "attacks.we", "based", "code", "serving", "image", "backbone,", "proxy", "data", "provably", "neural", "with", "train", "guarantees.by", "given", "point", "attacks.it", "https://github.com/saraghazanfari/lipsim.", "application.the", "developing", "work,", "guarded", "that", "ensemble", "vit-based", "their", "resilience,", "pixel-wise", "regarding", "provable", "natural", "growing", "1-lipschitz", "around", "called", "leveraging", "vulnerability", "retrieval", "metrics", "networks.in", "certified", "established", "infer", "then", "over", "this", "provides", "adversarial", "have", "certificates", "visual", "perceptual", "metrics.research", "concern", "system.on", "human", "lipsim:", "applying", "similarity"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264802494": {"id": "264802494", "openalex": null, "doi": null, "title": "UNLEASHING THE POWER OF PRE-TRAINED LANGUAGE MODELS FOR OFFLINE REINFORCEMENT LEARNING", "abstract": "Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets.In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the indomain data is limited.Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces Language Models for Motion Control (LaMo), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL.Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages.Empirical results indicate LaMo achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks.In particular, our method demonstrates superior performance in scenarios with limited data samples.Our project website is lamo2023.github.io.", "authors": [], "concepts": ["state-of-the-art", "challenging", "aims", "samples.our", "full-weight", "prowess,", "embeddings,", "tasks", "particular,", "effectively", "method", "recent", "indomain", "large", "lamo", "dense-reward", "general", "contrast", "limited.given", "performance", "from", "(lms)", "pre-collected", "using", "(lamo),", "languages.empirical", "auxiliary", "sparse-reward", "framework", "reinforcement", "projections,", "models", "achieves", "method,", "advances", "unleashing", "employing", "methods", "(rl)", "initializing", "linear", "superior", "learning", "rl.our", "tasks.in", "non-linear", "could", "components:", "offline", "lms,", "becomes", "decision", "value-based", "effectively,", "when", "based", "real-world", "indicate", "lora", "control", "policy", "data", "introduces", "fine-tuning,", "with", "few-shot", "retain", "project", "highlights", "lamo2023.github.io.", "abilities", "scenarios", "four", "instead", "between", "near-optimal", "transformers", "their", "knowledge", "motion", "transformation", "language", "fine-tuning", "integrating", "prediction", "demonstrates", "paper", "website", "find", "combine", "stabilize", "results", "risky;", "particularly", "collection", "power", "this", "during", "loss", "costly", "closes", "pre-trained", "original", "limited", "sequentially", "crucial", "generate", "in-domain", "datasets.in", "(llms)", "scenarios,", "therefore,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264825357": {"id": "264825357", "openalex": null, "doi": null, "title": "MgNO: Efficient Parameterization of Linear Operators via Multigrid", "abstract": "In this work, we propose a concise neural operator architecture for operator learning.Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the i-th neuron in a nonlinear operator layer is defined by O i (u) = \u03c3 j W i j u + B i j .Here, W i j denotes the bounded linear operator connecting j-th input neuron to i-th output neuron, and the bias B i j takes the form of a function rather than a scalar.Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role.As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons.This approach offers both mathematical rigor and practical expressivity.Additionally, MgNO obviates the need for conventional lifting and projecting operators typically required in previous neural operators.Moreover, it seamlessly accommodates diverse boundary conditions.Our empirical observations reveal that MgNO exhibits superior ease of training compared to other CNNbased models, while also displaying a reduced susceptibility to overfitting when contrasted with spectral-type neural operators.We demonstrate the efficiency and accuracy of our method with consistently state-of-the-art performance on different types of partial differential equations (PDEs).", "authors": [], "concepts": ["state-of-the-art", "demonstrate", "fully", "typically", "mgno:", "spectral-type", "parameterize", "method", "efficient", "conventional", "parameterization", "these", "mgno", "empirical", "reduced", "performance", "differential", "offers", "while", "function", "accuracy", "diverse", "compared", "spaces)", "cnnbased", "propose", "displaying", "critical", "rather", "both", "neurons", "contrasted", "i-th", "analogy", "linear", "superior", "(pdes).", "output", "susceptibility", "reveal", "neurons.this", "overfitting", "other", "operators.we", "types", "lifting", "obviates", "when", "define", "form", "accommodates", "takes", "mathematical", "input", "training", "expressivity.additionally,", "mgno,", "neural", "operators", "with", "bounded", "equations", "operators.moreover,", "bias", "structures", "efficiency", "work,", "j-th", "approach", "property,", "that", "role.as", "connecting", "between", "concise", "rigor", "required", "exhibits", "previous", "(banach", "different", "multigrid", "denotes", "operator", "connected", "network,", "approximation", "plays", "need", ".here,", "partial", "neuron,", "seamlessly", "follows:", "result,", "nonlinear", "utilizing", "learning.drawing", "practical", "universal", "observations", "also", "layer", "introduce", "than", "projecting", "conditions.our", "consistently", "this", "ease", "defined", "architecture", "scalar.given", "models,", "neuron", "boundary"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264825424": {"id": "264825424", "openalex": null, "doi": null, "title": "TEXT-TO-3D WITH CLASSIFIER SCORE DISTILLATION", "abstract": "Text-to-3D generation has made remarkable progress recently, particularly with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models.While the usage of classifier-free guidance is well acknowledged to be crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component.In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding: the guidance alone is enough for effective text-to-3D generation tasks.We name this method Classifier Score Distillation (CSD), which can be interpreted as using an implicit classification model for generation.This new perspective reveals new insights for understanding existing techniques.We validate the effectiveness of CSD across a variety of text-to-3D tasks including shape generation, texture synthesis, and shape editing, achieving results superior to those of state-of-the-art methods.Our project page is https://xinyu-andy.github.io/Classifier-Score-Distillation", "authors": [], "concepts": ["state-of-the-art", "essential", "achieving", "methods.our", "paper,", "generation,", "most", "tasks", "model", "synthesis,", "method", "score", "re-evaluate", "alone", "name", "reveals", "existing", "insights", "classification", "sampling", "using", "generation", "well", "classifier", "auxiliary", "https://xinyu-andy.github.io/classifier-score-distillation", "rather", "trick", "acknowledged", "methods", "made", "superior", "techniques.we", "component.in", "successful", "texture", "leverages", "recently,", "text-to-3d", "interpreted", "models.while", "perspective", "understanding", "diffusion", "based", "validate", "across", "with", "including", "implicit", "page", "editing,", "(sds)", "shape", "considered", "usage", "project", "that", "those", "guidance", "effective", "generation.this", "effectiveness", "tasks.we", "distillation", "progress", "results", "optimization,", "particularly", "than", "this", "which", "enough", "(csd),", "discover", "classifier-free", "role", "pre-trained", "remarkable", "finding:", "crucial", "variety", "surprising"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "264825556": {"id": "264825556", "openalex": null, "doi": null, "title": "BESPOKE SOLVERS FOR GENERATIVE FLOW MODELS", "abstract": "Diffusion or flow-based models are powerful generative paradigms that are notoriously hard to sample as samples are defined as solutions to high-dimensional Ordinary or Stochastic Differential Equations (ODEs/SDEs) which require a large Number of Function Evaluations (NFE) to approximate well.Existing methods to alleviate the costly sampling process include model distillation and designing dedicated ODE solvers.However, distillation is costly to train and sometimes can deteriorate quality, while dedicated solvers still require relatively large NFE to produce high quality samples.In this paper we introduce \"Bespoke solvers\", a novel framework for constructing custom ODE solvers tailored to the ODE of a given pre-trained flow model.Our approach optimizes an order consistent and parameter-efficient solver (e.g., with 80 learnable parameters), is trained for roughly 1% of the GPU time required for training the pre-trained model, and significantly improves approximation and generation quality compared to dedicated solvers.For example, a Bespoke solver for a CIFAR10 model produces samples with Fr\u00e9chet Inception Distance (FID) of 2.73 with 10 NFE, and gets to 1% of the Ground Truth (GT) FID (2.59) for this model with only 20 NFE.On the more challenging ImageNet-64\u00d764, Bespoke samples at 2.2 FID with 10 NFE, and gets within 2% of GT FID (1.71) with 20 NFE.", "authors": [], "concepts": ["inception", "challenging", "flow-based", "model.our", "solutions", "approximate", "model", "within", "(2.59)", "consistent", "solver", "(gt)", "large", "only", "solvers.however,", "solvers\",", "number", "more", "sampling", "relatively", "order", "parameters),", "differential", "generation", "example,", "generative", "while", "function", "quality,", "learnable", "compared", "gets", "(nfe)", "framework", "models", "solvers.for", "process", "produces", "high-dimensional", "sometimes", "methods", "\"bespoke", "include", "paradigms", "designing", "bespoke", "significantly", "custom", "constructing", "produce", "high", "diffusion", "distance", "2.73", "novel", "model,", "notoriously", "improves", "parameter-efficient", "powerful", "ordinary", "training", "evaluations", "with", "train", "given", "equations", "ground", "roughly", "trained", "deteriorate", "cifar10", "(odes/sdes)", "nfe,", "approach", "that", "solvers", "nfe.on", "required", "alleviate", "truth", "nfe.", "approximation", "well.existing", "distillation", "flow", "paper", "(e.g.,", "optimizes", "tailored", "imagenet-64\u00d764,", "introduce", "sample", "quality", "this", "which", "still", "samples", "dedicated", "samples.in", "time", "defined", "(fid)", "costly", "pre-trained", "require", "hard", "fr\u00e9chet", "(1.71)", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "27494814": {"id": "27494814", "openalex": null, "doi": null, "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression", "abstract": "Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports(Han et al., 2015a;Narang et al., 2017)prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.", "authors": [], "concepts": ["environments", "inference", "size.", "smaller,", "parameters", "outperform", "perhaps", "straightforward", "model", "reports(han", "broad", "recent", "reduction", "within", "accuracy.", "seq2seq", "exposing", "(small-dense)", "models/datasets", "these", "only", "prune:", "pruned", "number", "marginal", "(deep", "incorporated", "deep", "distinct", "induce", "lstm,", "while", "might", "accuracy", "possibility", "baseline", "simple", "propose", "2017)prune", "models", "sizable", "energy-efficient", "networks", "large-sparse", "(large-sparse)", "exploring", "counterparts", "compression", "model's", "viable", "models),", "units", "memory", "experiments", "resource-constrained", "network", "minimal", "cost", "severely", "achieve", "training", "context", "neural", "across", "hints", "with", "over-parameterized", "network's", "dense", "tuning", "lstm", "maintaining", "that", "2015a;narang", "connection", "their", "small-dense", "alternative", "outset", "footprint.", "prune,", "model.", "matrices,", "sparsity", "technique", "reduce", "cnns,", "paths", "range", "trade-off", "apply", "non-zero", "seamlessly", "simply", "seeks", "reducing", "size", "architectures", "stacked", "find", "hidden", "investigate", "al.,", "structure,", "various", "pruning", "consistently", "this", "efficacy", "loss", "gradual", "nonzero-valued", "large,", "similar", "process.", "compare", "thereby", "variety", "identical"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3290366": {"id": "3290366", "openalex": null, "doi": null, "title": "Graph Partition Neural Networks for Semi-Supervised Classification", "abstract": "We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.", "authors": [], "concepts": ["state-of-the-art", "nodes", "comparable", "globally", "model", "show", "locally", "test", "classification.", "information", "either", "gnns", "large", "scale", "extensively", "fast", "datasets", "classification", "able", "performance", "present", "extremely", "subgraphs", "graphs,", "propose", "graphs.", "subgraphs.", "networks", "wide", "propagating", "methods", "graph", "superior", "partition", "experimental", "(gpnn),", "novel", "indicate", "achieve", "alternate", "semi-supervised", "tasks.", "neural", "with", "standard", "several", "gpnns", "propagation", "that", "between", "graph-based", "handle", "steps.", "processing", "efficiently", "variant", "algorithms", "fewer", "results", "also", "partitioning", "node", "small", "extension", "experiment", "similar", "variety", "(gnns)"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "33513311": {"id": "33513311", "openalex": null, "doi": null, "title": "On the State of the Art of Evaluation in Neural Language Models", "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-theart results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.", "authors": [], "concepts": ["architectures,", "state-of-theart", "steady", "corpora,", "outperform", "innovations", "provided", "recent", "recurrent", "these", "wikitext-2", "been", "sources", "represent", "evaluated", "more", "however,", "using", "well", "somewhat", "large-scale", "models", "prize", "apparently", "popular", "methods", "arrive", "experimental", "differing", "uncontrolled", "bases", "regularised,", "network", "modelling", "when", "code", "hutter", "establish", "neural", "hyperparameter", "with", "standard", "reevaluate", "several", "treebank", "tuning", "lstm", "that", "automatic", "regularisation", "strong", "language", "influx", "black-box", "variation.", "baselines", "architectures", "conclusion", "computational", "results", "penn", "properly", "dataset.", "evaluation", "ongoing", "models.", "which", "state", "resources,", "benchmarks.", "have", "limited", "surprising"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3464416": {"id": "3464416", "openalex": null, "doi": null, "title": "TRAINING CONFIDENCE-CALIBRATED CLASSIFIERS FOR DETECTING OUT-OF-DISTRIBUTION SAMPLES", "abstract": "The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in-and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.", "authors": [], "concepts": ["inference", "known", "demonstrate", "machine", "paper,", "trains", "most", "particular,", "problem", "(implicitly)", "procedures.", "method", "they", "test", "such", "only", "prior", "out-of-distribution", "been", "highly", "classification", "performance", "from", "deep", "however,", "using", "predictions,", "convolutional", "many", "generative", "classifier", "issue,", "overconfident", "detectors", "confidence-calibrated", "arises", "networks", "added", "both", "classifier)", "popular", "work", "terms", "datasets.", "generating", "out-of-distributions.", "sufficiently", "learning", "additional", "recently,", "i.e.,", "in-and", "essence,", "forces", "entropy).", "classifiers.", "novel", "better.", "in-distribution", "state-of-art", "real-world", "image", "cross", "(i.e.,", "since", "training", "neural", "works", "train", "given", "jointly", "several", "less", "out-of-distribution.", "whether", "that", "distribution", "their", "handle", "different", "threshold-based", "effective", "distinguish", "effectiveness", "first", "focus", "(e.g.,", "algorithms", "detecting", "second", "various", "depends", "sample", "applications.", "this", "samples", "one.", "improving", "loss", "develop", "confident", "classifiers", "have", "proposed", "suggest", "pre-trained", "original"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3484654": {"id": "3484654", "openalex": null, "doi": null, "title": "RECASTING GRADIENT-BASED META-LEARNING AS HIERARCHICAL BAYES", "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al.(2017)as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.", "authors": [], "concepts": ["inference", "parameters", "understand", "here,", "approximate", "theoretical", "meta-learning", "method", "al.(2017)as", "techniques", "intelligent", "modeling", "efficient", "prior", "procedure", "contrast", "through", "probabilistic", "basis", "performance", "from", "leverage", "strategies", "well", "bayes", "maml", "function", "inference.", "framework", "propose", "furthermore,", "methods", "hierarchical", "learning", "episodes", "naturally", "bayesian", "quickly", "gradient", "bayes,", "procedure,", "novel", "identification", "reformulate", "makes", "shared", "tasks.", "model-agnostic", "across", "improvement", "task.", "that", "estimation.", "curvature", "algorithm's", "(maml)", "recasting", "scalable", "model.", "make", "operation", "gradient-based", "allows", "opportunity", "computational", "this", "provides", "algorithm", "improving", "agent", "finn", "posterior", "applicable", "formalizing", "complex", "approximators", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3508638": {"id": "3508638", "openalex": null, "doi": null, "title": "On Unifying Deep Generative Models", "abstract": "Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent study respectively. This paper establishes formal connections between deep generative modeling approaches through a new formulation of GANs and VAEs. We show that GANs and VAEs are essentially minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to exchange ideas across research lines in a principled way. For example, we transfer the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism for leveraging generated samples. Quantitative experiments show generality and effectiveness of the imported extensions.", "authors": [], "concepts": ["inference", "unified", "extensions.", "learning,", "variational", "minimizing", "wake-sleep", "impressive", "quantitative", "model", "show", "method", "samples.", "respective", "lines", "recent", "principled", "modeling", "achieved", "view", "existing", "been", "tool", "through", "vaes", "extending", "enhance", "deep", "variants,", "(vaes),", "distinct", "example,", "generative", "diverse", "ideas", "models", "networks", "years.", "frameworks", "transfer", "unifying", "opposite", "respectively.", "improved", "directions,", "learning", "paradigms", "divergences", "experiments", "importance", "success", "powerful", "way.", "generated", "imported", "classic", "received", "across", "with", "literatures", "considered", "gans", "generality", "vaes.", "independent", "that", "between", "analyze", "algorithm,", "autoencoders", "formal", "distributions", "effectiveness", "approaches", "leveraging", "connections", "paper", "research", "formulation", "weighting", "this", "provides", "phases", "enables", "adversarial", "largely", "essentially", "exchange", "have", "(gans)", "posterior", "establishes", "study", "mechanism", "extensive"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3509777": {"id": "3509777", "openalex": null, "doi": null, "title": "The power of deeper networks for expressing natural functions", "abstract": "It is well-known that neural networks are universal approximators, but that deeper networks tend to be much more efficient than shallow ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n 1/k , suggesting that the minimum number of layers required for computational tractability grows only logarithmically with n.", "authors": [], "concepts": ["tractability", "well-known", "merely", "total", "approximate", "approximators,", "efficient", "only", "layers", "light", "number", "much", "allowed.", "more", "from", "deep", "suggesting", "logarithmically", "tend", "networks", "neurons", "networks,", "linearly", "ones.", "classes", "exponentially", "when", "multivariate", "shed", "proving", "neural", "with", "requirement", "that", "functions", "required", "increased", "shallow", "natural", "deeper", "provide", "computational", "universal", "hidden", "also", "layer", "expressing", "than", "power", "this", "evidence", "grows", "variables", "minimum", "single", "polynomials", "neuron"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3516266": {"id": "3516266", "openalex": null, "doi": null, "title": "DON'T DECAY THE LEARNING RATE, INCREASE THE BATCH SIZE", "abstract": "It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate and scaling the batch size B \u221d . Finally, one can increase the momentum coefficient m and scale B \u221d 1/(1 \u2212 m), although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train Inception-ResNet-V2 on ImageNet to 77% validation accuracy in under 2500 parameter updates, efficiently utilizing training batches of 65536 images.", "authors": [], "concepts": ["momentum,", "batches", "here", "rate", "show", "techniques", "accuracy.", "momentum", "test", "epochs,", "(sgd),", "existing", "large", "scale", "updates", "procedure", "crucially,", "number", "increase", "greater", "validation", "decay", "accuracy", "parallelism", "reaches", "1/(1", "tends", "curve", "both", "scaling", "finally,", "learning", "usually", "repurpose", "successful", "gradient", "rate,", "equivalent", "rate.", "adam.", "schedules", "inception-resnet-v2", "training", "although", "with", "train", "common", "instead", "training.", "don't", "imagenet", "tuning.", "sets", "allow", "leading", "reduce", "efficiently", "accuracies", "size", "utilizing", "batch", "parameter", "hyper-parameter", "fewer", "increasing", "images.", "same", "slightly", "nesterov", "this", "during", "after", "updates,", "practice", "under", "coefficient", "shorter", "times.", "further", "obtain", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3522489": {"id": "3522489", "openalex": null, "doi": null, "title": "MINIMAL-ENTROPY CORRELATION ALIGNMENT FOR UNSUPERVISED DEEP DOMAIN ADAPTATION", "abstract": "In this work, we face the problem of unsupervised domain adaptation with a novel deep learning approach which leverages on our finding that entropy minimization is induced by the optimal alignment of second order statistics between source and target domains. We formally demonstrate this hypothesis and, aiming at achieving an optimal alignment in practical cases, we adopt a more principled strategy which, differently from the current Euclidean approaches, deploys alignment along geodesics. Our pipeline can be implemented by adding to the standard classification loss (on the labeled source domain), a source-to-target regularizer that is weighted in an unsupervised and data-driven fashion. We provide extensive experiments to assess the superiority of our framework on standard domain and modality adaptation benchmarks. . Log-hilbert-schmidt metric between positive definite operators on hilbert spaces. In NIPS, 2014. -shifting auto-encoder for unsupervised domain adaptation. In ICCV, 2015.Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.", "authors": [], "concepts": ["data-driven", "demonstrate", "achieving", "strategy", "metric", "and,", "problem", "method", "domain),", "superiority", "2014.", "principled", "networks.", "efficient", "2015.dong-hyun", "classification", "domains.", "more", "from", "source-to-target", "auto-encoder", "deep", "minimal-entropy", "order", "along", "nips,", "simple", "framework", "optimal", "labeled", "hypothesis", "current", "domain", "lee.", "learning", "leverages", "target", "approaches,", "euclidean", "source", "experiments", "geodesics.", "positive", "novel", "regularizer", "weighted", "aiming", "semi-supervised", "pseudo-label:", "neural", "operators", "adopt", "with", "hilbert", "standard", "-shifting", "adaptation", "work,", "approach", "induced", "which,", "that", "adding", "between", "finding", "formally", "pipeline", "minimization", "implemented", "adaptation.", "cases,", "differently", "deploys", "statistics", "provide", "unsupervised", "spaces.", "practical", "fashion.", "second", "entropy", "this", "which", "face", "assess", "loss", "log-hilbert-schmidt", "alignment", "benchmarks.", "correlation", "definite", "iccv,", "extensive", "modality"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3524184": {"id": "3524184", "openalex": null, "doi": null, "title": "INFERENCE SUBOPTIMALITY IN VARIATIONAL AUTOENCODERS", "abstract": "Amortized inference has led to efficient approximate inference for large datasets. The quality of posterior inference is largely determined by two factors: a) the ability of the variational distribution to model the true posterior and b) the capacity of the recognition network to generalize inference over all datapoints. We analyze approximate inference in variational autoencoders in terms of these factors. We find that suboptimal inference is often due to amortizing inference rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.", "authors": [], "concepts": ["inference", "parameters", "variational", "accommodate", "approximate", "model", "used", "show", "efficient", "these", "large", "increase", "amortized", "approximation.", "datapoints.", "expressiveness", "suboptimality", "approximating", "generalizing", "rather", "complexity", "furthermore,", "terms", "datasets.", "learning", "ability", "network", "play", "capacity", "that", "distribution", "suboptimal", "determined", "analyze", "factors:", "autoencoders", "approximation", "choice", "generalize", "simply", "find", "recognition", "true", "than", "quality", "over", "this", "amortizing", "improving", "generator", "largely", "often", "role", "limited", "posterior", "partly", "factors.", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3532296": {"id": "3532296", "openalex": null, "doi": null, "title": "CAN RECURRENT NEURAL NETWORKS WARP TIME?", "abstract": "Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use ad hoc gating mechanisms. Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues. We prove that learnable gates in a recurrent model formally provide quasiinvariance to general time transformations in the input data. We recover part of the LSTM architecture from a simple axiomatic approach. This result leads to a new way of initializing gate biases in LSTMs and GRUs. Experimentally, this new chrono initialization is shown to greatly improve learning of long term dependencies, with minimal implementation effort.Recurrent neural networks (e.g.(Jaeger, 2002)) are a standard machine learning tool to model and represent temporal data; mathematically they amount to learning the parameters of a parameterized dynamical system so that its behavior optimizes some criterion, such as the prediction of the next data in a sequence.Published as a conference paper at ICLR 2018 explains why this is reasonable in most cases, when facing medium term dependencies, but fails when facing long to very long term dependencies.", "authors": [], "concepts": ["experimentally,", "transformations", "machine", "parameters", "term", "most", "short-term", "recover", "model", "axiomatic", "sequence.published", "they", "gating", "recurrent", "gated", "2002))", "such", "these", "dependencies", "been", "tool", "general", "shown", "represent", "(e.g.(jaeger,", "from", "part", "dependencies,", "leads", "warp", "conference", "learnable", "chrono", "very", "simple", "parameterized", "models", "networks", "found", "some", "data.", "system", "(lstms)", "initializing", "mechanisms.", "issues.", "learning", "improve", "successful", "grus.", "gradient", "units", "long", "prove", "gate", "time?", "effort.recurrent", "minimal", "(grus)", "when", "vanishing", "data", "input", "biases", "neural", "facing", "with", "behavior", "fails", "explains", "standard", "approach.", "lstm", "that", "gates", "formally", "implementation", "prediction", "dynamical", "criterion,", "reasonable", "result", "empirically", "next", "cases,", "quasiinvariance", "greatly", "paper", "provide", "amount", "dependencies.", "lstms", "help", "optimizes", "temporal", "memories", "data;", "this", "initialization", "time", "have", "iclr", "architecture", "medium", "mathematically"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "3557557": {"id": "3557557", "openalex": null, "doi": null, "title": "Investigating Human Priors for Playing Video Games", "abstract": "What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012. github.io/humanRL_website/.", "authors": [], "concepts": ["games?", "players", "what", "studies", "humans", "modifying", "used", "available", "videos", "information", "game,", "efficient", "such", "about", "prior", "game", "github.io/humanrl_website/.", "general", "performance.", "good", "from", "systematically", "games", "objects", "priors.", "playing", "quantify", "critical", "ablation", "some", "seemingly", "deal", "conduct", "solve", "furthermore,", "causes", "great", "video", "solving", "could", "minutes", "priors", "manipulations", "environment", "decision", "minutes.", "importance", "types", "drastic", "degradation", "indicate", "game-play.", "mask", "unlike", "makes", "series", "investigating", "with", "given", "speed", "that", "making.", "knowledge", "investigates", "different", "world,", "consistency,", "enabling", "bring", "paper", "find", "results", "various", "sample", "over", "this", "which", "games.", "role", "visual", "computers,", "removal", "https://rach0012.", "human", "priors,", "complex", "e.g."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "43968607": {"id": "43968607", "openalex": null, "doi": null, "title": "Hyperbolic Attention Networks", "abstract": "We introduce hyperbolic attention networks to endow neural networks with enough capacity to match the complexity of data with hierarchical and power-law structure. A few recent approaches have successfully demonstrated the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We extend this line of work by imposing hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic geometry to reason about embeddings produced by deep networks. We achieve this by re-expressing the ubiquitous mechanism of soft attention in terms of operations defined for hyperboloid and Klein models. Our method shows improvements in terms of generalization on neural machine translation, learning on graphs and visual question answering tasks while keeping the neural representations compact.", "authors": [], "concepts": ["improvements", "machine", "parameters", "shows", "klein", "tasks", "geometry", "question", "ubiquitous", "activations", "method", "recent", "soft", "re-expressing", "networks.", "about", "deep", "hyperbolic", "while", "produced", "demonstrated", "reason", "endow", "answering", "generalization", "successfully", "networks", "complexity", "work", "terms", "hierarchical", "line", "learning", "translation,", "embeddings", "data", "achieve", "neural", "with", "capacity", "compact.", "exploit", "imposing", "shallow", "extend", "graphs", "approaches", "representations", "attention", "allows", "structure.", "match", "power-law", "introduce", "models.", "this", "enough", "operations", "benefits", "defined", "hyperboloid", "have", "visual", "mechanism", "keeping"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "44084312": {"id": "44084312", "openalex": null, "doi": null, "title": "The Singular Values of Convolutional Layers", "abstract": "A We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer; for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2% to 5.3%.", "authors": [], "concepts": ["error", "associated", "show", "test", "efficient", "cifar-10", "6.2%", "singular", "characterization", "layers", "from", "deep", "leads", "using", "convolutional", "example,", "linear", "multi-channel", "characterize", "network", "improves", "5.3%.", "with", "standard", "that", "their", "operator-norm", "transformation", "normalization", "residual", "effective", "regularizer;", "enabling", "batch", "computation.", "also", "layer", "ball.", "projecting", "this", "algorithm", "layer,", "onto", "values"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "44119895": {"id": "44119895", "openalex": null, "doi": null, "title": "PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks", "abstract": "Deep learning systems have become ubiquitous in many aspects of our lives. Unfortunately, it has been shown that such systems are vulnerable to adversarial attacks, making them prone to potential unlawful and harmful uses. Designing deep neural networks that are robust to adversarial attacks is a fundamental step in making such systems safer and deployable in a broader variety of applications (e.g., autonomous driving), but more importantly is a necessary step to design novel and more advanced architectures built on new computational paradigms rather than marginally modifying existing ones. In this paper we introduce PeerNets, a novel family of convolutional networks alternating classical Euclidean convolutions with graph convolutions to harness information from a graph of peer samples. This results in a form of non-local forward propagation in the model, where latent features are conditioned on the global structure induced by the data graph, that is up to 3\u00d7 more robust to a variety of white-and black-box adversarial attacks compared to conventional architectures with almost no drop in accuracy.", "authors": [], "concepts": ["vulnerable", "robust", "modifying", "ubiquitous", "them", "samples.", "accuracy.", "classical", "information", "exploiting", "conventional", "such", "existing", "driving),", "been", "shown", "more", "forward", "from", "broader", "deep", "convolutional", "many", "structure", "compared", "built", "fundamental", "deployable", "autonomous", "global", "wisdom", "unfortunately,", "networks", "rather", "graph", "features", "learning", "paradigms", "designing", "non-local", "applications", "prone", "euclidean", "latent", "ones.", "lives.", "family", "alternating", "novel", "conditioned", "harness", "model,", "form", "necessary", "step", "harmful", "attacks,", "potential", "data", "uses.", "safer", "neural", "making", "with", "design", "systems", "propagation", "induced", "that", "black-box", "paper", "architectures", "marginally", "computational", "(e.g.,", "results", "introduce", "importantly", "where", "almost", "than", "unlawful", "this", "white-and", "peernets,", "adversarial", "have", "aspects", "advanced", "attacks", "drop", "convolutions", "graph,", "variety", "peer", "against", "peernets:", "become"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "47015748": {"id": "47015748", "openalex": null, "doi": null, "title": "TEMPORAL DIFFERENCE VARIATIONAL AUTO-ENCODER", "abstract": "To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning. arXiv:1806.03107v3 [cs.LG] 2 Jan 2019 Milos Hauskrecht. Value-function approximations for partially observable Markov decision processes. , et al. Imaginationaugmented agents for deep reinforcement learning.", "authors": [], "concepts": ["partially", "td-vae", "variational", "uncertainty", "simulator", "requirements,", "model", "used", "value-function", "imaginationaugmented", "these", "about", "difference", "into", "auto-encoder", "deep", "using", "points,", "generative", "explicit", "should", "agents", "simple", "propose", "reinforcement", "learning.", "abstract", "exhibit", "plan", "learning", "absence", "motivated", "steps", "step-by-step", "without", "separated", "belief", "decision", "posit", "form", "represents", "satisfying", "td-vae,", "temporally", "representing", "world", "learns", "with", "observable", "condition", "several", "states", "characteristics:", "trained", "world;", "beyond", "that", "arxiv:1806.03107v3", "markov", "mental", "milos", "future,", "containing", "hauskrecht.", "sequence", "representations", "processes.", "build", "approximations", "three", "temporal", "[cs.lg]", "simulation,", "rolled", "which", "state", "beliefs", "time", "single-step", "pairs", "analogue", "transitions.", "have", "directly", "abstraction.", "complex", "environments,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "4737664": {"id": "4737664", "openalex": null, "doi": null, "title": "EMERGENCE OF LINGUISTIC COMMUNICATION FROM REFERENTIAL GAMES WITH SYMBOLIC AND PIXEL INPUT", "abstract": "The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.", "authors": [], "concepts": ["challenging", "learn", "here", "emergent", "data,", "most", "referential", "literature", "reinforcement-learning", "pixel", "linguistic", "scale", "perceive", "been", "were", "through", "able", "more", "from", "deep", "using", "structure", "evolution", "evolve", "games", "agents", "nature", "likely", "being", "found", "hypothesis", "emerge", "protocols", "methods", "degree", "learning", "affects", "ability", "corroborate", "network", "when", "emerged", "traditionally", "data", "input", "training", "tasks.", "world", "structured.", "neural", "with", "emergence", "trained", "developing", "work,", "that", "communication", "language", "previous", "representation.", "structured", "(compositional)", "extend", "symbolic", "research", "realistic", "algorithms", "find", "this", "which", "games.", "studied", "compositional", "protocols,", "thereby", "contemporary", "environments,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "48352800": {"id": "48352800", "openalex": null, "doi": null, "title": "Meta-Learning for Stochastic Gradient MCMC", "abstract": "Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become increasingly popular for simulating posterior samples in large-scale Bayesian modeling. However, existing SG-MCMC schemes are not tailored to any specific probabilistic model, even a simple modification of the underlying dynamical system requires significant physical intuition. This paper presents the first meta-learning algorithm that allows automated design for the underlying continuous dynamics of an SG-MCMC sampler. The learned sampler generalizes Hamiltonian dynamics with state-dependent drift and diffusion, enabling fast traversal and efficient exploration of neural network energy landscapes. Experiments validate the proposed approach on both Bayesian fully connected neural network and Bayesian recurrent neural network tasks, showing that the learned sampler out-performs generic, handdesigned SG-MCMC algorithms, and generalizes to different datasets and larger architectures.*Equal contribution.", "authors": [], "concepts": ["fully", "modification", "continuous", "meta-learning", "monte", "recurrent", "carlo", "efficient", "larger", "existing", "specific", "fast", "datasets", "probabilistic", "diffusion,", "out-performs", "however,", "intuition.", "schemes", "simulating", "hamiltonian", "handdesigned", "simple", "algorithms,", "requires", "physical", "increasingly", "large-scale", "sampler", "sampler.", "significant", "system", "landscapes.", "both", "sg-mcmc", "popular", "chain", "bayesian", "gradient", "experiments", "network", "energy", "model,", "exploration", "generic,", "validate", "neural", "with", "design", "approach", "that", "generalizes", "markov", "drift", "different", "dynamical", "connected", "(sg-mcmc)", "learned", "contribution.", "presents", "state-dependent", "automated", "enabling", "first", "paper", "allows", "showing", "modeling.", "tailored", "mcmc", "dynamics", "tasks,", "this", "samples", "algorithm", "proposed", "underlying", "posterior", "even", "architectures.*equal", "traversal", "become", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "49667762": {"id": "49667762", "openalex": null, "doi": null, "title": "UNIVERSAL TRANSFORMERS", "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset. * Equal contribution, alphabetically by last name. \u2020 Work performed while at Google Brain.", "authors": [], "concepts": ["inference", "challenging", "machine", "each", "field", "outperform", "wmt14", "(rnns)", "tasks", "logical", "model", "recently", "them", "train.", "show", "observed", "they", "translation", "feed-forward", "successes,", "inherently", "addresses", "recurrent", "modeling", "ease,", "faster", "such", "certain", "these", "algorithmic", "concurrently", "despite", "been", "shown", "contrast", "en-de", "however,", "transformer,", "convolutional", "many", "while", "accuracy", "task", "simple", "global", "propose", "models", "contribution,", "generalization", "networks", "wide", "added", "process", "some", "advantage", "popular", "work", "sequential", "superior", "issues.", "like", "translation,", "long", "understanding", "experiments", "inputs", "point,", "google", "alphabetically", "inductive", "halting", "slow", "when", "lambada", "improves", "easy", "makes", "data", "achieve", "training", "sequence,", "tasks.", "neural", "turing-complete.", "with", "including", "self-attentive", "last", "standard", "several", "facto", "bias", "dynamic", "improvement", "assumptions", "updating", "that", "transformers", "their", "cast", "copying", "art,", "handle", "language", "lengths", "those", "transformer", "per-position", "parallelization", "strings", "sequence", "choice", "rnns.", "receptive", "leading", "generalize", "range", "name.", "bleu", "performed", "architectures", "time.", "fail", "universal", "find", "combine", "results", "also", "where", "(ut),", "exceed", "dataset.", "over", "parallel-in-time", "tasks,", "which", "state", "string", "parallelizability", "have", "computation", "formula", "under", "brain.", "even", "sequentially", "times.", "mechanism", "equal", "e.g."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "51559": {"id": "51559", "openalex": null, "doi": null, "title": "QUASI-RECURRENT NEURAL NETWORKS", "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks. * Equal contribution", "authors": [], "concepts": ["size.", "applies", "demonstrate", "machine", "each", "basic", "data,", "they", "translation", "test", "recurrent", "modeling", "faster", "minimalist", "these", "despite", "tool", "times", "convolutional", "function", "accuracy", "predictive", "channels.", "parallelism", "very", "trainable", "lacking", "networks", "parallel", "block", "underline", "sequential", "better", "contribution", "output", "qrnns", "long", "experiments", "dependence", "powerful", "timesteps,", "(qrnns),", "makes", "tasks.", "neural", "across", "train", "modeling,", "parallelism,", "limits", "approach", "that", "quasi-recurrent", "their", "timestep's", "language", "previous", "increased", "sequence", "layers,", "apply", "sentiment", "lstms", "stacked", "time.", "building", "sequences.", "character-level", "hidden", "unwieldy", "pooling", "introduce", "same", "rnns", "than", "which", "advantages", "alternates", "have", "computation", "classification,", "viability", "variety", "equal"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "52890982": {"id": "52890982", "openalex": null, "doi": null, "title": "ADVERSARIAL AUDIO SYNTHESIS", "abstract": "While Generative Adversarial Networks (GANs) have seen wide success at the problem of synthesizing realistic images, they have seen little application to audio generation. Unlike for images, a barrier to success is that the best discriminative representations for audio tend to be non-invertible, and thus cannot be used to synthesize listenable outputs. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. Our experiments demonstrate that WaveGAN can produce intelligible words from a small vocabulary of speech, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. Qualitatively, we find that human judges prefer the sound quality of generated examples from WaveGAN over those from a method which na\u00efvely apply GANs on image-like audio feature representations.", "authors": [], "concepts": ["demonstrate", "drums,", "problem", "used", "audio.", "na\u00efvely", "method", "they", "such", "feature", "examples", "seen", "generation.", "barrier", "from", "listenable", "synthesis", "sound", "generative", "while", "outputs.", "representations.", "raw-waveform", "tend", "networks", "wide", "thus", "prefer", "judges", "application", "discriminative", "produce", "vocalizations,", "qualitatively,", "other", "experiments", "success", "synthesizing", "synthesize", "unlike", "generated", "wavegan,", "non-invertible,", "gans", "piano.", "domains", "that", "audio", "bird", "those", "images,", "representations", "apply", "first", "paper", "realistic", "unsupervised", "intelligible", "find", "also", "introduce", "best", "quality", "over", "this", "which", "words", "little", "adversarial", "vocabulary", "cannot", "have", "small", "(gans)", "wavegan", "image-like", "human", "applying", "speech,", "attempt"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "52893258": {"id": "52893258", "openalex": null, "doi": null, "title": "ON THE UNIVERSAL APPROXIMABILITY AND COMPLEXITY BOUNDS OF QUANTIZED RELU NEURAL NETWORKS", "abstract": "Compression is a key step to deploy large neural networks on resource-constrained platforms. As a popular compression technique, quantization constrains the number of distinct weight values and thus reducing the number of bits required to represent and store each weight. In this paper, we study the representation power of quantized neural networks. First, we prove the universal approximability of quantized ReLU networks on a wide class of functions. Then we provide upper bounds on the number of weights and the memory size for a given approximation error bound and the bit-width of weights for function-independent and functiondependent structures. Our results reveal that, to attain an approximation error bound of , the number of weights needed by a quantized network is no more than O log 5 (1/ ) times that of an unquantized network. This overhead is of much lower order than the lower bound of the number of weights needed for the error bound, supporting the empirical success of various quantization techniques. To the best of our knowledge, this is the first in-depth study on the complexity bounds of quantized neural networks.1", "authors": [], "concepts": ["each", "bound", "paper,", "structures.", "error", "networks.", "quantized", "large", "deploy", "number", "empirical", "much", "represent", "more", "times", "in-depth", "order", "distinct", "relu", "constrains", "bits", "weight.", "networks", "wide", "thus", "complexity", "popular", "lower", "function-independent", "compression", "functiondependent", "attain", "functions.", "that,", "first,", "reveal", "networks.1", "prove", "memory", "resource-constrained", "network", "quantization", "success", "class", "technique,", "step", "neural", "upper", "given", "network.", "approximability", "supporting", "that", "overhead", "needed", "required", "weights", "platforms.", "weight", "unquantized", "approximation", "bound,", "first", "reducing", "size", "provide", "universal", "results", "bit-width", "best", "various", "then", "than", "power", "this", "bounds", "techniques.", "representation", "study", "knowledge,", "values", "store"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "52898806": {"id": "52898806", "openalex": null, "doi": null, "title": "RIEMANNIAN ADAPTIVE OPTIMIZATION METHODS", "abstract": "Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools \u2212 namely ADAM, ADAGRAD and the more recent AMSGRAD \u2212 remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincar\u00e9 ball.arXiv:1810.00760v1 [cs.LG] 1 Oct 2018Under review as a conference paper at ICLR 2019Our contributions. In this work we (i) explain why generalizing these adaptive schemes to the most agnostic Riemannian setting in an intrinsic manner is compromised, and (ii) propose generalizations of the algorithms together with their convergence analysis in the particular case of a product of manifolds where each manifold represents one \"coordinate\" of the adaptive scheme. Finally, we (iii) empirically support our claims on the realistic task of hyperbolic taxonomy embedding.Our initial motivation. The particular application that motivated us in developing Riemannian versions of ADAGRAD and ADAM was the learning of symbolic embeddings in non-Euclidean spaces. As an example, the GloVe algorithm(Pennington et al., 2014)\u2212 an unsupervised method for learning Euclidean word embeddings capturing semantic/syntactic relationships \u2212 benefits significantly from optimizing with ADAGRAD compared to using SGD, presumably because different words are sampled at different frequencies. Hence the absence of Riemannian adaptive algorithms could constitute a significant obstacle to the development of competitive optimization-based Riemannian embedding methods. In particular, we believe that the recent rise of embedding methods in hyperbolic spaces", "authors": [], "concepts": ["case", "experimentally,", "known", "sgd,", "non-euclidean", "glove", "each", "regret", "most", "particular,", "geodesically", "particular", "believe", "used", "generalizations", "show", "method", "recent", "remain", "algorithms.", "(sgd),", "faster", "such", "certain", "these", "review", "been", "setting,", "were", "embedding", "(iii)", "motivation.", "discuss", "(ii)", "reduced", "adaptive", "more", "intrinsic", "from", "hyperbolic", "optimizing", "however,", "order", "using", "product.", "example,", "schemes", "word", "conference", "compared", "ball.arxiv:1810.00760v1", "task", "propose", "explain", "convergence", "riemannian", "generalization", "convex", "generalizing", "significant", "some", "domain", "manner", "manifolds.", "sense", "because", "popular", "lower", "methods", "contributions.", "work", "adagrad", "finally,", "generalized", "support", "relationships", "learning", "compromised,", "application", "algorithm(pennington", "absence", "gradient", "motivated", "2014)\u2212", "wordnet", "adaptivity", "significantly", "could", "methods.", "manifolds,", "euclidean", "hence", "space", "manifold", "cartesian", "agnostic", "capturing", "competitive", "analysis", "embeddings", "2019our", "rise", "corresponding", "already", "represents", "obstacle", "namely", "semantic/syntactic", "optimization-based", "versions", "difficulty", "across", "with", "train", "standard", "variance", "several", "adapted", "claims", "spaces", "developing", "settings.", "that", "presumably", "their", "tools", "manifolds", "choosing", "\"coordinate\"", "different", "implemented", "initial", "those", "adam,", "empirically", "yields", "first", "symbolic", "baselines", "tight", "value", "paper", "provide", "realistic", "accelerated", "unsupervised", "spaces.", "algorithms", "objectives", "together", "[cs.lg]", "2018under", "adam", "commonly", "al.,", "taxonomy", "same", "where", "then", "over", "this", "scheme.", "which", "words", "product", "amsgrad", "benefits", "bounds", "frequencies.", "loss", "proofs", "development", "have", "iclr", "optimization", "embedding.our", "poincar\u00e9", "sampled", "constitute", "setting", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "52909341": {"id": "52909341", "openalex": null, "doi": null, "title": "NEAR-OPTIMAL REPRESENTATION LEARNING FOR HIERARCHICAL REINFORCEMENT LEARNING", "abstract": "We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -the mapping of observation space to goal space -is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods. 1", "authors": [], "concepts": ["bound", "tasks", "problem", "observation", "continuous-control", "show", "such", "communicating", "solves", "these", "existing", "number", "expected", "lower-level", "using", "accordingly,", "well", "derive", "compared", "optimal", "reinforcement", "learning.", "crucial.", "reach.", "terms", "better", "quantitatively", "problem,", "policies,", "hierarchical", "mapping", "learning", "goal", "goals", "methods.", "space", "practice.", "controller", "policy", "higher-level", "iteratively", "goal-conditioned", "reward", "trained", "approach", "that", "near-optimal", "expressions", "representation,", "-the", "optimized", "representation.", "choice", "representations", "yields", "translated", "difficult", "objectives", "results", "this", "which", "notion", "structures,", "defined", "develop", "qualitatively", "representation", "study", "sub-optimality"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "52909749": {"id": "52909749", "openalex": null, "doi": null, "title": "OPTIMAL COMPLETION DISTILLATION FOR SEQUENCE LEARNING", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters of its own, and does not require pretraining or joint optimization with conditional log-likelihood. Given a partial sequence generated by the model, we first identify the set of optimal suffixes that minimize the total edit distance, using an efficient dynamic programming algorithm. Then, for each position of the generated sequence, we use a target distribution that puts equal probability on the first token of all the optimal suffixes. OCD achieves the state-ofthe-art performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets, achieving 9.3% WER and 4.5% WER respectively.", "authors": [], "concepts": ["achieving", "each", "total", "recognition,", "pretraining", "speech", "efficient", "joint", "does", "procedure", "distance,", "minimize", "performance", "present", "(ocd),", "optimizing", "distance.", "using", "librispeech", "log-likelihood.", "optimal", "models", "achieves", "conditional", "algorithm.", "position", "both", "respectively.", "learning", "probability", "target", "end-to-end", "street", "completion", "puts", "suffixes.", "model,", "based", "own,", "state-ofthe-art", "generated", "training", "sequence,", "with", "hyper-parameters", "identify", "given", "dynamic", "9.3%", "that", "distribution", "programming", "token", "sequence", "partial", "distillation", "first", "datasets,", "wall", "efficient,", "optimization", "edit", "4.5%", "require", "then,", "suffixes", "journal", "equal"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "52920181": {"id": "52920181", "openalex": null, "doi": null, "title": "EPISODIC CURIOSITY THROUGH REACHABILITY", "abstract": "Rewards are sparse in the real world and most today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself -thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward -making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory -which incorporates rich information about environment dynamics. This allows us to overcome the known \"couch-potato\" issues of prior work -when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in VizDoom, DMLab and MuJoCo. In navigational tasks from VizDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only. * Shared first authorship.", "authors": [], "concepts": ["environments", "state-of-the-art", "actions", "learn", "-which", "known", "curiosity", "\"couch-potato\"", "most", "tasks", "particular,", "problem", "observation", "bonus.", "reach", "method", "lead", "inspired", "test", "mujoco.", "information", "exploiting", "such", "visually", "about", "prior", "crucially,", "rewarded", "instantly", "memory.", "icm.", "through", "more", "from", "many", "mujoco,", "compared", "task", "locomotion", "propose", "behaviour", "reinforcement", "dynamics.", "learning.", "dmlab", "reachability", "current", "work", "today's", "create", "learning", "animals,", "-making", "-when", "steps", "novelty", "could", "sparse", "something", "done", "overcome", "memory", "environment", "uses", "sparsity.", "hardly", "novel", "based", "form", "shared", "takes", "curious", "world", "-thus", "learns", "making", "with", "finds", "reward", "struggle", "equipped", "comparison", "dense", "approach", "observing", "outperforms", "determine", "only.", "bonus", "real", "predictable", "episodic", "solution", "those", "consequences.", "itself", "rich", "module", "vizdoom", "summed", "allow", "dmlab,", "first", "allows", "combined", "navigational", "rewards", "algorithms", "possible", "observations", "bonus,", "suitable", "reward.", "this", "which", "issues", "first-person-view", "agent", "incorporates", "authorship.", "gratify", "vizdoom,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "52944914": {"id": "52944914", "openalex": null, "doi": null, "title": "Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks", "abstract": "Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters-typically a multiple of their output dimension-and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.", "authors": [], "concepts": ["combination", "state-of-the-art", "activation,", "each", "parameters", "paper,", "amenable", "decoder:", "particular", "theoretical", "underparameterization", "them", "show", "untrained", "classical", "wavelet-based", "such", "consists", "large", "only", "normalization.", "multiple", "light", "sheds", "number", "highly", "represent", "simplicity", "barrier", "performance", "images", "from", "into", "part", "deep", "channels,", "convolutional", "relu", "structure", "non-convolutional", "representations.", "very", "simple", "propose", "inpainting,", "networks", "dimensionality.", "denoising.", "enable", "networks,", "sense", "measurements.", "contrary", "datasets.", "linear", "analysis,", "problems", "signal", "reconstruction", "solving", "decoder", "output", "ability", "dimension-and", "attributed", "compress", "upsampling", "network", "overfitting,", "success", "parameters.", "model,", "form", "image", "makes", "weights,", "neural", "with", "noisy", "allowing", "including", "trained", "that", "unit,", "concise", "their", "tools", "pixel-wise", "weight", "compressing", "decoder,", "channelwise", "effective", "natural", "need", "representations", "called", "thresholding.", "inverse", "image-generating", "fewer", "parameters-typically", "layer", "than", "this", "which", "provides", "enables", "well.", "have", "aspects", "wavelets,", "convolutions", "architecture", "denoising,", "generate", "further,", "become", "identical"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "52947902": {"id": "52947902", "openalex": null, "doi": null, "title": "textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR", "abstract": "We address two challenges of probabilistic topic modelling in order to better estimate the probability of a word in a given context, i.e., P (word|context) : (1) No language structure in context: Probabilistic topic models ignore word order by summarizing a given context as a \"bag-of-word\" and consequently the semantics of words in the context is lost. In this work, we incorporate language structure by combining a neural autoregressive topic model (TM) (e.g., DocNADE) with a LSTM based language model (LSTM-LM) in a single probabilistic framework. The LSTM-LM learns a vector-space representation of each word by accounting for word order in local collocation patterns, while the TM simultaneously learns a latent representation from the entire document. In addition, the LSTM-LM models complex characteristics of language (e.g., syntax and semantics), while the TM discovers the underlying thematic structure in a collection of documents. We unite two complementary paradigms of learning the meaning of word occurrences by combining a topic model and a language model in a unified probabilistic framework, named as ctx-DocNADE. (2) Limited context and/or smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging. We address this challenge by incorporating external knowledge into neural autoregressive topic models via a language modelling approach: we use word embeddings as input of a LSTM-LM with the aim to improve the word-topic mapping on a smaller and/or short-text corpus. The proposed Doc-NADE extension is named as ctx-DocNADEe. We present novel neural autoregressive topic model variants coupled with neural language models and embeddings priors that consistently outperform state-of-theart generative topic models in terms of generalization (perplexity), interpretability (topic coherence) and applicability (retrieval and classification) over 7 long-text and 8 short-text datasets from diverse domains.", "authors": [], "concepts": ["smaller", "state-of-theart", "each", "unified", "outperform", "\"bag-of-word\"", "model", "corpus.", "short-text", "and/or", "settings", "(retrieval", "prior", "docnade)", "thematic", "number", "variants", "datasets", "probabilistic", "domains.", "from", "into", "present", "deep", "(lstm-lm)", "order", "estimate", "(topic", "generative", "structure", "while", "classification)", "word", "diverse", "meaning", "models", "generalization", "approach:", "syntax", "semantics", "(perplexity),", "context)", "collocation", "consequently", "terms", "better", "incorporating", "entire", "mapping", "learning", "paradigms", "improve", "application", "probability", "ctx-docnadee.", "simultaneously", "documents:", "i.e.,", "short", "long-text", "characteristics", "local", "latent", "priors", "unite", "challenging.", "challenges", "modelling", "text", "novel", "embeddings", "documents.", "based", "doc-nade", "accounting", "combining", "addition,", "(word|context)", "data", "summarizing", "input", "(i.e.,", "context:", "training", "context", "(tm)", "lstm-lm", "neural", "learns", "with", "corpus", "given", "coherence)", "lstm", "applicability", "work,", "that", "document.", "lost.", "complementary", "knowledge", "language", "autoregressive", "sparsity", "patterns,", "address", "lack", "vector-space", "challenge", "(e.g.,", "word-topic", "context,", "coupled", "external", "collection", "occurrences", "topic", "over", "consistently", "this", "words", "interpretability", "framework,", "ignore", "small", "semantics),", "incorporate", "proposed", "framework.", "named", "extension", "underlying", "limited", "contextualized", "compositional", "distributed", "representation", "documents,", "discovers", "texttovec:", "ctx-docnade.", "single", "complex"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "53208122": {"id": "53208122", "openalex": null, "doi": null, "title": "LANGUAGE GANS FALLING SHORT", "abstract": "Traditional natural language generation (NLG) models are trained using maximum likelihood estimation (MLE) which differs from the sample generation inference procedure. During training the ground truth tokens are passed to the model, however, during inference, the model instead reads its previously generated samples -a phenomenon coined exposure bias. Exposure bias was hypothesized to be a root cause of poor sample quality and thus many generative adversarial networks (GANs) were proposed as a remedy since they have identical training and inference. However, many of the ensuing GAN variants validated sample quality improvements but ignored loss of sample diversity. This work reiterates the fallacy of quality-only metrics and clearly demonstrate that the well-established technique of reducing softmax temperature can outperform GANs on a quality-only metric. Further, we establish a definitive quality-diversity evaluation procedure using temperature tuning over local and global sample metrics. Under this, we find that MLE models consistently outperform the proposed GAN variants over the whole quality-diversity space. Specifically, we find that 1) exposure bias appears to be less of an issue than the complications arising from non-differentiable, sequential GAN training; 2) MLE trained models provide a better quality/diversity tradeoff compared to their GAN counterparts, all while being easier to train, easier to cross-validate, and less computationally expensive. 1 * Authors contributed equally 1 Code to reproduce experiments is available at github.com/pclucas14/GansFallingShort", "authors": [], "concepts": ["improvements", "inference", "demonstrate", "diversity.", "outperform", "(nlg)", "root", "cause", "arising", "equally", "model", "available", "they", "tokens", "temperature", "metrics.", "expensive.", "quality-only", "procedure", "fallacy", "were", "train,", "variants", "from", "however,", "using", "(mle)", "generation", "many", "generative", "while", "inference,", "compared", "inference.", "whole", "github.com/pclucas14/gansfallingshort", "global", "issue", "tradeoff", "models", "ignored", "being", "networks", "thus", "cross-validate,", "exposure", "ensuing", "work", "sequential", "better", "bias.", "this,", "hypothesized", "procedure.", "short", "local", "computationally", "previously", "experiments", "estimation", "clearly", "poor", "specifically,", "traditional", "passed", "model,", "validated", "code", "differs", "well-established", "training;", "generated", "coined", "since", "training", "establish", "contributed", "phenomenon", "less", "ground", "bias", "space.", "gans", "trained", "tuning", "that", "instead", "quality-diversity", "reiterates", "their", "definitive", "quality/diversity", "easier", "truth", "language", "maximum", "softmax", "natural", "appears", "technique", "remedy", "reproduce", "falling", "reducing", "non-differentiable,", "provide", "complications", "find", "metrics", "metric.", "reads", "sample", "evaluation", "than", "quality", "over", "consistently", "this", "which", "samples", "during", "adversarial", "loss", "authors", "have", "proposed", "(gans)", "likelihood", "under", "counterparts,", "further,", "identical"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "53327717": {"id": "53327717", "openalex": null, "doi": null, "title": "DEEP FRANK-WOLFE FOR NEURAL NETWORK OPTIMIZATION", "abstract": "Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while converging faster. The code is publicly available at", "authors": [], "concepts": ["yielding", "challenging", "demonstrate", "unseen", "each", "solutions", "problem", "non-smooth", "hyper-parameter.", "snli", "non-convex", "recently", "rate", "show", "method", "superiority", "available", "problem:", "pass", "large", "only", "requiring", "number", "variants", "computes", "bpgrad", "adaptive", "good", "performance", "amsgrad.", "leverage", "present", "deep", "however,", "composite", "well", "offers", "while", "faster.", "worlds:", "schedule.", "simple", "hand-designed", "requires", "optimal", "nature", "time-step.", "generalization", "tend", "convex", "networks", "iteration", "significant", "current", "both", "furthermore,", "learning", "direction", "converging", "frank-wolfe", "gradient", "solving", "sgd.", "produce", "rely", "step-size", "experiments", "(sgd)", "network", "adagrad,", "terms.", "specifically,", "cost", "rate.", "based", "code", "cifar", "powerful", "proximal", "addition,", "data", "employ", "neural", "with", "given", "less", "design.", "approach", "that", "sets,", "closed-form", "minimization", "network,", "adam,", "generalize", "backward", "empirically", "high-dimensional,", "svm,", "yields", "schedule", "computational", "algorithms", "best", "exploits", "same", "where", "publicly", "than", "over", "which", "provides", "algorithm", "framework,", "proposed", "optimization", "practice", "similar", "compositional", "compare", "(fw)", "further", "variants.", "schedule,", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "53729760": {"id": "53729760", "openalex": null, "doi": null, "title": "GAN DISSECTION: VISUALIZING AND UNDERSTANDING GENERATIVE ADVERSARIAL NETWORKS", "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models * . * Interactive demos, video, code, and data are available at GitHub and gandissect.", "authors": [], "concepts": ["researchers", "improvements", "stability.", "what", "understand", "impressive", "visualized", "github", "contextual", "choices", "recently", "object", "interactively", "show", "group", "code,", "available", "they", "scene-level.", "segmentation-based", "interactive", "understood.", "achieved", "such", "these", "insights", "does", "been", "variants", "examine", "represent", "interpretable", "from", "into", "interventions", "present", "however,", "comparing", "learning?", "using", "surroundings", "well", "many", "generative", "applications,", "open", "objects", "relationship", "framework", "models", "answering", "video,", "quantify", "networks", "dissection", "enable", "affect", "dissection:", "causes", "better", "architectural", "measuring", "artifact-causing", "results?", "ability", "could", "applications", "units", "source", "understanding", "network", "visualize", "real-world", "control", "emerged", "visualizing", "data", "training", "method.", "world", "across", "with", "units,", "identify", "interpretation", "several", "locating", "questions", "gans", "work,", "that", "related", "between", "their", "internally?", "scene.", "tools", "discovered", "different", "analytic", "enabled", "layers,", "representations", "first", "datasets,", "manipulating", "causal", "closely", "provide", "help", "practical", "concepts", "object-,", "results", "demos,", "output.", "internal", "images.", "artifacts", "sample", "quality", "models.", "this", "practitioners", "framework,", "improving", "adversarial", "removing", "develop", "have", "inserting", "(gans)", "visual", "effect", "gandissect.", "then,", "models,", "unit-,"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "53951481": {"id": "53951481", "openalex": null, "doi": null, "title": "CAVEATS FOR INFORMATION BOTTLENECK IN DETERMINISTIC SCENARIOS", "abstract": "Information bottleneck (IB) is a method for extracting information from one random variable X that is relevant for predicting another random variable Y . To do so, IB identifies an intermediate \"bottleneck\" variable T that has low mutual information I(X; T ) and high mutual information I(Y ; T ). The IB curve characterizes the set of bottleneck variables that achieve maximal I(Y ; T ) for a given I(X; T ), and is typically explored by maximizing the IB Lagrangian, I(Y ; T ) \u2212 \u03b2I(X; T ). In some cases, Y is a deterministic function of X, including many classification problems in supervised learning where the output class Y is a deterministic function of the input X. We demonstrate three caveats when using IB in any situation where Y is a deterministic function of X: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of \u03b2; (2) there are \"uninteresting\" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal. We also show that when Y is a small perturbation away from being a deterministic function of X, these three caveats arise in an approximate way. To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases. We demonstrate the three caveats on the MNIST dataset. * Published as a conference paper at ICLR 2019 where \u03b2 \u2208 [0, 1] is a parameter that controls the trade-off between compression and prediction. The advantage of optimizing L \u03b2 IB is that it avoids the non-linear constraint in Eq. (1) 1 . Several recent papers have drawn connections between IB and supervised learning, in particular classification using neural networks. In this context, X represents input vectors, Y represents the output classes, and T represents intermediate representations used by the network architecture, such as the activity of hidden layer(s)(Tishby & Zaslavsky, 2015). Some of these papers modify neural", "authors": [], "concepts": ["demonstrate", "(ib)", "typically", "classes,", "learning,", "perturbation", "solutions", "problem", "particular", "approximate", "recover", "used", "show", "trivial", "method", "recent", "lagrangian", "networks.", "information", "avoids", "such", "these", "layers", "papers", "classification", "controls", "from", "curve;", "variable", "vectors,", "optimizing", "using", "2015).", "many", "function", "error,", "conference", "i(x;", "propose", "there", "curve", "being", "bottleneck", "some", "maximizing", "advantage", "\u03b2i(x;", "away", "exhibit", "random", "cases.", "mnist", "contrary", "compression", "functional", "learning", "that,", "situation", "zaslavsky,", "caveats", "problems", "output", "non-linear", "activity", "high", "network", "explored", "recovered", "when", "class", "maximal", "points", "unlike", "predicting", "represents", "way.", "input", "achieve", "neural", "published", "including", "given", "deterministic", "several", "identifies", "\"uninteresting\"", "scenarios", "\"bottleneck\"", "that", "between", "constraint", "strict", "extracting", "different", "prediction", "intermediate", "representations", "trade-off", "cases,", "connections", "address", "paper", "prediction,", "parameter", "three", "multi-layer", "hidden", "context,", "layer(s)(tishby", "characterizes", "prediction.", "also", "where", "dataset.", "this", "lagrangian,", "drawn", "architecture,", "cannot", "classifiers", "have", "modify", "small", "iclr", "proposal.", "variables", "relevant", "arise", "supervised", "(1),", "mutual", "values", "another"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "57825721": {"id": "57825721", "openalex": null, "doi": null, "title": "ON THE TURING COMPLETENESS OF MODERN NEURAL NETWORK ARCHITECTURES", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "authors": [], "concepts": ["fully", "access", "most", "particular,", "show", "complete", "momentum", "recurrent", "reveals", "these", "been", "(kaiser", "sutskever,", "alternatives", "requires", "models", "some", "data.", "relevance,", "both", "networks,", "explored.", "convolutions,", "paradigmatic", "exclusively", "completeness", "memory", "network", "minimal", "exemplifying", "based", "gaining", "input", "neural", "turing", "(vaswani", "dense", "capacity", "results.", "2016).", "needed", "their", "complete.", "transformer", "processing", "sets", "representations", "attention", "neither", "spite", "architectures", "computational", "mechanisms:", "sequences.", "al.,", "internal", "also", "external", "power", "elements", "2017)", "have", "study", "properties", "compute", "modern", "obtain", "become"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "5834589": {"id": "5834589", "openalex": null, "doi": null, "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA", "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32-512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap. Published as a conference paper at ICLR 2017 These methods minimize the objective function f by iteratively taking steps of the form: J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. . Weak sharp minima and penalty functions in mathematical programming. PhD thesis, University of Cambridge, 1988.Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting.", "authors": [], "concepts": ["poorer", "generalization.", "data,", "cause", "duchi,", "inherent", "method", "observed", "lead", "programming.", "fraction", "weak", "view", "larger", "these", "been", "hybrid", "discuss", "variants", "minimize", "adaptive", "singer.", "friedlander", "present", "learning:", "deep", "mark", "strategies", "using", "held", "points,", "well", "many", "function", "conference", "hazan,", "wherein", "there", "generalization", "tend", "methods", "support", "learning", "cambridge,", "online", "gradient", "form:", "steps", "ability", "large-batch", "experiments", "(sgd)", "numerical", "minimizers,", "model,", "degradation", "when", "subgradient", "deterministic-stochastic", "regime", "supports", "gradient.", "mathematical", "data", "training", "generalize.", "iteratively", "tasks.", "published", "gap.", "several", "that", "functions", "estimation.", "optimization.", "noise", "testing", "converge", "sharp", "flat", "approximation", "choice", "contrast,", "eliminate", "small-batch", "operate", "paper", "measured", "objective", "batch", "help", "algorithms", "investigate", "commonly", "fitting.", "schmidt.", "functions-and", "quality", "consistently", "this", "taking", "evidence", "thesis,", "known,", "penalty", "university", "iclr", "drop", "1988.michael", "practice", "minimizers", "sampled", "32-512", "minima", "compute", "attempt", "stochastic", "descent"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "58554701": {"id": "58554701", "openalex": null, "doi": null, "title": "VARIATIONAL SMOOTHING IN RECURRENT NEURAL NETWORK LANGUAGE MODELS", "abstract": "We present a new theoretical perspective of data noising in recurrent neural network language models(Xie et al., 2017). We show that each variant of data noising is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). We use this insight to propose a more principled method to apply at prediction time and propose natural extensions to data noising under the variational framework. In particular, we propose variational smoothing with tied input and output embedding matrices and an element-wise variational smoothing method. We empirically verify our analysis on two benchmark language modeling datasets and demonstrate performance improvements over existing data noising methods.", "authors": [], "concepts": ["improvements", "demonstrate", "each", "variational", "particular,", "particular", "theoretical", "extensions", "show", "verify", "method", "principled", "recurrent", "modeling", "such", "existing", "embedding", "datasets", "more", "performance", "from", "present", "propose", "models", "networks", "derived", "models(xie", "bayesian", "output", "methods.", "perspective", "noising", "network", "analysis", "data", "input", "(i.e.,", "method.", "neural", "with", "corpus", "benchmark", "2017).", "that", "distribution", "weights", "language", "element-wise", "prediction", "unigram", "natural", "apply", "empirically", "distribution).", "statistics", "variant", "gaussians", "al.,", "instance", "over", "this", "insight", "time", "mixture", "framework.", "under", "depend", "matrices", "tied", "whose", "smoothing"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "58981389": {"id": "58981389", "openalex": null, "doi": null, "title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models. *", "authors": [], "concepts": ["inference", "demonstrate", "happen,", "approximated", "feed-forward", "purpose", "succeed", "cases", "recurrent", "networks.", "empirically,", "these", "light", "much", "unstable", "well", "many", "fundamental", "thorough", "explain", "models", "networks", "conduct", "both", "happens,", "made", "counterparts", "learning", "gradient", "theoretically,", "prove", "investigation", "bearing", "stable", "stability", "shed", "training", "tasks.", "neural", "systems,", "descent.", "benchmark", "regime.", "work,", "their", "date", "findings", "dynamical", "replacing", "effective", "sequence", "help", "together,", "perform", "moreover,", "results", "models.", "power", "this", "property", "practitioners", "taken", "little", "often", "suggest", "practice"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "59317031": {"id": "59317031", "openalex": null, "doi": null, "title": "FIXUP INITIALIZATION: RESIDUAL LEARNING WITHOUT NORMALIZATION", "abstract": "Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation. Despite the enormous empirical success of training deep networks with normalization, and recent progress on understanding the working of batch normalization(Santurkar et al., 2018), there is currently no general consensus on why these normalization techniques help training residual neural networks. Intrigued by this topic, in this work we study (i) without normalization, can a deep residual network be trained reliably? (And if so,) (ii) without normalization, can a deep residual network be trained with the same learning rate, converge at the same speed, and generalize equally well (or even better)?Perhaps surprisingly, we find the answers to both questions are Yes. In particular, we show:\u2022 Why normalization helps training. We derive a lower bound for the gradient norm of a residual network at initialization, which explains why with standard initializations, normalization techniques are essential for training deep residual networks at maximal learning rate. (Section 2) * Work done at Facebook. Equal contribution. \u2020 Work done at Facebook. Equal contribution. \u2021 Work done at Facebook.", "authors": [], "concepts": ["state-of-the-art", "essential", "widely", "machine", "bound", "yes.", "generalization,", "fixed-update", "-even", "particular,", "problem", "working", "equally", "norm", "recent", "they", "techniques", "networks.", "regularization,", "these", "normalization.", "exploding", "despite", "layers", "general", "(ii)", "empirical", "classification", "rescaling", "performance", "(and", "speed,", "deep", "topic,", "staple", "well", "surprisingly,", "derive", "reason", "consensus", "better)?perhaps", "propose", "2018),", "convergence", "there", "networks", "initialization:", "enable", "both", "furthermore,", "lower", "work", "unique", "none", "learning", "normalization,", "improve", "perceived", "gradient", "motivated", "solving", "rate,", "normalization(santurkar", "without", "currently", "intrigued", "done", "architectures.", "initialization.", "understanding", "stable", "network", "success", "specifically,", "proper", "rate.", "vanishing", "image", "maximal", "achieve", "training", "topic.", "answers", "show:\u2022", "facebook.", "neural", "with", "explains", "standard", "questions", "trained", "active", "work,", "10,000", "training,", "commonly-held", "that", "training.", "initializations,", "their", "reliably?", "converge", "initialization,", "(section", "normalization", "higher", "(fixup),", "residual", "contribution.", "effectiveness", "so,)", "generalize", "accelerate", "layers.", "research", "showing", "batch", "believed", "challenge", "help", "find", "progress", "stabilize", "al.,", "same", "properly", "fixup", "this", "which", "still", "beliefs", "initialization", "enables", "benefits", "enormous", "translation.", "even", "study", "beginning", "helps", "equal", "though"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "60441438": {"id": "60441438", "openalex": null, "doi": null, "title": "LEARNING WHAT YOU CAN DO BEFORE DOING ANYTHING", "abstract": "Intelligent agents can learn to represent the action spaces of other agents simply by observing them act.Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences.In this work, we address the problem of learning an agent's action space purely from visual observation.We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content.We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions.We call the full model with composable action representations Composable Learned Action Space Predictor (CLASP) .We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings.When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels. 1 * Equal contribution.Ordering determined by a coin flip. 1 Project website: https://daniilidis-group.github.io/learned_action_spaces", "authors": [], "concepts": ["effects", "actions", "learn", "static", "fully", "what", "term", "action", "act.such", "captures", "tasks", "composability", "problem", "https://daniilidis-group.github.io/learned_action_spaces", "model", "used", "them", "show", "method", "settings", "intelligent", "(clasp)", "such", "sequences", "existing", "requiring", "setting,", "purely", "represent", "observation.we", "from", "variable", "predict", "leads", "magnitude", "before", "structure", "while", "agents", "capture", "labels.", "being", "contribution.ordering", "doing", "full", "methods", "plan", "video", "learning", "content.we", "quickly", "orders", "flip.", "minimally", "latent", "other", "space", "environment", "network", "potential", "semi-supervised", "website:", "anything", "sequences.in", "with", "sensitive", "space,", "coin", "complex,", "scene's", "disentangle", "composable", "project", "call", "applicability", "spaces", "actions.we", "work,", "agent's", "observing", "that", "comparably", "determined", "their", "action-conditioned", "encourages", "prediction", "learned", "representations", "settings.when", "simply", "address", "realistic", "help", "fewer", "perform", "synthetic", "introduce", "dynamics", "this", "predictor", "loss", "visual", "planning", "supervised", "equal", "complex", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "67855552": {"id": "67855552", "openalex": null, "doi": null, "title": "ON THE SENSITIVITY OF ADVERSARIAL ROBUSTNESS TO INPUT DATA DISTRIBUTIONS", "abstract": "Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve comparable clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon.A semantically-lossless shift on the data distribution could result in a drastically different robustness for adversarially trained models.Note that this is different from the transferability of a fixed model that is trained on one data distribution but tested on another distribution. Even retraining the model on the new data distribution may give us a completely different adversarial robustness on the same new distribution. This is also in sharp contrast to the clean accuracy of standard training, which, as we show in later sections, is insensitive to such shifts. To our best knowledge, our paper is the first work in the literature that demonstrates such sensitivity.", "authors": [], "concepts": ["demonstrate", "transformations", "understand", "paper,", "implications", "comparable", "vulnerable", "semantically-lossless", "most", "robust", "tasks", "cause", "behaviors", "model", "insensitive", "show", "method", "literature", "respectively,", "later", "alone", "such", "existing", "about", "discuss", "empirical", "variants", "training:", "contrast", "evaluated", "from", "them,", "investigations", "shifts.", "counter-intuitive", "clean", "bayes", "accuracy", "models", "networks", "affect", "both", "adversarially", "networks,", "transferability", "popular", "work", "mnist", "retraining", "necessarily", "significantly", "could", "focused", "completely", "understanding", "sensitivity", "based", "unlike", "data", "input", "achieve", "accuracy,", "lastly,", "training", "phenomenon", "neural", "perturbations.", "sensitive", "robustness,", "standard", "evaluating", "trained", "cifar10", "themselves.", "attempts", "disentangles", "training,", "shift", "mitigating", "which,", "that", "distribution", "finding.", "fixed", "semantics-preserving", "sharp", "different", "initial", "robustness", "models.note", "learned", "distributions", "demonstrates", "make", "result", "sensitivity.", "literature,", "first", "indicates", "phenomenon.a", "accuracies", "paper", "vulnerability", "standardly", "practical", "discovery", "also", "best", "same", "accuracies.", "models.", "tested", "sections,", "this", "which", "confirm", "intriguing", "adversarial", "give", "largely", "small", "semantically-identical", "even", "drastically", "study", "classifier.", "knowledge,", "further", "complex", "another", "construct", "distribution."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "67856213": {"id": "67856213", "openalex": null, "doi": null, "title": "GANSYNTH: ADVERSARIAL NEURAL AUDIO SYNTHESIS", "abstract": "Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure but have slow iterative sampling and lack global latent structure. In contrast, Generative Adversarial Networks (GANs) have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts. 1", "authors": [], "concepts": ["demonstrate", "machine", "outperform", "model", "domain.", "inherently", "modeling", "efficient", "faster", "such", "empirical", "high-fidelity", "through", "able", "waveform", "sampling", "investigations", "magnitudes", "synthesis", "frequencies", "iterative", "waveforms.", "magnitude", "generative", "structure", "sampling,", "fine-scale", "resolution", "dataset,", "frequency", "spectral", "global", "networks", "task,", "parallel", "both", "learning", "fact", "perception", "wavenet", "orders", "local", "latent", "slow", "neural", "with", "metrics,", "sufficient", "sensitive", "struggle", "several", "instantaneous", "gans", "conditioning", "gansynth:", "that", "their", "audio", "strong", "autoregressive", "counterparts.", "locally-coherent", "contrast,", "automated", "efficiently", "baselines", "lack", "structure.", "difficult", "herein,", "wavenet,", "evaluation", "than", "coherence.", "adversarial", "have", "(gans)", "nsynth", "generate", "models,", "human", "extensive"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "7167114": {"id": "7167114", "openalex": null, "doi": null, "title": "DEEP VARIATIONAL INFORMATION BOTTLENECK", "abstract": "We present a variational approximation to the information bottleneck ofTishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.", "authors": [], "concepts": ["outperform", "variational", "model", "show", "parameterize", "method", "information", "regularization,", "efficient", "performance", "leverage", "present", "deep", "using", "forms", "models", "generalization", "attack.", "bottleneck", "trick", "terms", "other", "network", "\"deep", "(1999).", "neural", "with", "trained", "call", "approach", "that", "training.", "bottleneck\",", "those", "robustness", "approximation", "allows", "objective", "reparameterization", "this", "adversarial", "oftishby", "vib."], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "8394195": {"id": "8394195", "openalex": null, "doi": null, "title": "LOSSY IMAGE COMPRESSION WITH COMPRESSIVE AUTOENCODERS", "abstract": "We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.", "authors": [], "concepts": ["architectures,", "high-resolution", "here", "problem", "expensive", "recently", "show", "inherent", "sub-pixel", "approximations,", "furthermore", "efficient", "methods,", "technology,", "existing", "contrast", "compressive", "more", "codecs.", "deep", "optimizing", "using", "flexible", "well", "diverse", "propose", "coarser", "outperforming", "optimize", "compression.", "work", "create", "compression", "jpeg", "computationally", "thanks", "network", "changes", "minimal", "types", "competitive", "based", "image", "content", "makes", "potential", "focusing", "with", "media", "train", "sufficient", "lossy", "approach", "that", "non-differentiabilty", "previous", "hardware", "autoencoders", "need", "approaches", "rnns.", "address", "formats,", "difficult", "algorithms", "changing", "requirements", "images.", "suitable", "than", "this", "which", "need,", "loss.", "architecture,", "loss", "have", "directly", "small", "proposed", "shallower"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "86393936": {"id": "86393936", "openalex": null, "doi": null, "title": "SPECTRAL INFERENCE NETWORKS: UNIFYING DEEP AND SPECTRAL LEARNING", "abstract": "We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or graph-structured data. We cast training Spectral Inference Networks as a bilevel optimization problem, which allows for online learning of multiple eigenfunctions. We show results of training Spectral Inference Networks on problems in quantum mechanics and feature learning for videos on synthetic datasets. Our results demonstrate that Spectral Inference Networks accurately recover eigenfunctions of linear operators and can discover interpretable representations from video in a fully unsupervised manner.", "authors": [], "concepts": ["inference", "demonstrate", "fully", "variational", "graph-structured", "recover", "monte", "show", "eigenfunctions.", "they", "videos", "carlo", "symmetric", "feature", "tool", "multiple", "interpretable", "from", "present", "deep", "spectral", "framework", "networks", "data.", "unifying", "networks,", "methods", "datasets.", "problem,", "linear", "video", "learning", "problems", "online", "analysis", "slow", "powerful", "operators,", "mechanics", "training", "physics.", "operators", "quantum", "networks:", "that", "related", "optimization.", "bilevel", "cast", "such,", "representations", "generalize", "closely", "allows", "unsupervised", "computational", "results", "synthetic", "accurately", "generic", "eigenfunctions", "which", "discover", "optimization", "representation", "manner.", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "9725544": {"id": "9725544", "openalex": null, "doi": null, "title": "Towards Metamerism via Foveated Style Transfer", "abstract": "Given the recent successes of deep learning applied to style transfer and texture synthesis, we propose a new theoretical framework to construct visual metamers: a family of perceptually identical, yet physically different images. We review work both in neuroscience related to metameric stimuli, as well as computer vision research in style transfer. We propose our NeuroFovea metamer model that is based on a mixture of peripheral representations and style transfer forward-pass algorithms for any image from the recent work of Adaptive Instance Normalization (Huang & Belongie). Our model is parametrized by a VGG-Net versus a set of joint statistics of complex wavelet coefficients which allows us to encode images in high dimensional space and interpolate between the content and texture information. We empirically show that human observers discriminate our metamers at a similar rate as the metamers of Freeman & Simoncelli (FS) In addition, our NeuroFovea metamer model gives us the benefit of near real-time generation which presents a \u00d71000 speed-up compared to previous work. Critically, psychophysical studies show that both the FS and NeuroFovea metamers are discriminable from the original images highlighting an important limitation of current metamer generation methods.", "authors": [], "concepts": ["\u00d71000", "computer", "identical,", "real-time", "metamer", "studies", "belongie).", "metamers:", "theoretical", "model", "synthesis,", "gives", "rate", "show", "wavelet", "recent", "speed-up", "joint", "review", "forward-pass", "adaptive", "images", "from", "deep", "neurofovea", "generation", "metamers", "well", "discriminate", "towards", "compared", "applied", "framework", "propose", "psychophysical", "metameric", "simoncelli", "current", "transfer", "both", "work", "peripheral", "information.", "freeman", "highlighting", "learning", "texture", "coefficients", "work.", "methods.", "space", "high", "family", "based", "dimensional", "image", "content", "addition,", "vision", "perceptually", "given", "physically", "stimuli,", "that", "related", "vgg-net", "between", "parametrized", "benefit", "previous", "metamerism", "successes", "different", "normalization", "presents", "representations", "empirically", "foveated", "critically,", "versus", "statistics", "allows", "research", "algorithms", "transfer.", "discriminable", "style", "observers", "images.", "encode", "instance", "which", "neuroscience", "(fs)", "near", "mixture", "(huang", "limitation", "visual", "original", "similar", "important", "interpolate", "human", "complex", "construct"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}, "997870": {"id": "997870", "openalex": null, "doi": null, "title": "DIVIDE-AND-CONQUER REINFORCEMENT LEARNING", "abstract": "Standard model-free deep reinforcement learning (RL) algorithms sample a new initial state for each trial, allowing them to optimize policies that can perform well even in highly stochastic environments. However, problems that exhibit considerable initial state variation typically produce high-variance gradient estimates for model-free RL, making direct policy or value function optimization challenging. In this paper, we develop a novel algorithm that instead optimizes an ensemble of policies, each on a different \"slice\" of the initial state space, and gradually unifies them into a single policy that can succeed on the whole state space. This approach, which we term divide-and-conquer RL, is able to solve complex tasks where conventional deep RL methods are ineffective. Our results show that divide-and-conquer RL greatly outperforms conventional policy gradient methods on challenging grasping, manipulation, and locomotion tasks, and exceeds the performance of a variety of prior methods. Videos of policies learned by our algorithm can be viewed here.", "authors": [], "concepts": ["challenging", "each", "typically", "paper,", "term", "tasks", "approach,", "exceeds", "them", "show", "succeed", "videos", "conventional", "prior", "here.", "highly", "able", "performance", "into", "deep", "however,", "well", "function", "whole", "locomotion", "unifies", "viewed", "reinforcement", "estimates", "solve", "exhibit", "optimize", "methods", "(rl)", "policies,", "learning", "manipulation,", "variation", "problems", "gradient", "model-free", "methods.", "produce", "considerable", "challenging.", "novel", "policy", "making", "allowing", "direct", "environments.", "space,", "standard", "high-variance", "space.", "\"slice\"", "outperforms", "that", "ensemble", "instead", "ineffective.", "divide-and-conquer", "different", "initial", "trial,", "gradually", "learned", "grasping,", "greatly", "value", "policies", "algorithms", "optimizes", "perform", "results", "where", "sample", "tasks,", "this", "which", "state", "algorithm", "develop", "optimization", "even", "variety", "single", "complex", "stochastic"], "topics": [], "year": null, "cited_by_count": 0, "publication_date": null, "type": null, "language": null, "is_retracted": false, "is_paratext": false}}
- 1 5.703782474656201 0
-1 1 5.703782474656201 12
-10 1 5.703782474656201 24
-5 2 5.0106352940962555 36
-a 3 4.605170185988092 56
-achieving 2 5.0106352940962555 84
-algorithm 2 5.0106352940962555 104
-allow 2 5.0106352940962555 124
-an 2 5.0106352940962555 144
-approximation 2 5.0106352940962555 164
-block-wise 2 5.0106352940962555 184
-ciphers 2 5.0106352940962555 204
-close 2 5.0106352940962555 224
-divergence 2 5.0106352940962555 244
-employing 2 5.0106352940962555 264
-even 2 5.0106352940962555 284
-i.e 2 5.0106352940962555 304
-is 1 5.703782474656201 324
-it 1 5.703782474656201 336
-large 2 5.0106352940962555 348
-making 2 5.0106352940962555 368
-meaning 2 5.0106352940962555 388
-medfair 2 5.0106352940962555 408
-metrizable 2 5.0106352940962555 428
-mini-batch 2 5.0106352940962555 448
-models 2 5.0106352940962555 468
-norm 2 5.0106352940962555 488
-on 2 5.0106352940962555 508
-shifting 2 5.0106352940962555 528
-sub-gaussian 2 5.0106352940962555 548
-subgraph 2 5.0106352940962555 568
-supervised 2 5.0106352940962555 588
-th 2 5.0106352940962555 608
-that 2 5.0106352940962555 628
-the 6 3.912023005428146 648
-thus 2 5.0106352940962555 700
-when 2 5.0106352940962555 720
-which 2 5.0106352940962555 740
-without 2 5.0106352940962555 760
-yet 2 5.0106352940962555 780
. 593 -0.6814119243415244 800
... 2 5.0106352940962555 5548
.09272 2 5.0106352940962555 5568
.10 1 5.703782474656201 5588
.3 1 5.703782474656201 5600
.8 1 5.703782474656201 5612
.a 2 5.0106352940962555 5624
.arxiv 2 5.0106352940962555 5644
.different 2 5.0106352940962555 5664
.during 2 5.0106352940962555 5684
.each 2 5.0106352940962555 5704
.experimentwe 2 5.0106352940962555 5724
.figure 2 5.0106352940962555 5744
.given 2 5.0106352940962555 5764
.here 2 5.0106352940962555 5784
.however 2 5.0106352940962555 5804
.in 6 3.912023005428146 5824
.interesting 2 5.0106352940962555 5876
.our 2 5.0106352940962555 5896
.promptattack 2 5.0106352940962555 5916
.related 2 5.0106352940962555 5936
.sdes 2 5.0106352940962555 5956
.step 2 5.0106352940962555 5976
.such 2 5.0106352940962555 5996
.the 2 5.0106352940962555 6016
.there 2 5.0106352940962555 6036
.these 2 5.0106352940962555 6056
.to 2 5.0106352940962555 6076
.we 15 2.995732273553991 6096
.while 2 5.0106352940962555 6220
0 4 4.31748811353631 6240
0. 1 5.703782474656201 6276
0.0 2 5.0106352940962555 6288
0.05 2 5.0106352940962555 6308
0.2. 2 5.0106352940962555 6328
0.3 2 5.0106352940962555 6348
0.75 2 5.0106352940962555 6368
0.84 2 5.0106352940962555 6388
0.9 1 5.703782474656201 6408
000 14 3.0647251450409425 6420
048 1 5.703782474656201 6536
1 112 0.9852836033611064 6548
1-76 2 5.0106352940962555 7448
1-dimensional 2 5.0106352940962555 7468
1-hop 2 5.0106352940962555 7488
1-lipschitz 2 5.0106352940962555 7508
1-wl 2 5.0106352940962555 7528
1. 4 4.31748811353631 7548
1.0 2 5.0106352940962555 7584
1.1 2 5.0106352940962555 7604
1.16 2 5.0106352940962555 7624
1.2. 2 5.0106352940962555 7644
1.3b 2 5.0106352940962555 7664
1.5b 2 5.0106352940962555 7684
1.6 2 5.0106352940962555 7704
1.6m 2 5.0106352940962555 7724
1.71 2 5.0106352940962555 7744
1.87 2 5.0106352940962555 7764
1.concept 2 5.0106352940962555 7784
10 13 3.138833117194664 7804
10.5 2 5.0106352940962555 7912
100 5 4.0943445622221 7932
100-step 2 5.0106352940962555 7976
1000 2 5.0106352940962555 7996
1024 2 5.0106352940962555 8016
10x 1 5.703782474656201 8036
11 6 3.912023005428146 8048
11.0 2 5.0106352940962555 8100
12 4 4.31748811353631 8120
12.2 2 5.0106352940962555 8156
121 1 5.703782474656201 8176
124-131 2 5.0106352940962555 8188
128 2 5.0106352940962555 8208
129 2 5.0106352940962555 8228
13 9 3.506557897319982 8248
130 2 5.0106352940962555 8324
131 2 5.0106352940962555 8344
1312.3005 2 5.0106352940962555 8364
1312.3005.kevin 2 5.0106352940962555 8384
132 2 5.0106352940962555 8404
133 2 5.0106352940962555 8424
134 2 5.0106352940962555 8444
135 2 5.0106352940962555 8464
136 2 5.0106352940962555 8484
137 2 5.0106352940962555 8504
14 10 3.4011973816621555 8524
144 1 5.703782474656201 8608
148m-parameters 2 5.0106352940962555 8620
15 8 3.624340932976365 8640
15.5k 2 5.0106352940962555 8708
1506.07540 2 5.0106352940962555 8728
1554 1 5.703782474656201 8748
16 15 2.995732273553991 8760
16-bit 2 5.0106352940962555 8884
1611.02779 2 5.0106352940962555 8904
1611.04231 2 5.0106352940962555 8924
16b 1 5.703782474656201 8944
16dan 2 5.0106352940962555 8956
17 7 3.7578723256008875 8976
17.9 2 5.0106352940962555 9036
1710.09412 2 5.0106352940962555 9056
1710.10368 2 5.0106352940962555 9076
1711.00501 2 5.0106352940962555 9096
175 1 5.703782474656201 9116
175b 2 5.0106352940962555 9128
18 8 3.624340932976365 9148
1802.05957 2 5.0106352940962555 9216
1802.08246 2 5.0106352940962555 9236
1806.02450 2 5.0106352940962555 9256
1806.03107v3 2 5.0106352940962555 9276
1810.00760v1 2 5.0106352940962555 9296
1810.12715 2 5.0106352940962555 9316
1811.08888 2 5.0106352940962555 9336
1812.04606 2 5.0106352940962555 9356
1854 2 5.0106352940962555 9376
19 7 3.7578723256008875 9396
1904.00687 2 5.0106352940962555 9456
1905 1 5.703782474656201 9476
1905. 1 5.703782474656201 9488
1905.09922 2 5.0106352940962555 9500
1905arxiv 2 5.0106352940962555 9520
1906.05827 2 5.0106352940962555 9540
1913.william 2 5.0106352940962555 9560
193 1 5.703782474656201 9580
1980.leonhard 2 5.0106352940962555 9592
1982 2 5.0106352940962555 9612
1982.david 2 5.0106352940962555 9632
1983 2 5.0106352940962555 9652
1984 2 5.0106352940962555 9672
1985 2 5.0106352940962555 9692
1988 2 5.0106352940962555 9712
1988.michael 2 5.0106352940962555 9732
1989 2 5.0106352940962555 9752
1990. 2 5.0106352940962555 9772
1991.serban 2 5.0106352940962555 9792
1994 2 5.0106352940962555 9812
1995 2 5.0106352940962555 9832
1996. 2 5.0106352940962555 9852
1998 2 5.0106352940962555 9872
1999 4 4.31748811353631 9892
1and 2 5.0106352940962555 9928
1b 1 5.703782474656201 9948
1d 1 5.703782474656201 9960
1e 1 5.703782474656201 9972
1figure 2 5.0106352940962555 9984
1illustrates 2 5.0106352940962555 10004
1shows 2 5.0106352940962555 10024
1st 1 5.703782474656201 10044
2 64 1.5448993912965292 10056
2-10x 2 5.0106352940962555 10572
2-d 3 4.605170185988092 10592
2-d. 2 5.0106352940962555 10620
2-layer 4 4.31748811353631 10640
2. 2 5.0106352940962555 10676
2.17 2 5.0106352940962555 10696
2.2 2 5.0106352940962555 10716
2.3 4 4.31748811353631 10736
2.59 2 5.0106352940962555 10772
2.7 2 5.0106352940962555 10792
2.73 2 5.0106352940962555 10812
2.7b 2 5.0106352940962555 10832
2.9 1 5.703782474656201 10852
20 13 3.138833117194664 10864
20-60 2 5.0106352940962555 10972
20.2 2 5.0106352940962555 10992
200 3 4.605170185988092 11012
2000 1 5.703782474656201 11040
2002 4 4.31748811353631 11052
2003 5 4.0943445622221 11088
2003. 1 5.703782474656201 11132
2003.10555 2 5.0106352940962555 11144
2005.09288 2 5.0106352940962555 11164
2006 1 5.703782474656201 11184
2006. 1 5.703782474656201 11196
2006.00104 2 5.0106352940962555 11208
2006.09790v1 2 5.0106352940962555 11228
2007 1 5.703782474656201 11248
2007. 1 5.703782474656201 11260
2008.02116 2 5.0106352940962555 11272
2009 2 5.0106352940962555 11292
2010.03104 2 5.0106352940962555 11312
2010.07611v2 2 5.0106352940962555 11332
2011 2 5.0106352940962555 11352
2012 4 4.31748811353631 11372
2012.05942v1 2 5.0106352940962555 11408
2012.08561 2 5.0106352940962555 11428
2012.pietro 2 5.0106352940962555 11448
2013 8 3.624340932976365 11468
2013. 2 5.0106352940962555 11536
2013.todd 2 5.0106352940962555 11556
2014 7 3.7578723256008875 11576
2014. 1 5.703782474656201 11636
2015 13 3.138833117194664 11648
2015. 3 4.605170185988092 11756
2015.dong-hyun 2 5.0106352940962555 11784
2015.moritz 2 5.0106352940962555 11804
2015a 2 5.0106352940962555 11824
2016 4 4.31748811353631 11844
2016. 2 5.0106352940962555 11880
2016.chelsea 2 5.0106352940962555 11900
2017 26 2.445685936634719 11920
2017. 4 4.31748811353631 12132
2017.dmitry 2 5.0106352940962555 12168
2017.tero 2 5.0106352940962555 12188
2017b.ken-ichi 2 5.0106352940962555 12208
2018 24 2.5257286443082556 12228
2018. 9 3.506557897319982 12424
2018a 2 5.0106352940962555 12500
2018a.suriya 2 5.0106352940962555 12520
2018b.benjamin 2 5.0106352940962555 12540
2018b.vivek 2 5.0106352940962555 12560
2018under 2 5.0106352940962555 12580
2019 30 2.302585092994046 12600
2019. 2 5.0106352940962555 12844
2019.gilad 2 5.0106352940962555 12864
2019a 2 5.0106352940962555 12884
2019our 2 5.0106352940962555 12904
2020 37 2.0928645620119766 12924
2020. 4 4.31748811353631 13224
2020a 4 4.31748811353631 13260
2020a.kevin 2 5.0106352940962555 13296
2020b 4 4.31748811353631 13316
2020b.cyprien 2 5.0106352940962555 13352
2020b.john 2 5.0106352940962555 13372
2020flow 2 5.0106352940962555 13392
2020yu 2 5.0106352940962555 13412
2021 28 2.371577964480997 13432
2021. 3 4.605170185988092 13660
2021.xinyun 2 5.0106352940962555 13688
2021b 2 5.0106352940962555 13708
2021published 4 4.31748811353631 13728
2021the 2 5.0106352940962555 13764
2022 28 2.371577964480997 13784
2022. 4 4.31748811353631 14012
2022.arnaud 2 5.0106352940962555 14048
2022definition 2 5.0106352940962555 14068
2022published 6 3.912023005428146 14088
2022we 2 5.0106352940962555 14140
2023 24 2.5257286443082556 14160
2023. 1 5.703782474656201 14356
20231 1 5.703782474656201 14368
20234c8d833f4622c6a583127c3a667e25a5arxiv 2 5.0106352940962555 14380
2023a 4 4.31748811353631 14400
2023b 2 5.0106352940962555 14436
2023iclr 2 5.0106352940962555 14456
2023preprint 2 5.0106352940962555 14476
2023published 4 4.31748811353631 14496
2023we 2 5.0106352940962555 14532
2080ti 2 5.0106352940962555 14552
21 4 4.31748811353631 14572
210-hour 2 5.0106352940962555 14608
2101.10318v2 2 5.0106352940962555 14628
2102.00940v1 2 5.0106352940962555 14648
2102.04168 2 5.0106352940962555 14668
2104.03670 2 5.0106352940962555 14688
2107.03374 2 5.0106352940962555 14708
2107.08981v2 2 5.0106352940962555 14728
2110.04844v3 2 5.0106352940962555 14748
2110.07682v3 2 5.0106352940962555 14768
2111.07941v6 2 5.0106352940962555 14788
2112.07954v3 2 5.0106352940962555 14808
2112.13487 2 5.0106352940962555 14828
21k 1 5.703782474656201 14848
22 3 4.605170185988092 14860
2201.13182v1 2 5.0106352940962555 14888
2203.09168v2 2 5.0106352940962555 14908
2205.15480v2 2 5.0106352940962555 14928
2206.07764 2 5.0106352940962555 14948
2208.06193v3 2 5.0106352940962555 14968
2209.02257v2 2 5.0106352940962555 14988
2209.13570v5 2 5.0106352940962555 15008
2210.02443v1 2 5.0106352940962555 15028
2210.08114v1 2 5.0106352940962555 15048
2211.01288v2 2 5.0106352940962555 15068
23 3 4.605170185988092 15088
23.9 2 5.0106352940962555 15116
2301.12811v3 2 5.0106352940962555 15136
2303.03378 2 5.0106352940962555 15156
2305.14705v2 2 5.0106352940962555 15176
2308.00951v1 2 5.0106352940962555 15196
2308.12820v1 2 5.0106352940962555 15216
2309.03350v1 2 5.0106352940962555 15236
2309.04669v2 2 5.0106352940962555 15256
2309.17410v1 2 5.0106352940962555 15276
24 1 5.703782474656201 15296
240 2 5.0106352940962555 15308
249 2 5.0106352940962555 15328
25 10 3.4011973816621555 15348
25.0 2 5.0106352940962555 15432
25.9 2 5.0106352940962555 15452
2500 1 5.703782474656201 15472
255. 2 5.0106352940962555 15484
256 8 3.624340932976365 15504
256. 6 3.912023005428146 15572
256.comprehensive 2 5.0106352940962555 15624
26 3 4.605170185988092 15644
26.7 2 5.0106352940962555 15672
28 4 4.31748811353631 15692
280b 2 5.0106352940962555 15728
2897-2905 2 5.0106352940962555 15748
29 4 4.31748811353631 15768
290 1 5.703782474656201 15804
2aaron 2 5.0106352940962555 15816
2d 8 3.624340932976365 15836
2or 1 5.703782474656201 15904
2x 1 5.703782474656201 15916
3 41 1.9902104079518932 15928
3-d 2 5.0106352940962555 16260
3-layer 2 5.0106352940962555 16280
3-wl 2 5.0106352940962555 16300
3. 2 5.0106352940962555 16320
3.1 1 5.703782474656201 16340
3.2 4 4.31748811353631 16352
3.2. 2 5.0106352940962555 16388
3.3 5 4.0943445622221 16408
3.4. 2 5.0106352940962555 16452
3.7 2 5.0106352940962555 16472
3.9 2 5.0106352940962555 16492
30 3 4.605170185988092 16512
3090 1 5.703782474656201 16540
31 1 5.703782474656201 16552
32 7 3.7578723256008875 16564
32-512 2 5.0106352940962555 16624
32-bit 2 5.0106352940962555 16644
328-347 2 5.0106352940962555 16664
32b 1 5.703782474656201 16684
33 2 5.0106352940962555 16696
33-57 2 5.0106352940962555 16716
33-75 2 5.0106352940962555 16736
34 2 5.0106352940962555 16756
35 5 4.0943445622221 16776
350 1 5.703782474656201 16820
36 3 4.605170185988092 16832
360 2 5.0106352940962555 16860
371k 2 5.0106352940962555 16880
378 2 5.0106352940962555 16900
38 3 4.605170185988092 16920
39 3 4.605170185988092 16948
39.6 2 5.0106352940962555 16976
3d 15 2.995732273553991 16996
4 23 2.5682882587270512 17120
4. 1 5.703782474656201 17308
4.1 2 5.0106352940962555 17320
4.2 2 5.0106352940962555 17340
4.3 2 5.0106352940962555 17360
4.5 2 5.0106352940962555 17380
4.6 2 5.0106352940962555 17400
4.8 2 5.0106352940962555 17420
40 5 4.0943445622221 17440
41 1 5.703782474656201 17484
414-419 2 5.0106352940962555 17496
419 1 5.703782474656201 17516
42 2 5.0106352940962555 17528
43 3 4.605170185988092 17548
44 2 5.0106352940962555 17576
44k 2 5.0106352940962555 17596
45 4 4.31748811353631 17616
45.5 2 5.0106352940962555 17652
46.2 2 5.0106352940962555 17672
47.1 2 5.0106352940962555 17692
48 2 5.0106352940962555 17712
48-54 2 5.0106352940962555 17732
48.0 2 5.0106352940962555 17752
49 5 4.0943445622221 17772
49-56 2 5.0106352940962555 17816
4x 1 5.703782474656201 17836
5 16 2.9311937524164198 17848
5-shot 2 5.0106352940962555 17980
5.2 2 5.0106352940962555 18000
5.3 2 5.0106352940962555 18020
5.3. 2 5.0106352940962555 18040
5.4 2 5.0106352940962555 18060
5.5 1 5.703782474656201 18080
5.7 2 5.0106352940962555 18092
50 5 4.0943445622221 18112
50.0 2 5.0106352940962555 18156
50.4 2 5.0106352940962555 18176
500k 2 5.0106352940962555 18196
51 2 5.0106352940962555 18216
512 7 3.7578723256008875 18236
52 2 5.0106352940962555 18296
521-530 2 5.0106352940962555 18316
55 2 5.0106352940962555 18336
57 3 4.605170185988092 18356
57.0 2 5.0106352940962555 18384
57.6 2 5.0106352940962555 18404
58-63 2 5.0106352940962555 18424
59 1 5.703782474656201 18444
6 10 3.4011973816621555 18456
6. 1 5.703782474656201 18540
6.2 2 5.0106352940962555 18552
6.4 2 5.0106352940962555 18572
60 1 5.703782474656201 18592
600 1 5.703782474656201 18604
602 2 5.0106352940962555 18616
61.8 2 5.0106352940962555 18636
62b 1 5.703782474656201 18656
63.6 2 5.0106352940962555 18668
64 10 3.4011973816621555 18688
65536 2 5.0106352940962555 18772
66 1 5.703782474656201 18792
66.94 2 5.0106352940962555 18804
6615 2 5.0106352940962555 18824
68 2 5.0106352940962555 18844
68.8 2 5.0106352940962555 18864
7 7 3.7578723256008875 18884
7-dof 2 5.0106352940962555 18944
7.02 2 5.0106352940962555 18964
70 1 5.703782474656201 18984
71 2 5.0106352940962555 18996
73 2 5.0106352940962555 19016
75 2 5.0106352940962555 19036
76 1 5.703782474656201 19056
76.6 2 5.0106352940962555 19068
77 2 5.0106352940962555 19088
7897 2 5.0106352940962555 19108
7b 1 5.703782474656201 19128
8 8 3.624340932976365 19140
8-bit 3 4.605170185988092 19208
8.6 1 5.703782474656201 19236
80 4 4.31748811353631 19248
800 2 5.0106352940962555 19284
836 2 5.0106352940962555 19304
87 1 5.703782474656201 19324
880 2 5.0106352940962555 19336
895-903 2 5.0106352940962555 19356
9 5 4.0943445622221 19376
9.3 2 5.0106352940962555 19420
90 3 4.605170185988092 19440
94 2 5.0106352940962555 19468
9461-9471 2 5.0106352940962555 19488
95 2 5.0106352940962555 19508
96 1 5.703782474656201 19528
98 3 4.605170185988092 19540
983 1 5.703782474656201 19568
a 325 -0.08004270767353637 19580
aamodt 2 5.0106352940962555 22184
abbeel 2 5.0106352940962555 22204
abboud 2 5.0106352940962555 22224
abelian 2 5.0106352940962555 22244
abilities 12 3.2188758248682006 22264
ability 66 1.5141277326297755 22364
ablate 2 5.0106352940962555 22896
ablated 2 5.0106352940962555 22916
ablation 6 3.912023005428146 22936
able 44 1.91959284073794 22988
about 48 1.8325814637483102 23344
above 12 3.2188758248682006 23732
abs 4 4.31748811353631 23832
absence 16 2.9311937524164198 23868
absolute 2 5.0106352940962555 24000
absorb 2 5.0106352940962555 24020
absorbing 2 5.0106352940962555 24040
absorbs 2 5.0106352940962555 24060
abstain 2 5.0106352940962555 24080
abstract 13 3.138833117194664 24100
abstracted 2 5.0106352940962555 24208
abstraction 4 4.31748811353631 24228
abundance 2 5.0106352940962555 24264
accelerate 24 2.5257286443082556 24284
accelerated 6 3.912023005428146 24480
accelerates 2 5.0106352940962555 24532
accelerating 7 3.7578723256008875 24552
acceleration 8 3.624340932976365 24612
access 32 2.2380465718564744 24680
access.our 2 5.0106352940962555 24940
accessed 2 5.0106352940962555 24960
accesses 4 4.31748811353631 24980
accessibility 2 5.0106352940962555 25016
accessible 6 3.912023005428146 25036
accessing 2 5.0106352940962555 25088
accommodate 2 5.0106352940962555 25108
accommodates 4 4.31748811353631 25128
accompanied 4 4.31748811353631 25164
accompanying 2 5.0106352940962555 25200
accomplished 4 4.31748811353631 25220
according 12 3.2188758248682006 25256
accordingly 2 5.0106352940962555 25356
account 16 2.9311937524164198 25376
accounting 4 4.31748811353631 25508
accounts 4 4.31748811353631 25544
accredited 2 5.0106352940962555 25580
accumulated 4 4.31748811353631 25600
accumulates 2 5.0106352940962555 25636
accumulation 4 4.31748811353631 25656
accuracies 6 3.912023005428146 25692
accuracy 84 1.2729656758128876 25744
accuracy-rate-computation 2 5.0106352940962555 26420
accuracy-throughput 2 5.0106352940962555 26440
accuracy.published 2 5.0106352940962555 26460
accurate 18 2.8134107167600364 26480
accurately 22 2.6127400212978853 26628
achievability 2 5.0106352940962555 26808
achieve 76 1.3730491343698699 26828
achieved 32 2.2380465718564744 27440
achieves 56 1.6784307839210517 27700
achieving 26 2.445685936634719 28152
acknowledged 4 4.31748811353631 28364
acquire 4 4.31748811353631 28400
acquiring 2 5.0106352940962555 28436
acquisition 6 3.912023005428146 28456
across 111 0.9942522733438669 28508
act 2 5.0106352940962555 29400
act.such 2 5.0106352940962555 29420
acting 2 5.0106352940962555 29440
action 23 2.5682882587270512 29460
action-conditioned 2 5.0106352940962555 29648
action-spaces 2 5.0106352940962555 29668
action-value 2 5.0106352940962555 29688
action-values 2 5.0106352940962555 29708
actionability 2 5.0106352940962555 29728
actions 32 2.2380465718564744 29748
actions.we 2 5.0106352940962555 30008
activation 18 2.8134107167600364 30028
activations 16 2.9311937524164198 30176
active 6 3.912023005428146 30308
activities 2 5.0106352940962555 30360
activity 6 3.912023005428146 30380
actor 4 4.31748811353631 30432
actor-critic 10 3.4011973816621555 30468
acts 4 4.31748811353631 30552
actual 4 4.31748811353631 30588
actually 12 3.2188758248682006 30624
acutely 2 5.0106352940962555 30724
ad 1 5.703782474656201 30744
ad-hoc 4 4.31748811353631 30756
adagrad 7 3.7578723256008875 30792
adam 10 3.4011973816621555 30852
adamw 6 3.912023005428146 30936
adapt 20 2.70805020110221 30988
adaptation 29 2.336486644669727 31152
adapted 6 3.912023005428146 31388
adapter 2 5.0106352940962555 31440
adapters 2 5.0106352940962555 31460
adapting 6 3.912023005428146 31480
adaptive 31 2.2697952701710546 31532
adaptivity 5 4.0943445622221 31784
adaptivity-boundary 2 5.0106352940962555 31828
adapts 4 4.31748811353631 31848
add 5 4.0943445622221 31884
added 6 3.912023005428146 31928
adding 8 3.624340932976365 31980
addition 36 2.120263536200091 32048
additional 36 2.120263536200091 32340
additionally 16 2.9311937524164198 32632
additions 2 5.0106352940962555 32764
additive 2 5.0106352940962555 32784
addp 3 4.605170185988092 32804
address 88 1.2264456601779945 32832
addressed 2 5.0106352940962555 33540
addresses 10 3.4011973816621555 33560
addressing 6 3.912023005428146 33644
adds 8 3.624340932976365 33696
ade20k 2 5.0106352940962555 33764
adheres 2 5.0106352940962555 33784
adhering 4 4.31748811353631 33804
adjoint 2 5.0106352940962555 33840
adjustment.with 2 5.0106352940962555 33860
adjusts 2 5.0106352940962555 33880
adm 4 4.31748811353631 33900
adm-es 2 5.0106352940962555 33936
adm-esand 2 5.0106352940962555 33956
admission 2 5.0106352940962555 33976
admits 6 3.912023005428146 33996
admitted 2 5.0106352940962555 34048
adobe 2 5.0106352940962555 34068
adopt 4 4.31748811353631 34088
adopted 8 3.624340932976365 34124
adopting 2 5.0106352940962555 34192
adoption 4 4.31748811353631 34212
adopts 2 5.0106352940962555 34248
adr 2 5.0106352940962555 34268
advance 4 4.31748811353631 34288
advanced 6 3.912023005428146 34324
advancements 2 5.0106352940962555 34376
advances 20 2.70805020110221 34396
advancing 2 5.0106352940962555 34560
advantage 12 3.2188758248682006 34580
advantaged 2 5.0106352940962555 34680
advantageous 4 4.31748811353631 34700
advantages 8 3.624340932976365 34736
advent 2 5.0106352940962555 34804
adversarial 63 1.5606477482646683 34824
adversarial-example 2 5.0106352940962555 35332
adversarially 6 3.912023005428146 35352
adversarially-perturbed 2 5.0106352940962555 35404
adversaries 6 3.912023005428146 35424
adversary 4 4.31748811353631 35476
adversely 2 5.0106352940962555 35512
advglue 2 5.0106352940962555 35532
advocate 6 3.912023005428146 35552
advocated 2 5.0106352940962555 35604
aequationum 2 5.0106352940962555 35624
aerial 2 5.0106352940962555 35644
affect 12 3.2188758248682006 35664
affecting 2 5.0106352940962555 35764
affects 6 3.912023005428146 35784
affine 4 4.31748811353631 35836
affine-invariant 2 5.0106352940962555 35872
affirmative 2 5.0106352940962555 35892
affirmatively 2 5.0106352940962555 35912
affordable 2 5.0106352940962555 35932
affordance 2 5.0106352940962555 35952
affords 2 5.0106352940962555 35972
aforementioned 8 3.624340932976365 35992
after 32 2.2380465718564744 36060
afterchange 2 5.0106352940962555 36320
ag 2 5.0106352940962555 36340
again 2 5.0106352940962555 36360
against 29 2.336486644669727 36380
age 1 5.703782474656201 36616
agent 35 2.1484344131667874 36628
agents 29 2.336486644669727 36912
aggravated 2 5.0106352940962555 37148
aggregates 4 4.31748811353631 37168
aggregating 2 5.0106352940962555 37204
aggregation 11 3.3058872018578307 37224
agnostic 4 4.31748811353631 37316
agnostically 2 5.0106352940962555 37352
ai 6 3.912023005428146 37372
aila 2 5.0106352940962555 37424
aim 13 3.138833117194664 37444
aiming 6 3.912023005428146 37552
aims 24 2.5257286443082556 37604
air 2 5.0106352940962555 37800
al 77 1.3599770528025172 37820
alajaji 2 5.0106352940962555 38440
albeit 2 5.0106352940962555 38460
aleatoric 2 5.0106352940962555 38480
alexander 2 5.0106352940962555 38500
alg 1 5.703782474656201 38520
algebra 2 5.0106352940962555 38532
algebraic 2 5.0106352940962555 38552
algorithm 113 0.9763946559438605 38572
algorithm-byalgorithm 2 5.0106352940962555 39480
algorithm.our 2 5.0106352940962555 39500
algorithm.the 2 5.0106352940962555 39520
algorithmic 16 2.9311937524164198 39540
algorithms 103 1.0690534864265653 39672
algorithms.experimentally 2 5.0106352940962555 40500
algorithms.summary 2 5.0106352940962555 40520
align 8 3.624340932976365 40540
aligned 2 5.0106352940962555 40608
aligning 6 3.912023005428146 40628
alignment 27 2.407945608651872 40680
alignment.safe 2 5.0106352940962555 40900
aligns 2 5.0106352940962555 40920
all 57 1.6607312068216509 40940
alleviate 14 3.0647251450409425 41400
alleviated 3 4.605170185988092 41516
alleviates 2 5.0106352940962555 41544
alleviating 2 5.0106352940962555 41564
allocated 2 5.0106352940962555 41584
allocation 2 5.0106352940962555 41604
allow 14 3.0647251450409425 41624
allowable 2 5.0106352940962555 41740
allowed 4 4.31748811353631 41760
allowing 22 2.6127400212978853 41796
allows 66 1.5141277326297755 41976
alluded 2 5.0106352940962555 42508
almost 12 3.2188758248682006 42528
alone 12 3.2188758248682006 42628
along 20 2.70805020110221 42728
alongside 4 4.31748811353631 42892
alpaca-7b 2 5.0106352940962555 42928
alphabetical 4 4.31748811353631 42948
alphabetically 4 4.31748811353631 42984
alphafold2-predicted 2 5.0106352940962555 43020
already 4 4.31748811353631 43040
also 143 0.7409378443962937 43076
alt-text 3 4.605170185988092 44224
alter 2 5.0106352940962555 44252
altering 4 4.31748811353631 44272
alternate 2 5.0106352940962555 44308
alternated 2 5.0106352940962555 44328
alternates 4 4.31748811353631 44348
alternating 7 3.7578723256008875 44384
alternatingly 2 5.0106352940962555 44444
alternative 22 2.6127400212978853 44464
alternatively 2 5.0106352940962555 44644
alternatives 4 4.31748811353631 44664
although 26 2.445685936634719 44700
always 10 3.4011973816621555 44912
am 1 5.703782474656201 44996
ambibench 2 5.0106352940962555 45008
ambiguity 7 3.7578723256008875 45028
ambiguous 6 3.912023005428146 45088
ambiguously-specified 2 5.0106352940962555 45140
amenable 6 3.912023005428146 45160
amiri 2 5.0106352940962555 45212
among 20 2.70805020110221 45232
amongst 6 3.912023005428146 45396
amortized 2 5.0106352940962555 45448
amortizing 2 5.0106352940962555 45468
amount 20 2.70805020110221 45488
amounts 6 3.912023005428146 45652
ample 2 5.0106352940962555 45704
amplified 2 5.0106352940962555 45724
amplify 4 4.31748811353631 45744
amsgrad 4 4.31748811353631 45780
an 199 0.4104776499317087 45816
analogous 4 4.31748811353631 47412
analogue 2 5.0106352940962555 47448
analogy 2 5.0106352940962555 47468
analyses 6 3.912023005428146 47488
analysis 78 1.3470736479666094 47540
analysis.additionally 2 5.0106352940962555 48168
analytic 4 4.31748811353631 48188
analytical 6 3.912023005428146 48224
analytical.other 2 5.0106352940962555 48276
analytically 4 4.31748811353631 48296
analyze 24 2.5257286443082556 48332
analyze.existing 2 5.0106352940962555 48528
analyzed 4 4.31748811353631 48548
analyzes 4 4.31748811353631 48584
analyzing 10 3.4011973816621555 48620
anatomical 2 5.0106352940962555 48704
anchors 2 5.0106352940962555 48724
and 343 -0.13394797250973886 48744
andpreprint 2 5.0106352940962555 51492
andrew 2 5.0106352940962555 51512
animal 2 5.0106352940962555 51532
animals 6 3.912023005428146 51552
ann-equivalent 2 5.0106352940962555 51604
annealers 2 5.0106352940962555 51624
annealing 5 4.0943445622221 51644
annotate 2 5.0106352940962555 51688
annotated 4 4.31748811353631 51708
annotation 8 3.624340932976365 51744
annotations 4 4.31748811353631 51812
anomaly 7 3.7578723256008875 51848
another 10 3.4011973816621555 51908
answer 24 2.5257286443082556 51992
answering 16 2.9311937524164198 52188
answers 8 3.624340932976365 52320
ant 1 5.703782474656201 52388
antivirus 2 5.0106352940962555 52400
any 51 1.7719568419318754 52420
anything 4 4.31748811353631 52832
ao 2 5.0106352940962555 52868
apart 4 4.31748811353631 52888
api 1 5.703782474656201 52924
apis 4 4.31748811353631 52936
apparatuses 2 5.0106352940962555 52972
apparent 4 4.31748811353631 52992
apparently 2 5.0106352940962555 53028
appealing 2 5.0106352940962555 53048
appear 2 5.0106352940962555 53068
appearance 2 5.0106352940962555 53088
appearances 2 5.0106352940962555 53108
appearing 2 5.0106352940962555 53128
appears 2 5.0106352940962555 53148
appendix 2 5.0106352940962555 53168
apples 2 5.0106352940962555 53188
applicability 8 3.624340932976365 53208
applicable 18 2.8134107167600364 53276
application 20 2.70805020110221 53424
application.the 2 5.0106352940962555 53588
applications 80 1.3217558399823195 53608
applications.to 2 5.0106352940962555 54252
applied 28 2.371577964480997 54272
applies 12 3.2188758248682006 54500
apply 32 2.2380465718564744 54600
applying 26 2.445685936634719 54860
approach 149 0.6998361687107421 55072
approach-without 2 5.0106352940962555 56268
approach.our 2 5.0106352940962555 56288
approach.published 2 5.0106352940962555 56308
approaches 80 1.3217558399823195 56328
approaches.as 2 5.0106352940962555 56972
approaches.the 2 5.0106352940962555 56992
approaches.we 2 5.0106352940962555 57012
approaching 2 5.0106352940962555 57032
appropriate 14 3.0647251450409425 57052
approval 2 5.0106352940962555 57168
approximability 3 4.605170185988092 57188
approximable 4 4.31748811353631 57216
approximate 51 1.7719568419318754 57252
approximated 6 3.912023005428146 57664
approximately 6 3.912023005428146 57716
approximates 2 5.0106352940962555 57768
approximating 6 3.912023005428146 57788
approximation 45 1.8971199848858813 57840
approximationem 2 5.0106352940962555 58204
approximations 12 3.2188758248682006 58224
approximators 10 3.4011973816621555 58324
appx 2 5.0106352940962555 58408
apr 2 5.0106352940962555 58428
aqua 2 5.0106352940962555 58448
ar 2 5.0106352940962555 58468
ar-based 2 5.0106352940962555 58488
arbitrarily 12 3.2188758248682006 58508
arbitrary 18 2.8134107167600364 58608
arc-challenge 2 5.0106352940962555 58756
architectural 8 3.624340932976365 58776
architecture 56 1.6784307839210517 58844
architectures 49 1.8119621765455745 59296
architectures.one 2 5.0106352940962555 59692
architectures.the 2 5.0106352940962555 59712
are 186 0.47803580094299974 59732
area 4 4.31748811353631 61224
areas 4 4.31748811353631 61260
aren 2 5.0106352940962555 61296
arg 1 5.703782474656201 61316
argue 26 2.445685936634719 61328
argues 2 5.0106352940962555 61540
arise 8 3.624340932976365 61560
arises 6 3.912023005428146 61628
arising 6 3.912023005428146 61680
arithmetic 7 3.7578723256008875 61732
arjevani 2 5.0106352940962555 61792
arm 1 5.703782474656201 61812
arms 2 5.0106352940962555 61824
around 10 3.4011973816621555 61844
arrangement 2 5.0106352940962555 61928
arrangements 2 5.0106352940962555 61948
arrive 6 3.912023005428146 61968
arrives 2 5.0106352940962555 62020
arriving 4 4.31748811353631 62040
art 11 3.3058872018578307 62076
art-dream 2 5.0106352940962555 62168
articulated 2 5.0106352940962555 62188
artifact 2 5.0106352940962555 62208
artifact-causing 2 5.0106352940962555 62228
artifacts 8 3.624340932976365 62248
artificial 10 3.4011973816621555 62316
artificially 4 4.31748811353631 62400
artistic 2 5.0106352940962555 62436
arxiv 68 1.4842747694800944 62456
arxiv.org 2 5.0106352940962555 63004
as 205 0.38077249551779285 63024
ascent 2 5.0106352940962555 64668
aside 2 5.0106352940962555 64688
ask 1 5.703782474656201 64708
asking 3 4.605170185988092 64720
asks 2 5.0106352940962555 64748
aspect 2 5.0106352940962555 64768
aspects 12 3.2188758248682006 64788
assembly 2 5.0106352940962555 64888
assess 12 3.2188758248682006 64908
assessment 4 4.31748811353631 65008
assign 4 4.31748811353631 65044
assigned 4 4.31748811353631 65080
assigning 2 5.0106352940962555 65116
assignment 8 3.624340932976365 65136
assignments 2 5.0106352940962555 65204
assigns 6 3.912023005428146 65224
assist 2 5.0106352940962555 65276
assistance 2 5.0106352940962555 65296
assistants 2 5.0106352940962555 65316
associated 20 2.70805020110221 65336
association 2 5.0106352940962555 65500
associations 4 4.31748811353631 65520
associative 2 5.0106352940962555 65556
assume 14 3.0647251450409425 65576
assumed 2 5.0106352940962555 65692
assumes 2 5.0106352940962555 65712
assuming 14 3.0647251450409425 65732
assumption 16 2.9311937524164198 65848
assumptions 34 2.17742195004004 65980
ast 2 5.0106352940962555 66256
astonishingly 2 5.0106352940962555 66276
astronomy 2 5.0106352940962555 66296
asymmetric 2 5.0106352940962555 66316
asymptotic 7 3.7578723256008875 66336
asymptotically 4 4.31748811353631 66396
asymptotics 2 5.0106352940962555 66432
async-red 3 4.605170185988092 66452
asynchronous 8 3.624340932976365 66480
at 124 0.8835009090511642 66548
atari 4 4.31748811353631 67544
atom 2 5.0106352940962555 67580
atomic 2 5.0106352940962555 67600
atoms 2 5.0106352940962555 67620
attack 32 2.2380465718564744 67640
attack-and-defense 2 5.0106352940962555 67900
attacker 4 4.31748811353631 67920
attackers 4 4.31748811353631 67956
attacks 32 2.2380465718564744 67992
attacks-we 2 5.0106352940962555 68252
attacks.client-side 2 5.0106352940962555 68272
attacks.however 2 5.0106352940962555 68292
attacks.it 2 5.0106352940962555 68312
attacks.preprint 2 5.0106352940962555 68332
attacks.this 2 5.0106352940962555 68352
attacks.we 2 5.0106352940962555 68372
attain 4 4.31748811353631 68392
attempt 12 3.2188758248682006 68428
attempted 2 5.0106352940962555 68528
attempts 10 3.4011973816621555 68548
attend 4 4.31748811353631 68632
attended 2 5.0106352940962555 68668
attending 2 5.0106352940962555 68688
attention 39 2.0402208285265546 68708
attracted 2 5.0106352940962555 69024
attracting 2 5.0106352940962555 69044
attractive 2 5.0106352940962555 69064
attribute 6 3.912023005428146 69084
attributed 2 5.0106352940962555 69136
attributes 12 3.2188758248682006 69156
audio 8 3.624340932976365 69256
audit 4 4.31748811353631 69324
auditors 2 5.0106352940962555 69360
aug 3 4.605170185988092 69380
augment 8 3.624340932976365 69408
augmentation 13 3.138833117194664 69476
augmentations 6 3.912023005428146 69584
augmented 10 3.4011973816621555 69636
augmenting 4 4.31748811353631 69720
augments 2 5.0106352940962555 69756
austin 2 5.0106352940962555 69776
author 2 5.0106352940962555 69796
authors 32 2.2380465718564744 69816
authors.our 2 5.0106352940962555 70076
authorship 4 4.31748811353631 70096
auto-completion 3 4.605170185988092 70132
auto-cot 2 5.0106352940962555 70160
auto-encoder 4 4.31748811353631 70180
auto-encoders 2 5.0106352940962555 70216
auto-regressive 5 4.0943445622221 70236
autoencoder 9 3.506557897319982 70280
autoencoders 12 3.2188758248682006 70356
automated 11 3.3058872018578307 70456
automatic 13 3.138833117194664 70548
automatically 12 3.2188758248682006 70656
automating 2 5.0106352940962555 70756
autonomous 8 3.624340932976365 70776
autonomously 2 5.0106352940962555 70844
autoregressive 19 2.7593434954897607 70864
autume 2 5.0106352940962555 71020
auxiliary 15 2.995732273553991 71040
availability 4 4.31748811353631 71164
availability.results 2 5.0106352940962555 71200
available 98 1.118814995985629 71220
avatar 2 5.0106352940962555 72008
avatars 2 5.0106352940962555 72028
average 10 3.4011973816621555 72048
average-case 2 5.0106352940962555 72132
average-iterate 2 5.0106352940962555 72152
averages 2 5.0106352940962555 72172
averaging 2 5.0106352940962555 72192
avoid 10 3.4011973816621555 72212
avoiding 6 3.912023005428146 72296
avoids 8 3.624340932976365 72348
avril 2 5.0106352940962555 72416
away 16 2.9311937524164198 72436
ax 2 5.0106352940962555 72568
axes 2 5.0106352940962555 72588
axial 2 5.0106352940962555 72608
axiomatic 2 5.0106352940962555 72628
axis 4 4.31748811353631 72648
b 13 3.138833117194664 72684
baby 2 5.0106352940962555 72792
back 8 3.624340932976365 72812
backbone 6 3.912023005428146 72880
backbones 7 3.7578723256008875 72932
backdoor 8 3.624340932976365 72992
background 6 3.912023005428146 73060
backpropagation 7 3.7578723256008875 73112
backstepping 3 4.605170185988092 73172
backups 2 5.0106352940962555 73200
backward 10 3.4011973816621555 73220
backwards 2 5.0106352940962555 73304
bag 1 5.703782474656201 73324
bag-of-word 2 5.0106352940962555 73336
bag-of-words 2 5.0106352940962555 73356
bagnell 2 5.0106352940962555 73376
bags 2 5.0106352940962555 73396
balance 6 3.912023005428146 73416
balances 4 4.31748811353631 73468
balancing 6 3.912023005428146 73504
ball 4 4.31748811353631 73556
ball.arxiv 2 5.0106352940962555 73592
ball.finally 2 5.0106352940962555 73612
balls 2 5.0106352940962555 73632
banach 2 5.0106352940962555 73652
bandit 9 3.506557897319982 73672
bandits 6 3.912023005428146 73748
bank 2 5.0106352940962555 73800
bank.user 2 5.0106352940962555 73820
baptist 2 5.0106352940962555 73840
bar 1 5.703782474656201 73860
barrier 10 3.4011973816621555 73872
barriers 2 5.0106352940962555 73956
barto 6 3.912023005428146 73976
barton 2 5.0106352940962555 74028
barzilay 2 5.0106352940962555 74048
base 14 3.0647251450409425 74068
based 109 1.0124345924270572 74184
baseline 19 2.7593434954897607 75060
baselines 44 1.91959284073794 75216
baselines.preprint 4 4.31748811353631 75572
bases 2 5.0106352940962555 75608
bases.more 2 5.0106352940962555 75628
basic 8 3.624340932976365 75648
basis 4 4.31748811353631 75716
basis.notably 2 5.0106352940962555 75752
batch 19 2.7593434954897607 75772
batch.this 2 5.0106352940962555 75928
batches 6 3.912023005428146 75948
battery 2 5.0106352940962555 76000
baur-strassen 2 5.0106352940962555 76020
bayes 5 4.0943445622221 76040
bayesian 21 2.659260036932778 76084
bbh 1 5.703782474656201 76256
bc 2 5.0106352940962555 76268
bcc 2 5.0106352940962555 76288
be 146 0.7201758529478647 76308
beam 2 5.0106352940962555 77480
bear 2 5.0106352940962555 77500
bearing 2 5.0106352940962555 77520
beat 2 5.0106352940962555 77540
beauchamp 2 5.0106352940962555 77560
because 24 2.5257286443082556 77580
become 22 2.6127400212978853 77776
becomes 16 2.9311937524164198 77956
been 120 0.9162907318741551 78088
beetle 2 5.0106352940962555 79052
before 10 3.4011973816621555 79072
before.effectively 2 5.0106352940962555 79156
beforehand 2 5.0106352940962555 79176
beginning 6 3.912023005428146 79196
begins 2 5.0106352940962555 79248
behave 4 4.31748811353631 79268
behavior 35 2.1484344131667874 79304
behavior-cloned 2 5.0106352940962555 79588
behavioral 6 3.912023005428146 79608
behaviorally-relevant 2 5.0106352940962555 79660
behaviors 16 2.9311937524164198 79680
behaviour 2 5.0106352940962555 79812
behavioural 2 5.0106352940962555 79832
behaviours 2 5.0106352940962555 79852
behind 4 4.31748811353631 79872
being 36 2.120263536200091 79908
belief 4 4.31748811353631 80200
beliefs 6 3.912023005428146 80236
believe 6 3.912023005428146 80288
believed 4 4.31748811353631 80340
bellman 4 4.31748811353631 80376
belongie 4 4.31748811353631 80412
belonging 4 4.31748811353631 80448
below 4 4.31748811353631 80484
benchmark 80 1.3217558399823195 80520
benchmarking 8 3.624340932976365 81164
benchmarks 57 1.6607312068216509 81232
benchmarks-and 2 5.0106352940962555 81692
benchmarks.under 2 5.0106352940962555 81712
benchmarks.wanda 2 5.0106352940962555 81732
beneficence 2 5.0106352940962555 81752
beneficial 4 4.31748811353631 81772
benefit 19 2.7593434954897607 81808
benefits 31 2.2697952701710546 81964
benign 4 4.31748811353631 82216
bernhardsson 2 5.0106352940962555 82252
bert 4 4.31748811353631 82272
bert-based 2 5.0106352940962555 82308
besides 8 3.624340932976365 82328
bespoke 3 4.605170185988092 82396
best 40 2.0149030205422647 82424
best-performing 2 5.0106352940962555 82748
best-policy 2 5.0106352940962555 82768
best.arxiv 2 5.0106352940962555 82788
beta 2 5.0106352940962555 82808
bets 2 5.0106352940962555 82828
better 102 1.07880966137193 82848
better-than-monte-carlo 2 5.0106352940962555 83668
between 158 0.6411874416292342 83688
between.for 2 5.0106352940962555 84956
beyond 29 2.336486644669727 84976
bgrl 2 5.0106352940962555 85212
bhandari 2 5.0106352940962555 85232
bi-arm 2 5.0106352940962555 85252
bias 36 2.120263536200091 85272
bias.we 2 5.0106352940962555 85564
biased 11 3.3058872018578307 85584
biases 12 3.2188758248682006 85676
biasing 2 5.0106352940962555 85776
bigcode-project 2 5.0106352940962555 85796
bilevel 10 3.4011973816621555 85816
billion 4 4.31748811353631 85900
billion-scale 2 5.0106352940962555 85936
billions 2 5.0106352940962555 85956
bimodal 2 5.0106352940962555 85976
binary 11 3.3058872018578307 85996
binder 3 4.605170185988092 86088
binding 5 4.0943445622221 86116
binocular 2 5.0106352940962555 86160
bins 2 5.0106352940962555 86180
biochemical 2 5.0106352940962555 86200
biodiversity 2 5.0106352940962555 86220
biological 4 4.31748811353631 86240
bird 4 4.31748811353631 86276
birds 2 5.0106352940962555 86312
birkuser 2 5.0106352940962555 86332
bisimulation 2 5.0106352940962555 86352
bit-depth 2 5.0106352940962555 86372
bit-width 2 5.0106352940962555 86392
bits 2 5.0106352940962555 86412
bivariate 2 5.0106352940962555 86432
black 4 4.31748811353631 86452
black-box 16 2.9311937524164198 86488
blackbox 4 4.31748811353631 86620
blank 2 5.0106352940962555 86656
blended 2 5.0106352940962555 86676
bleu 8 3.624340932976365 86696
block 10 3.4011973816621555 86764
block-slot 2 5.0106352940962555 86848
block-wise 3 4.605170185988092 86868
blocks 10 3.4011973816621555 86896
blocks.we 2 5.0106352940962555 86980
blow 2 5.0106352940962555 87000
blue 2 5.0106352940962555 87020
blue-print 2 5.0106352940962555 87040
blurring 2 5.0106352940962555 87060
blurs 2 5.0106352940962555 87080
bm 2 5.0106352940962555 87100
bm25 2 5.0106352940962555 87120
bncde 2 5.0106352940962555 87140
bo 2 5.0106352940962555 87160
board 2 5.0106352940962555 87180
body 4 4.31748811353631 87200
boil 4 4.31748811353631 87236
bolsters 2 5.0106352940962555 87272
bond 2 5.0106352940962555 87292
bonding 2 5.0106352940962555 87312
bonneel 2 5.0106352940962555 87332
bonus 4 4.31748811353631 87352
book-related 2 5.0106352940962555 87388
books 2 5.0106352940962555 87408
boolean 2 5.0106352940962555 87428
boost 8 3.624340932976365 87448
boosted 2 5.0106352940962555 87516
boosting 2 5.0106352940962555 87536
boosts 6 3.912023005428146 87556
bootstrapped 2 5.0106352940962555 87608
bootstrapping 4 4.31748811353631 87628
bootstraps 2 5.0106352940962555 87664
borkar 2 5.0106352940962555 87684
borrows 2 5.0106352940962555 87704
both 158 0.6411874416292342 87724
bottleneck 25 2.4849066497880004 88992
bottlenecks 4 4.31748811353631 89196
bottom-up 7 3.7578723256008875 89232
bottomup 2 5.0106352940962555 89292
boucheron 2 5.0106352940962555 89312
bound 40 2.0149030205422647 89332
bound.these 2 5.0106352940962555 89656
boundaries 4 4.31748811353631 89676
boundary 6 3.912023005428146 89712
bounded 6 3.912023005428146 89764
boundedness 2 5.0106352940962555 89816
bounding 8 3.624340932976365 89836
bounds 25 2.4849066497880004 89904
bounds.specifically 2 5.0106352940962555 90108
box 2 5.0106352940962555 90128
boxes 2 5.0106352940962555 90148
boy 2 5.0106352940962555 90168
bpgrad 2 5.0106352940962555 90188
bradbury 2 5.0106352940962555 90208
bradtke 2 5.0106352940962555 90228
brain 11 3.3058872018578307 90248
brain-inspired 2 5.0106352940962555 90340
branch 2 5.0106352940962555 90360
breadth 2 5.0106352940962555 90380
breadth-first 2 5.0106352940962555 90400
break 4 4.31748811353631 90420
breaking 2 5.0106352940962555 90456
breaks 2 5.0106352940962555 90476
brevity 4 4.31748811353631 90496
bridge 2 5.0106352940962555 90532
bridgedata 2 5.0106352940962555 90552
bridges 2 5.0106352940962555 90572
bridging 4 4.31748811353631 90592
bring 6 3.912023005428146 90628
bringing 4 4.31748811353631 90680
broad 8 3.624340932976365 90716
broader 12 3.2188758248682006 90784
broadly 4 4.31748811353631 90884
brought 6 3.912023005428146 90920
brown 6 3.912023005428146 90972
brownian 2 5.0106352940962555 91024
browning 2 5.0106352940962555 91044
brushes 2 5.0106352940962555 91064
brute 2 5.0106352940962555 91084
brx 1 5.703782474656201 91104
bu-depend-lab 2 5.0106352940962555 91116
bubeck 2 5.0106352940962555 91136
budget 6 3.912023005428146 91156
budgeted 2 5.0106352940962555 91208
budgets 4 4.31748811353631 91228
buffer 2 5.0106352940962555 91264
bufr 2 5.0106352940962555 91284
buggy 2 5.0106352940962555 91304
build 16 2.9311937524164198 91324
building 22 2.6127400212978853 91456
builds 6 3.912023005428146 91636
built 6 3.912023005428146 91688
burden 8 3.624340932976365 91740
but 72 1.4271163556401458 91808
buyable 2 5.0106352940962555 92388
by 214 0.3378064596343495 92408
by-product 2 5.0106352940962555 94124
byalmost 2 5.0106352940962555 94144
bymaron 2 5.0106352940962555 94164
byol 2 5.0106352940962555 94184
bypass 2 5.0106352940962555 94204
bysutton 2 5.0106352940962555 94224
bytes 2 5.0106352940962555 94244
byvaswani 2 5.0106352940962555 94264
bhlmann 2 5.0106352940962555 94284
c 7 3.7578723256008875 94304
c4 2 5.0106352940962555 94364
calculate 2 5.0106352940962555 94384
calculating 4 4.31748811353631 94404
calculation 4 4.31748811353631 94440
calculators 2 5.0106352940962555 94476
calculus 2 5.0106352940962555 94496
calibration 6 3.912023005428146 94516
calibration-minimizing 2 5.0106352940962555 94568
call 20 2.70805020110221 94588
called 50 1.791759469228055 94752
cambridge 2 5.0106352940962555 95156
camera-only 2 5.0106352940962555 95176
cameras 4 4.31748811353631 95196
caml 2 5.0106352940962555 95232
can 204 0.38566248081198473 95252
candidate 2 5.0106352940962555 96888
candidates 6 3.912023005428146 96908
canonical 4 4.31748811353631 96960
cantor 2 5.0106352940962555 96996
capabilities 30 2.302585092994046 97016
capabilities.conjugate 2 5.0106352940962555 97260
capability 26 2.445685936634719 97280
capability.we 2 5.0106352940962555 97492
capable 14 3.0647251450409425 97512
capacities.preprint 2 5.0106352940962555 97628
capacity 29 2.336486644669727 97648
capacity-limited 2 5.0106352940962555 97884
capitalizes 2 5.0106352940962555 97904
capsule 2 5.0106352940962555 97924
caption 4 4.31748811353631 97944
captioning 2 5.0106352940962555 97980
capture 24 2.5257286443082556 98000
captured 6 3.912023005428146 98196
captures 10 3.4011973816621555 98248
capturing 16 2.9311937524164198 98332
card 3 4.605170185988092 98464
cards 2 5.0106352940962555 98492
cards-specialized 2 5.0106352940962555 98512
careful 6 3.912023005428146 98532
carefully 11 3.3058872018578307 98584
carlo 8 3.624340932976365 98676
carry 10 3.4011973816621555 98744
cars 2 5.0106352940962555 98828
cartesian 2 5.0106352940962555 98848
cascaded 2 5.0106352940962555 98868
case 42 1.9661128563728327 98888
cases 44 1.91959284073794 99228
cases.we 4 4.31748811353631 99584
cast 10 3.4011973816621555 99620
casting 2 5.0106352940962555 99704
casual 2 5.0106352940962555 99724
catalyst 2 5.0106352940962555 99744
catalyst-accelerated 2 5.0106352940962555 99764
catalysts 2 5.0106352940962555 99784
catalyzed 2 5.0106352940962555 99804
catastrophic 6 3.912023005428146 99824
catch 2 5.0106352940962555 99876
categorical 3 4.605170185988092 99896
categories 8 3.624340932976365 99924
category 2 5.0106352940962555 99992
category.empirically 2 5.0106352940962555 100012
causal 10 3.4011973816621555 100032
causality 5 4.0943445622221 100116
cause 20 2.70805020110221 100160
caused 4 4.31748811353631 100324
causes 18 2.8134107167600364 100360
causing 4 4.31748811353631 100508
caveats 3 4.605170185988092 100544
cbm 1 5.703782474656201 100572
cbms 2 5.0106352940962555 100584
ccil 2 5.0106352940962555 100604
cd 2 5.0106352940962555 100624
cdvae 2 5.0106352940962555 100644
ce 1 5.703782474656201 100664
celeba 4 4.31748811353631 100676
celeba-hq 2 5.0106352940962555 100712
cell 3 4.605170185988092 100732
cells 2 5.0106352940962555 100760
center 2 5.0106352940962555 100780
centered 2 5.0106352940962555 100800
central 4 4.31748811353631 100820
centralised 2 5.0106352940962555 100856
centralized-decentralized 2 5.0106352940962555 100876
certain 32 2.2380465718564744 100896
certificates 2 5.0106352940962555 101156
certified 7 3.7578723256008875 101176
certify 4 4.31748811353631 101236
certifying 2 5.0106352940962555 101272
cf-sgd 2 5.0106352940962555 101292
cfr 2 5.0106352940962555 101312
cfr-based 2 5.0106352940962555 101332
cgnet 2 5.0106352940962555 101352
chain 15 2.995732273553991 101372
chain-of-thought 8 3.624340932976365 101496
chain-of-thoughts 2 5.0106352940962555 101564
chains 8 3.624340932976365 101584
challenge 40 2.0149030205422647 101652
challenge.through 2 5.0106352940962555 101976
challenges 42 1.9661128563728327 101996
challenges.due 2 5.0106352940962555 102336
challenging 64 1.5448993912965292 102356
challenging.dynamics 2 5.0106352940962555 102872
chameleon 2 5.0106352940962555 102892
chamfer 2 5.0106352940962555 102912
chance 2 5.0106352940962555 102932
chances 2 5.0106352940962555 102952
chang 2 5.0106352940962555 102972
change 16 2.9311937524164198 102992
change.increasing 2 5.0106352940962555 103124
changes 22 2.6127400212978853 103144
changing 10 3.4011973816621555 103324
channel-wise 2 5.0106352940962555 103408
channels 6 3.912023005428146 103428
channelwise 2 5.0106352940962555 103480
character 2 5.0106352940962555 103500
character-level 2 5.0106352940962555 103520
characteristic 2 5.0106352940962555 103540
characteristics 12 3.2188758248682006 103560
characterization 8 3.624340932976365 103660
characterize 8 3.624340932976365 103728
characterized 2 5.0106352940962555 103796
characterizes 2 5.0106352940962555 103816
characterizing 8 3.624340932976365 103836
characters 2 5.0106352940962555 103904
chat 3 4.605170185988092 103924
chatbots 4 4.31748811353631 103952
chatgpt 10 3.4011973816621555 103988
cheap 4 4.31748811353631 104072
cheap-weak 2 5.0106352940962555 104108
cheaper 2 5.0106352940962555 104128
cheaply 2 5.0106352940962555 104148
cheapness 2 5.0106352940962555 104168
checkpoints 2 5.0106352940962555 104188
checks 4 4.31748811353631 104208
chelba 2 5.0106352940962555 104244
chemical 4 4.31748811353631 104264
chemically 2 5.0106352940962555 104300
chemnet 2 5.0106352940962555 104320
chemrlformer 2 5.0106352940962555 104340
chen 10 3.4011973816621555 104360
chess 2 5.0106352940962555 104444
chinese 2 5.0106352940962555 104464
choice 28 2.371577964480997 104484
choices 12 3.2188758248682006 104712
choose 10 3.4011973816621555 104812
chooses 2 5.0106352940962555 104896
choosing 2 5.0106352940962555 104916
chortai 2 5.0106352940962555 104936
chosen 8 3.624340932976365 104956
christopher 2 5.0106352940962555 105024
chrono 2 5.0106352940962555 105044
cifar 8 3.624340932976365 105064
cifar-10 12 3.2188758248682006 105132
cifar10 12 3.2188758248682006 105232
cipher 3 4.605170185988092 105332
cipherchat 2 5.0106352940962555 105360
ciphers 2 5.0106352940962555 105380
circles 2 5.0106352940962555 105400
circuits 2 5.0106352940962555 105420
circuits.inferred 2 5.0106352940962555 105440
circumstances 2 5.0106352940962555 105460
circumvent 4 4.31748811353631 105480
circumvents 2 5.0106352940962555 105516
cl 2 5.0106352940962555 105536
claims 6 3.912023005428146 105556
clarify 4 4.31748811353631 105608
clarity 2 5.0106352940962555 105644
clark 2 5.0106352940962555 105664
clarke 2 5.0106352940962555 105684
clasp 2 5.0106352940962555 105704
class 43 1.9425823589626385 105724
class-conditional 4 4.31748811353631 106072
class-dependent 2 5.0106352940962555 106108
class-separation 2 5.0106352940962555 106128
classes 22 2.6127400212978853 106148
classic 12 3.2188758248682006 106328
classical 16 2.9311937524164198 106428
classification 93 1.171182981502945 106560
classification-based 3 4.605170185988092 107308
classification.published 2 5.0106352940962555 107336
classifications 2 5.0106352940962555 107356
classified 6 3.912023005428146 107376
classifier 21 2.659260036932778 107428
classifier-free 6 3.912023005428146 107600
classifier-score-distillation 2 5.0106352940962555 107652
classifier.despite 2 5.0106352940962555 107672
classifiers 19 2.7593434954897607 107692
classify 4 4.31748811353631 107848
clean 12 3.2188758248682006 107884
cleaned 2 5.0106352940962555 107984
clear 12 3.2188758248682006 108004
clearly 2 5.0106352940962555 108104
clevr-tex 2 5.0106352940962555 108124
client 8 3.624340932976365 108144
client-side 2 5.0106352940962555 108212
clients 8 3.624340932976365 108232
clip 4 4.31748811353631 108300
clipping 2 5.0106352940962555 108336
cloned 2 5.0106352940962555 108356
cloning 2 5.0106352940962555 108376
close 16 2.9311937524164198 108396
close-to 2 5.0106352940962555 108528
closed 2 5.0106352940962555 108548
closed-form 6 3.912023005428146 108568
closedform 2 5.0106352940962555 108620
closely 10 3.4011973816621555 108640
closer 10 3.4011973816621555 108724
closes 2 5.0106352940962555 108808
closest 2 5.0106352940962555 108828
closing 8 3.624340932976365 108848
closure 2 5.0106352940962555 108916
cloth 2 5.0106352940962555 108936
cloud 9 3.506557897319982 108956
clouds 4 4.31748811353631 109032
cloze 2 5.0106352940962555 109068
cls 2 5.0106352940962555 109088
cluster 4 4.31748811353631 109108
clustering 3 4.605170185988092 109144
clusters 2 5.0106352940962555 109172
cmlm 2 5.0106352940962555 109192
cnn 3 4.605170185988092 109212
cnn-based 2 5.0106352940962555 109240
cnnbased 2 5.0106352940962555 109260
cnns 2 5.0106352940962555 109280
co-adapt 2 5.0106352940962555 109300
co-corresponding 2 5.0106352940962555 109320
co-linearity 2 5.0106352940962555 109340
coarse 10 3.4011973816621555 109360
coarsely 2 5.0106352940962555 109444
coarsening 3 4.605170185988092 109464
coarser 2 5.0106352940962555 109492
coco 6 3.912023005428146 109512
code 99 1.1086626245216111 109564
code-understanding 2 5.0106352940962555 110360
code.assembly 2 5.0106352940962555 110380
codecs 2 5.0106352940962555 110400
codes 8 3.624340932976365 110420
coding 8 3.624340932976365 110488
coefficient 4 4.31748811353631 110556
coefficients 6 3.912023005428146 110592
coefficients.furthermore 2 5.0106352940962555 110644
cognition 4 4.31748811353631 110664
coherence 6 3.912023005428146 110700
coherence.in 2 5.0106352940962555 110752
coherency 2 5.0106352940962555 110772
coherent 8 3.624340932976365 110792
coin 6 3.912023005428146 110860
coined 6 3.912023005428146 110912
cold 2 5.0106352940962555 110964
collaborative 2 5.0106352940962555 110984
collaboratively 2 5.0106352940962555 111004
collage 2 5.0106352940962555 111024
collapse 5 4.0943445622221 111044
collapses 2 5.0106352940962555 111088
collapsing 2 5.0106352940962555 111108
collect 8 3.624340932976365 111128
collected 14 3.0647251450409425 111196
collecting 2 5.0106352940962555 111312
collection 20 2.70805020110221 111332
collective 4 4.31748811353631 111496
collects 2 5.0106352940962555 111532
college 2 5.0106352940962555 111552
collocation 2 5.0106352940962555 111572
color 2 5.0106352940962555 111592
colored 2 5.0106352940962555 111612
colors 2 5.0106352940962555 111632
colossal 2 5.0106352940962555 111652
columns 2 5.0106352940962555 111672
combination 14 3.0647251450409425 111692
combinations 10 3.4011973816621555 111808
combinatorial 2 5.0106352940962555 111892
combinatorially 2 5.0106352940962555 111912
combinatorially-large 2 5.0106352940962555 111932
combine 16 2.9311937524164198 111952
combined 22 2.6127400212978853 112084
combines 10 3.4011973816621555 112264
combining 22 2.6127400212978853 112348
combo 2 5.0106352940962555 112528
come 6 3.912023005428146 112548
comes 14 3.0647251450409425 112600
comln 2 5.0106352940962555 112716
commercial 4 4.31748811353631 112736
commit 2 5.0106352940962555 112772
commitpack 2 5.0106352940962555 112792
commits 2 5.0106352940962555 112812
common 53 1.7334905611040792 112832
commoncrawl 2 5.0106352940962555 113260
commonly 20 2.70805020110221 113280
commonly-held 2 5.0106352940962555 113444
commonly-used 2 5.0106352940962555 113464
commonsense 2 5.0106352940962555 113484
commonsenseqa 2 5.0106352940962555 113504
communicate 2 5.0106352940962555 113524
communicating 2 5.0106352940962555 113544
communication 17 2.870569130599985 113564
community 8 3.624340932976365 113704
compact 4 4.31748811353631 113772
compactly 2 5.0106352940962555 113808
comparability 2 5.0106352940962555 113828
comparable 16 2.9311937524164198 113848
comparably 2 5.0106352940962555 113980
comparatively 2 5.0106352940962555 114000
compare 12 3.2188758248682006 114020
compared 88 1.2264456601779945 114120
comparing 8 3.624340932976365 114828
comparison 14 3.0647251450409425 114896
comparisons 10 3.4011973816621555 115012
compelling 6 3.912023005428146 115096
compensating 2 5.0106352940962555 115148
competition 4 4.31748811353631 115168
competitions 2 5.0106352940962555 115204
competitive 32 2.2380465718564744 115224
competitively 4 4.31748811353631 115484
competitiveness 2 5.0106352940962555 115520
competitor 4 4.31748811353631 115540
compile 2 5.0106352940962555 115576
complementary 16 2.9311937524164198 115596
complemented 2 5.0106352940962555 115728
complete 29 2.336486644669727 115748
completed 2 5.0106352940962555 115984
completely 14 3.0647251450409425 116004
completeness 7 3.7578723256008875 116120
completion 18 2.8134107167600364 116180
complex 75 1.3862943611198906 116328
complexes 2 5.0106352940962555 116932
complexities 2 5.0106352940962555 116952
complexity 49 1.8119621765455745 116972
complicated 8 3.624340932976365 117368
complicates 2 5.0106352940962555 117436
complications 2 5.0106352940962555 117456
component 4 4.31748811353631 117476
component.in 2 5.0106352940962555 117512
components 18 2.8134107167600364 117532
composability 2 5.0106352940962555 117680
composable 2 5.0106352940962555 117700
compose 4 4.31748811353631 117720
composed 10 3.4011973816621555 117756
composing 2 5.0106352940962555 117840
composite 4 4.31748811353631 117860
composition 6 3.912023005428146 117896
compositional 11 3.3058872018578307 117948
compositionality 2 5.0106352940962555 118040
compositionally 2 5.0106352940962555 118060
compositions 2 5.0106352940962555 118080
compounding 2 5.0106352940962555 118100
comprehensive 22 2.6127400212978853 118120
comprehensively 2 5.0106352940962555 118300
compress 4 4.31748811353631 118320
compressed 2 5.0106352940962555 118356
compressing 6 3.912023005428146 118376
compression 26 2.445685936634719 118428
compressive 2 5.0106352940962555 118640
comprises 2 5.0106352940962555 118660
comprising 2 5.0106352940962555 118680
compromise 4 4.31748811353631 118700
compromised 2 5.0106352940962555 118736
compromising 2 5.0106352940962555 118756
computation 40 2.0149030205422647 118776
computation-efficient 2 5.0106352940962555 119100
computational 58 1.6433394641097818 119120
computationally 28 2.371577964480997 119588
computationally-efficient 2 5.0106352940962555 119816
computations 2 5.0106352940962555 119836
compute 30 2.302585092994046 119856
computed 6 3.912023005428146 120100
computer 22 2.6127400212978853 120152
computers 2 5.0106352940962555 120332
computes 8 3.624340932976365 120352
computing 8 3.624340932976365 120420
computing-friendly 2 5.0106352940962555 120488
concatenated 2 5.0106352940962555 120508
concatenation.published 2 5.0106352940962555 120528
concentration 2 5.0106352940962555 120548
concept 15 2.995732273553991 120568
concept-based 2 5.0106352940962555 120692
concept-level 2 5.0106352940962555 120712
conceptlevel 2 5.0106352940962555 120732
conceptor 2 5.0106352940962555 120752
concepts 8 3.624340932976365 120772
conceptually 2 5.0106352940962555 120840
concern 4 4.31748811353631 120860
concerning 4 4.31748811353631 120896
concerningly 2 5.0106352940962555 120932
concerns 8 3.624340932976365 120952
concise 5 4.0943445622221 121020
conclude 2 5.0106352940962555 121064
conclusion 2 5.0106352940962555 121084
conclusions 2 5.0106352940962555 121104
concrete 6 3.912023005428146 121124
concretely 2 5.0106352940962555 121176
concurrent 2 5.0106352940962555 121196
concurrently 2 5.0106352940962555 121216
condition 12 3.2188758248682006 121236
conditional 27 2.407945608651872 121336
conditioned 22 2.6127400212978853 121556
conditioning 14 3.0647251450409425 121736
conditions 28 2.371577964480997 121852
conditions.our 2 5.0106352940962555 122080
conduct 28 2.371577964480997 122100
conducted 6 3.912023005428146 122328
confaide 2 5.0106352940962555 122380
conference 52 1.7525387560747736 122400
conferring 2 5.0106352940962555 122820
confidence 6 3.912023005428146 122840
confidence-calibrated 2 5.0106352940962555 122892
confident 2 5.0106352940962555 122912
confined 4 4.31748811353631 122932
confirm 2 5.0106352940962555 122968
confirmation 5 4.0943445622221 122988
confirmed 2 5.0106352940962555 123032
confirming 2 5.0106352940962555 123052
conflicting 2 5.0106352940962555 123072
conflicts 3 4.605170185988092 123092
conflicts.we 2 5.0106352940962555 123120
conform 2 5.0106352940962555 123140
conformal 3 4.605170185988092 123160
conformations 2 5.0106352940962555 123188
conformer 3 4.605170185988092 123208
confounding 2 5.0106352940962555 123236
confronted 2 5.0106352940962555 123256
confusion 2 5.0106352940962555 123276
conjugate 4 4.31748811353631 123296
conjunction 4 4.31748811353631 123332
connect 2 5.0106352940962555 123368
connected 12 3.2188758248682006 123388
connecting 4 4.31748811353631 123488
connection 8 3.624340932976365 123524
connections 15 2.995732273553991 123592
connectivity 4 4.31748811353631 123716
connectivity-oriented 2 5.0106352940962555 123752
connectome 2 5.0106352940962555 123772
connectome.here 2 5.0106352940962555 123792
consecutive 2 5.0106352940962555 123812
consensus 8 3.624340932976365 123832
consequence 4 4.31748811353631 123900
consequences 2 5.0106352940962555 123936
consequential 2 5.0106352940962555 123956
consequently 2 5.0106352940962555 123976
conservative 2 5.0106352940962555 123996
consider 22 2.6127400212978853 124016
considerable 6 3.912023005428146 124196
considerably 4 4.31748811353631 124248
considerably.non-ar 2 5.0106352940962555 124284
consideration 2 5.0106352940962555 124304
considered 22 2.6127400212978853 124324
considered.these 2 5.0106352940962555 124504
considering 10 3.4011973816621555 124524
considers 6 3.912023005428146 124608
consistency 12 3.2188758248682006 124660
consistent 24 2.5257286443082556 124760
consistently 42 1.9661128563728327 124956
consisting 2 5.0106352940962555 125296
consists 24 2.5257286443082556 125316
consolidation 2 5.0106352940962555 125512
constant 10 3.4011973816621555 125532
constantly 2 5.0106352940962555 125616
constantmemory 2 5.0106352940962555 125636
constants 2 5.0106352940962555 125656
constituent 4 4.31748811353631 125676
constitute 6 3.912023005428146 125712
constituted 2 5.0106352940962555 125764
constrain 4 4.31748811353631 125784
constrained 20 2.70805020110221 125820
constrains 6 3.912023005428146 125984
constraint 11 3.3058872018578307 126036
constraints 20 2.70805020110221 126128
constraints.experiments 2 5.0106352940962555 126292
constraints.leveraging 2 5.0106352940962555 126312
constrastive 2 5.0106352940962555 126332
construct 22 2.6127400212978853 126352
constructed 12 3.2188758248682006 126532
constructing 8 3.624340932976365 126632
construction 6 3.912023005428146 126700
constructive 2 5.0106352940962555 126752
constructs 4 4.31748811353631 126772
consume 2 5.0106352940962555 126808
consumer-facing 2 5.0106352940962555 126828
consumers 2 5.0106352940962555 126848
consumption 10 3.4011973816621555 126868
contact 2 5.0106352940962555 126952
contain 6 3.912023005428146 126972
contained 2 5.0106352940962555 127024
containing 12 3.2188758248682006 127044
contains 16 2.9311937524164198 127144
contaminate 2 5.0106352940962555 127276
contemporary 2 5.0106352940962555 127296
contend 2 5.0106352940962555 127316
content 18 2.8134107167600364 127336
content-independent 2 5.0106352940962555 127484
content-level 2 5.0106352940962555 127504
content.we 2 5.0106352940962555 127524
context 38 2.0661963149298153 127544
context-aware 3 4.605170185988092 127852
context-specific.also 2 5.0106352940962555 127880
context.in 2 5.0106352940962555 127900
contexts 4 4.31748811353631 127920
contexts.our 2 5.0106352940962555 127956
contextual 7 3.7578723256008875 127976
contextualized 2 5.0106352940962555 128036
contiguous 2 5.0106352940962555 128056
continual 8 3.624340932976365 128076
continuation 2 5.0106352940962555 128144
continue 4 4.31748811353631 128164
continues 12 3.2188758248682006 128200
continuity 2 5.0106352940962555 128300
continuity-based 2 5.0106352940962555 128320
continuous 45 1.8971199848858813 128340
continuous-control 2 5.0106352940962555 128704
continuous-time 10 3.4011973816621555 128724
continuously 8 3.624340932976365 128808
contradicting 2 5.0106352940962555 128876
contrary 6 3.912023005428146 128896
contrast 44 1.91959284073794 128948
contrasted 2 5.0106352940962555 129304
contrasting 2 5.0106352940962555 129324
contrastive 16 2.9311937524164198 129344
contrastively 2 5.0106352940962555 129476
contrasts 4 4.31748811353631 129496
contribute 8 3.624340932976365 129532
contributed 12 3.2188758248682006 129600
contributing 4 4.31748811353631 129700
contribution 42 1.9661128563728327 129736
contribution.arxiv 2 5.0106352940962555 130076
contribution.code 2 5.0106352940962555 130096
contribution.ordering 2 5.0106352940962555 130116
contribution.published 4 4.31748811353631 130136
contributions 30 2.302585092994046 130172
contributions.first 2 5.0106352940962555 130416
contributions.published 2 5.0106352940962555 130436
control 40 2.0149030205422647 130456
control-theoretic 2 5.0106352940962555 130780
controllability 2 5.0106352940962555 130800
controllable 2 5.0106352940962555 130820
controlled 9 3.506557897319982 130840
controller 2 5.0106352940962555 130916
controlling 4 4.31748811353631 130936
controlnet 2 5.0106352940962555 130972
controls 2 5.0106352940962555 130992
controlvideo 3 4.605170185988092 131012
controlvideo.recent 2 5.0106352940962555 131040
conv 2 5.0106352940962555 131060
conventional 22 2.6127400212978853 131080
converge 12 3.2188758248682006 131260
convergence 49 1.8119621765455745 131360
convergent 6 3.912023005428146 131756
converges 12 3.2188758248682006 131808
converging 2 5.0106352940962555 131908
conversations 2 5.0106352940962555 131928
conversion 2 5.0106352940962555 131948
convert 2 5.0106352940962555 131968
converted 2 5.0106352940962555 131988
converts 2 5.0106352940962555 132008
convex 19 2.7593434954897607 132028
convex-concave 2 5.0106352940962555 132184
convexity 8 3.624340932976365 132204
convey 4 4.31748811353631 132272
conveys 2 5.0106352940962555 132308
convincing 2 5.0106352940962555 132328
convincing.on 2 5.0106352940962555 132348
convolution 4 4.31748811353631 132368
convolutional 34 2.17742195004004 132404
convolutions 8 3.624340932976365 132680
coordinate 2 5.0106352940962555 132748
coordinates 2 5.0106352940962555 132768
cope 2 5.0106352940962555 132788
coped 2 5.0106352940962555 132808
copy 2 5.0106352940962555 132828
copying 2 5.0106352940962555 132848
core 2 5.0106352940962555 132868
cores 2 5.0106352940962555 132888
corl 2 5.0106352940962555 132908
corpora 8 3.624340932976365 132928
corpus 6 3.912023005428146 132996
corr 2 5.0106352940962555 133048
correct 8 3.624340932976365 133068
correcting 2 5.0106352940962555 133136
correction 4 4.31748811353631 133156
corrective 3 4.605170185988092 133192
correctly 4 4.31748811353631 133220
correctness 2 5.0106352940962555 133256
correctness.in 2 5.0106352940962555 133276
corrects 2 5.0106352940962555 133296
correlated 4 4.31748811353631 133316
correlation 9 3.506557897319982 133352
correlations 8 3.624340932976365 133428
correspondences 2 5.0106352940962555 133496
corresponding 34 2.17742195004004 133516
corresponds 4 4.31748811353631 133792
corroborate 2 5.0106352940962555 133828
corrupted 2 5.0106352940962555 133848
cosine 4 4.31748811353631 133868
cost 48 1.8325814637483102 133904
cost-effective 4 4.31748811353631 134292
cost.typical 2 5.0106352940962555 134328
costly 10 3.4011973816621555 134348
costs 14 3.0647251450409425 134432
costs.experiments 2 5.0106352940962555 134548
costs.we 2 5.0106352940962555 134568
cot 4 4.31748811353631 134588
cot3dref 3 4.605170185988092 134624
couch-potato 2 5.0106352940962555 134652
could 32 2.2380465718564744 134672
counter 3 4.605170185988092 134932
counter-intuitive 2 5.0106352940962555 134960
counter-memory 2 5.0106352940962555 134980
counterbased 2 5.0106352940962555 135000
counterfactual 8 3.624340932976365 135020
countering 2 5.0106352940962555 135088
counterpart 2 5.0106352940962555 135108
counterparts 18 2.8134107167600364 135128
counting 2 5.0106352940962555 135276
countries 2 5.0106352940962555 135296
counts.like 2 5.0106352940962555 135316
couple 2 5.0106352940962555 135336
coupled 8 3.624340932976365 135356
coupling 2 5.0106352940962555 135424
couplings 2 5.0106352940962555 135444
course 12 3.2188758248682006 135464
courty 2 5.0106352940962555 135564
covariance 4 4.31748811353631 135584
covariant 2 5.0106352940962555 135620
coverage 6 3.912023005428146 135640
covering 6 3.912023005428146 135692
covers 2 5.0106352940962555 135744
cp-flow 2 5.0106352940962555 135764
cp-flows 2 5.0106352940962555 135784
cql 1 5.703782474656201 135804
craft 2 5.0106352940962555 135816
crafting 2 5.0106352940962555 135836
crawl 2 5.0106352940962555 135856
cream 2 5.0106352940962555 135876
create 12 3.2188758248682006 135896
created 2 5.0106352940962555 135996
creates 6 3.912023005428146 136016
creating 8 3.624340932976365 136068
creation 2 5.0106352940962555 136136
creative 2 5.0106352940962555 136156
credit 6 3.912023005428146 136176
criteria 12 3.2188758248682006 136228
criteria.existing 2 5.0106352940962555 136328
criterion 10 3.4011973816621555 136348
critic 6 3.912023005428146 136432
critical 20 2.70805020110221 136484
critical.however 2 5.0106352940962555 136648
critically 4 4.31748811353631 136668
critics 2 5.0106352940962555 136704
crops 2 5.0106352940962555 136724
crops.this 2 5.0106352940962555 136744
cross 2 5.0106352940962555 136764
cross-attention 2 5.0106352940962555 136784
cross-attention.we 2 5.0106352940962555 136804
cross-domain 2 5.0106352940962555 136824
cross-entropy 6 3.912023005428146 136844
cross-file 2 5.0106352940962555 136896
cross-frame 2 5.0106352940962555 136916
cross-relations 2 5.0106352940962555 136936
cross-sectional 2 5.0106352940962555 136956
cross-validate 2 5.0106352940962555 136976
crossattention 2 5.0106352940962555 136996
crossframe 2 5.0106352940962555 137016
crowdworkers 2 5.0106352940962555 137036
crown 2 5.0106352940962555 137056
crown-ibp 2 5.0106352940962555 137076
crucial 24 2.5257286443082556 137096
crucially 6 3.912023005428146 137292
crying 2 5.0106352940962555 137344
crystal 3 4.605170185988092 137364
cs.cl 6 3.912023005428146 137392
cs.cv 10 3.4011973816621555 137444
cs.lg 34 2.17742195004004 137528
cs.toronto.edu 2 5.0106352940962555 137804
csd 2 5.0106352940962555 137824
ctx-docnade 2 5.0106352940962555 137844
ctx-docnadee 2 5.0106352940962555 137864
cube 4 4.31748811353631 137884
cue 2 5.0106352940962555 137920
cues 6 3.912023005428146 137940
culprit 2 5.0106352940962555 137992
cumulative 4 4.31748811353631 138012
cup 1 5.703782474656201 138048
curated 6 3.912023005428146 138060
curiosity 3 4.605170185988092 138112
curious 4 4.31748811353631 138140
current 70 1.455287232606842 138176
currently 6 3.912023005428146 138740
curriculum 2 5.0106352940962555 138792
curse 4 4.31748811353631 138812
curvature 6 3.912023005428146 138848
curve 6 3.912023005428146 138900
curve.figure 2 5.0106352940962555 138952
custom 2 5.0106352940962555 138972
cut 1 5.703782474656201 138992
cwt 2 5.0106352940962555 139004
cycles 4 4.31748811353631 139024
d 15 2.995732273553991 139060
d-dimensional 4 4.31748811353631 139184
d-snr 2 5.0106352940962555 139220
d. 1 5.703782474656201 139240
d.4 2 5.0106352940962555 139252
d3pm 2 5.0106352940962555 139272
d4rl 6 3.912023005428146 139292
dabp 2 5.0106352940962555 139344
daily 2 5.0106352940962555 139364
damped 2 5.0106352940962555 139384
damping 2 5.0106352940962555 139404
dan 1 5.703782474656201 139424
dane 2 5.0106352940962555 139436
daniel 4 4.31748811353631 139456
daniilidis-group.github.io 2 5.0106352940962555 139492
dark 2 5.0106352940962555 139512
data 262 0.13543797089510412 139532
data-constrained 2 5.0106352940962555 141632
data-dependent 2 5.0106352940962555 141652
data-driven 10 3.4011973816621555 141672
data-efficiency 2 5.0106352940962555 141756
data-efficient 15 2.995732273553991 141776
data-fidelity 2 5.0106352940962555 141900
data-fitting 2 5.0106352940962555 141920
data.first 2 5.0106352940962555 141940
data.for 2 5.0106352940962555 141960
data.herein 2 5.0106352940962555 141980
data.however 2 5.0106352940962555 142000
data.in 2 5.0106352940962555 142020
data.the 2 5.0106352940962555 142040
data.we 2 5.0106352940962555 142060
datapoint 2 5.0106352940962555 142080
datapoints 6 3.912023005428146 142100
dataset 87 1.2378743560016172 142152
dataset-specific 2 5.0106352940962555 142852
dataset.these 2 5.0106352940962555 142872
dataset.training 2 5.0106352940962555 142892
datasets 121 0.90799192905946 142912
datasets.in 2 5.0106352940962555 143884
datasets.preprint 2 5.0106352940962555 143904
datasets.ultimately 2 5.0106352940962555 143924
datasetspecific 2 5.0106352940962555 143944
date 4 4.31748811353631 143964
david 2 5.0106352940962555 144000
dawn 4 4.31748811353631 144020
day 4 4.31748811353631 144056
dcm 2 5.0106352940962555 144092
ddim 4 4.31748811353631 144112
ddpm 4 4.31748811353631 144148
de 5 4.0943445622221 144184
de-randomized 3 4.605170185988092 144228
deal 12 3.2188758248682006 144256
dealing 4 4.31748811353631 144356
deals 4 4.31748811353631 144392
debiasing 2 5.0106352940962555 144428
deblurring 2 5.0106352940962555 144448
debug 2 5.0106352940962555 144468
dec 1 5.703782474656201 144488
decade 4 4.31748811353631 144500
decades.consequently 2 5.0106352940962555 144536
decay 11 3.3058872018578307 144556
decays 4 4.31748811353631 144648
decentralized 4 4.31748811353631 144684
deceptive 2 5.0106352940962555 144720
decide 4 4.31748811353631 144740
decided 2 5.0106352940962555 144776
decides 2 5.0106352940962555 144796
decipher 2 5.0106352940962555 144816
decision 26 2.445685936634719 144836
decision-making 6 3.912023005428146 145048
decision-pretrained 2 5.0106352940962555 145100
decisions 8 3.624340932976365 145120
declarative 2 5.0106352940962555 145188
decoded 2 5.0106352940962555 145208
decoder 9 3.506557897319982 145228
decodes 2 5.0106352940962555 145304
decoding 12 3.2188758248682006 145324
decomposability 2 5.0106352940962555 145424
decompose 4 4.31748811353631 145444
decomposed 3 4.605170185988092 145480
decomposer 2 5.0106352940962555 145508
decomposing 4 4.31748811353631 145528
decomposition 14 3.0647251450409425 145564
decomposition-based 2 5.0106352940962555 145680
decompositions 2 5.0106352940962555 145700
decontextualization 3 4.605170185988092 145720
decouple 4 4.31748811353631 145748
decoupled 2 5.0106352940962555 145784
decouples 4 4.31748811353631 145804
decoupling 4 4.31748811353631 145840
decrease 2 5.0106352940962555 145876
decrease.motivated 2 5.0106352940962555 145896
decreases 2 5.0106352940962555 145916
decreases.while 2 5.0106352940962555 145936
decreasing 10 3.4011973816621555 145956
dedicated 4 4.31748811353631 146040
deduce 2 5.0106352940962555 146076
deemed 2 5.0106352940962555 146096
deep 148 0.7065702008920861 146116
deep-neural-network-based 2 5.0106352940962555 147304
deep_representation_one_class 2 5.0106352940962555 147324
deepcoder 2 5.0106352940962555 147344
deepening 2 5.0106352940962555 147364
deeper 7 3.7578723256008875 147384
deepmind 4 4.31748811353631 147444
def 1 5.703782474656201 147480
default 2 5.0106352940962555 147492
defend 4 4.31748811353631 147512
defending 2 5.0106352940962555 147548
defense 12 3.2188758248682006 147568
defenses 6 3.912023005428146 147668
deficiency 2 5.0106352940962555 147720
definable 2 5.0106352940962555 147740
define 14 3.0647251450409425 147760
defined 22 2.6127400212978853 147876
defining 4 4.31748811353631 148056
definite 2 5.0106352940962555 148092
definition 10 3.4011973816621555 148112
definitions 2 5.0106352940962555 148196
definitive 2 5.0106352940962555 148216
deformable 3 4.605170185988092 148236
deformation 2 5.0106352940962555 148264
degenerate 2 5.0106352940962555 148284
degeneration 4 4.31748811353631 148304
degradation 12 3.2188758248682006 148340
degrades 2 5.0106352940962555 148440
degree 4 4.31748811353631 148460
degrees 2 5.0106352940962555 148496
delay 2 5.0106352940962555 148516
delay-aware 2 5.0106352940962555 148536
delayed 4 4.31748811353631 148556
delays 2 5.0106352940962555 148592
delete 2 5.0106352940962555 148612
deleted 3 4.605170185988092 148632
deleting 2 5.0106352940962555 148660
delicious 2 5.0106352940962555 148680
delineate 2 5.0106352940962555 148700
deliver 2 5.0106352940962555 148720
delivers 4 4.31748811353631 148740
delved 2 5.0106352940962555 148776
demand 2 5.0106352940962555 148796
demanding 2 5.0106352940962555 148816
demands 4 4.31748811353631 148836
demo 2 5.0106352940962555 148872
demographic 2 5.0106352940962555 148892
demonstrable 2 5.0106352940962555 148912
demonstrate 170 0.5679840376059393 148932
demonstrated 28 2.371577964480997 150296
demonstrates 18 2.8134107167600364 150524
demonstrating 16 2.9311937524164198 150672
demonstration 6 3.912023005428146 150804
demonstrations 17 2.870569130599985 150856
demonstrative 2 5.0106352940962555 150996
demos 2 5.0106352940962555 151016
demystifying 4 4.31748811353631 151036
den 2 5.0106352940962555 151072
denied 2 5.0106352940962555 151092
denoiser 2 5.0106352940962555 151112
denoisers 2 5.0106352940962555 151132
denoising 18 2.8134107167600364 151152
denote 2 5.0106352940962555 151300
denotes 6 3.912023005428146 151320
dense 24 2.5257286443082556 151372
dense-reward 2 5.0106352940962555 151568
densely-trained 2 5.0106352940962555 151588
density 10 3.4011973816621555 151608
departing 2 5.0106352940962555 151692
depend 4 4.31748811353631 151712
dependence 10 3.4011973816621555 151748
dependencies 10 3.4011973816621555 151832
dependencies.in 2 5.0106352940962555 151916
dependency 2 5.0106352940962555 151936
depending 16 2.9311937524164198 151956
depends 14 3.0647251450409425 152088
depicting 2 5.0106352940962555 152204
deploy 4 4.31748811353631 152224
deployable 2 5.0106352940962555 152260
deployed 6 3.912023005428146 152280
deploying 2 5.0106352940962555 152332
deployment 14 3.0647251450409425 152352
deployments.most 2 5.0106352940962555 152468
deploys 4 4.31748811353631 152488
depth 11 3.3058872018578307 152524
depths 6 3.912023005428146 152616
dequantization 2 5.0106352940962555 152668
derivation 2 5.0106352940962555 152688
derivations 2 5.0106352940962555 152708
derivative 2 5.0106352940962555 152728
derivatives 4 4.31748811353631 152748
derive 26 2.445685936634719 152784
derived 8 3.624340932976365 152996
deriving 2 5.0106352940962555 153064
descent 44 1.91959284073794 153084
descent-ascent 4 4.31748811353631 153440
descent.arxiv 2 5.0106352940962555 153476
descent.however 2 5.0106352940962555 153496
descent.published 2 5.0106352940962555 153516
descents 2 5.0106352940962555 153536
describe 14 3.0647251450409425 153556
described 4 4.31748811353631 153672
describes 4 4.31748811353631 153708
describing 8 3.624340932976365 153744
description 2 5.0106352940962555 153812
descriptions 8 3.624340932976365 153832
descriptive 2 5.0106352940962555 153900
desiderata 4 4.31748811353631 153920
design 63 1.5606477482646683 153956
design.while 2 5.0106352940962555 154464
designed 30 2.302585092994046 154484
designers 2 5.0106352940962555 154728
designing 12 3.2188758248682006 154748
designs 10 3.4011973816621555 154848
desirable 8 3.624340932976365 154932
desired 24 2.5257286443082556 155000
despite 50 1.791759469228055 155196
destroy 2 5.0106352940962555 155600
detail 6 3.912023005428146 155620
detailed 6 3.912023005428146 155672
details 14 3.0647251450409425 155724
detect 6 3.912023005428146 155840
detectability 4 4.31748811353631 155892
detectable 2 5.0106352940962555 155928
detecting 9 3.506557897319982 155948
detection 34 2.17742195004004 156024
detection.specifically 2 5.0106352940962555 156300
detector 2 5.0106352940962555 156320
detectors 4 4.31748811353631 156340
deteriorate 2 5.0106352940962555 156376
deterioration 2 5.0106352940962555 156396
determine 6 3.912023005428146 156416
determined 14 3.0647251450409425 156468
determines 2 5.0106352940962555 156584
determining 2 5.0106352940962555 156604
deterministic 15 2.995732273553991 156624
deterministic-stochastic 2 5.0106352940962555 156748
detr 3 4.605170185988092 156768
develop 60 1.6094379124341003 156796
developed 16 2.9311937524164198 157280
developers.however 2 5.0106352940962555 157412
developing 20 2.70805020110221 157432
development 16 2.9311937524164198 157596
developments 2 5.0106352940962555 157728
develops 2 5.0106352940962555 157748
deviate 2 5.0106352940962555 157768
deviation 2 5.0106352940962555 157788
deviations 2 5.0106352940962555 157808
device 2 5.0106352940962555 157828
devices 4 4.31748811353631 157848
devise 6 3.912023005428146 157884
devlin 4 4.31748811353631 157936
devoid 2 5.0106352940962555 157972
devote 2 5.0106352940962555 157992
devoted 2 5.0106352940962555 158012
dexterous 2 5.0106352940962555 158032
dffhvv 2 5.0106352940962555 158052
dhillon 2 5.0106352940962555 158072
diagnosis 2 5.0106352940962555 158092
diagonal 2 5.0106352940962555 158112
dialog 2 5.0106352940962555 158132
dialograph 3 4.605170185988092 158152
dialogue 4 4.31748811353631 158180
dialogues 3 4.605170185988092 158216
diana 2 5.0106352940962555 158244
diego 2 5.0106352940962555 158264
dietterich 2 5.0106352940962555 158284
differ 4 4.31748811353631 158304
difference 25 2.4849066497880004 158340
differences 5 4.0943445622221 158544
different 161 0.622378109671738 158588
differentiability 2 5.0106352940962555 159880
differentiable 5 4.0943445622221 159900
differential 15 2.995732273553991 159944
differential-algebraic 2 5.0106352940962555 160068
differentialium 2 5.0106352940962555 160088
differentially 4 4.31748811353631 160108
differentiation 6 3.912023005428146 160144
differently 2 5.0106352940962555 160196
differing 6 3.912023005428146 160216
differs 2 5.0106352940962555 160268
difficult 34 2.17742195004004 160288
difficulties 4 4.31748811353631 160564
difficultly 2 5.0106352940962555 160600
difficulty 10 3.4011973816621555 160620
diffusion 59 1.6262450307504817 160704
diffusion-ql 2 5.0106352940962555 161180
diffusion-refinement 3 4.605170185988092 161200
diffusion_refinement 2 5.0106352940962555 161228
dilated 2 5.0106352940962555 161248
dilemma 2 5.0106352940962555 161268
dimension 23 2.5682882587270512 161288
dimension-and 2 5.0106352940962555 161476
dimension-dependent 2 5.0106352940962555 161496
dimensional 8 3.624340932976365 161516
dimensional-dependent 2 5.0106352940962555 161584
dimensionality 14 3.0647251450409425 161604
dimensions 8 3.624340932976365 161720
dimensions.published 2 5.0106352940962555 161788
diminishing 2 5.0106352940962555 161808
direct 26 2.445685936634719 161828
directed 2 5.0106352940962555 162040
direction 18 2.8134107167600364 162060
directional 4 4.31748811353631 162208
directions 5 4.0943445622221 162244
directions.marcel 2 5.0106352940962555 162288
directly 44 1.91959284073794 162308
disaggregating 2 5.0106352940962555 162664
disaggregation 2 5.0106352940962555 162684
disagreement-based 2 5.0106352940962555 162704
disassociate 2 5.0106352940962555 162724
discard 2 5.0106352940962555 162744
discern 2 5.0106352940962555 162764
discerning 2 5.0106352940962555 162784
disco 4 4.31748811353631 162804
discontinuity 2 5.0106352940962555 162840
discontinuous 2 5.0106352940962555 162860
discount 2 5.0106352940962555 162880
discounted 2 5.0106352940962555 162900
discover 22 2.6127400212978853 162920
discovered 2 5.0106352940962555 163100
discoveries 2 5.0106352940962555 163120
discovers 2 5.0106352940962555 163140
discovery 8 3.624340932976365 163160
discrepancy 12 3.2188758248682006 163228
discrete 30 2.302585092994046 163328
discretetime 2 5.0106352940962555 163572
discretization 6 3.912023005428146 163592
discretizations 2 5.0106352940962555 163644
discretize 2 5.0106352940962555 163664
discretized 4 4.31748811353631 163684
discriminable 2 5.0106352940962555 163720
discriminant 2 5.0106352940962555 163740
discriminate 2 5.0106352940962555 163760
discrimination 2 5.0106352940962555 163780
discriminative 13 3.138833117194664 163800
discriminator 8 3.624340932976365 163908
discriminators 6 3.912023005428146 163976
discuss 14 3.0647251450409425 164028
disentangle 2 5.0106352940962555 164144
disentangled 2 5.0106352940962555 164164
disentanglement 7 3.7578723256008875 164184
disentangles 2 5.0106352940962555 164244
disentangling 2 5.0106352940962555 164264
disguising 3 4.605170185988092 164284
disjoint 4 4.31748811353631 164312
disk 2 5.0106352940962555 164348
dispatch 2 5.0106352940962555 164368
display 2 5.0106352940962555 164388
displaying 2 5.0106352940962555 164408
displays 2 5.0106352940962555 164428
disregarding 2 5.0106352940962555 164448
disrupts 2 5.0106352940962555 164468
dissect 2 5.0106352940962555 164488
dissection 3 4.605170185988092 164508
dissimilarity 4 4.31748811353631 164536
distance 25 2.4849066497880004 164572
distance-independent 2 5.0106352940962555 164776
distance-preserving 2 5.0106352940962555 164796
distances 5 4.0943445622221 164816
distant 4 4.31748811353631 164860
distillation 19 2.7593434954897607 164896
distilled 2 5.0106352940962555 165052
distilling 5 4.0943445622221 165072
distinct 18 2.8134107167600364 165116
distinction 2 5.0106352940962555 165264
distinctive 2 5.0106352940962555 165284
distinguish 6 3.912023005428146 165304
distinguishable 4 4.31748811353631 165356
distinguished 2 5.0106352940962555 165392
distortion 2 5.0106352940962555 165412
distract 2 5.0106352940962555 165432
distraction 2 5.0106352940962555 165452
distractor 2 5.0106352940962555 165472
distractors 2 5.0106352940962555 165492
distributed 15 2.995732273553991 165512
distributing 2 5.0106352940962555 165636
distribution 100 1.0986122886681098 165656
distribution-augmented 2 5.0106352940962555 166460
distribution.however 2 5.0106352940962555 166480
distribution.in 2 5.0106352940962555 166500
distribution.similarity 2 5.0106352940962555 166520
distribution.such 2 5.0106352940962555 166540
distribution.to 2 5.0106352940962555 166560
distribution.we 2 5.0106352940962555 166580
distributional 4 4.31748811353631 166600
distributionally 8 3.624340932976365 166636
distributions 52 1.7525387560747736 166704
distributions.further 2 5.0106352940962555 167124
disturbances.while 2 5.0106352940962555 167144
dit 1 5.703782474656201 167164
divadi 2 5.0106352940962555 167176
divelab 2 5.0106352940962555 167196
divergence 12 3.2188758248682006 167216
divergences 2 5.0106352940962555 167316
divergent 4 4.31748811353631 167336
diverging 2 5.0106352940962555 167372
diverse 54 1.7147984280919266 167392
diversity 16 2.9311937524164198 167828
divide 2 5.0106352940962555 167960
divide-and-conquer 3 4.605170185988092 167980
divide-to-adapt 2 5.0106352940962555 168008
divided 2 5.0106352940962555 168028
divides 4 4.31748811353631 168048
dmlab 2 5.0106352940962555 168084
dnn 3 4.605170185988092 168104
dnns 12 3.2188758248682006 168132
dnp 1 5.703782474656201 168232
do 26 2.445685936634719 168244
doc-nade 2 5.0106352940962555 168456
docking 2 5.0106352940962555 168476
docnade 2 5.0106352940962555 168496
document 6 3.912023005428146 168516
documents 6 3.912023005428146 168568
does 58 1.6433394641097818 168620
dog 2 5.0106352940962555 169088
dogs 2 5.0106352940962555 169108
doing 10 3.4011973816621555 169128
dollars 4 4.31748811353631 169212
domain 33 2.207274913189721 169248
domain-agnostic 2 5.0106352940962555 169516
domain-invariant 2 5.0106352940962555 169536
domain-specific 2 5.0106352940962555 169556
domain.recently 2 5.0106352940962555 169576
domains 36 2.120263536200091 169596
domains.active 2 5.0106352940962555 169888
domains.its 2 5.0106352940962555 169908
domains.moreover 2 5.0106352940962555 169928
domains.we 2 5.0106352940962555 169948
domainsfrom 2 5.0106352940962555 169968
dominant 6 3.912023005428146 169988
dominates 2 5.0106352940962555 170040
don 4 4.31748811353631 170060
done 34 2.17742195004004 170096
done.2 2 5.0106352940962555 170372
dong 4 4.31748811353631 170392
dop 3 4.605170185988092 170428
dora 2 5.0106352940962555 170456
dormand 2 5.0106352940962555 170476
dot 1 5.703782474656201 170496
dot-product 2 5.0106352940962555 170508
douban 2 5.0106352940962555 170528
double 2 5.0106352940962555 170548
doubly-weighted 2 5.0106352940962555 170568
doucet 2 5.0106352940962555 170588
down 16 2.9311937524164198 170608
down-sampling 2 5.0106352940962555 170740
downstream 30 2.302585092994046 170760
dp-icl 2 5.0106352940962555 171004
dpolylog 2 5.0106352940962555 171024
dpt 1 5.703782474656201 171044
dq 1 5.703782474656201 171056
draft 2 5.0106352940962555 171068
drafts 2 5.0106352940962555 171088
dramatic 4 4.31748811353631 171108
dramatically 6 3.912023005428146 171144
drastic 2 5.0106352940962555 171196
drastically 4 4.31748811353631 171216
draw 2 5.0106352940962555 171252
drawing 6 3.912023005428146 171272
drawn 10 3.4011973816621555 171324
draws 2 5.0106352940962555 171408
dream 2 5.0106352940962555 171428
dreamer 2 5.0106352940962555 171448
drift 6 3.912023005428146 171468
drive 2 5.0106352940962555 171520
driven 9 3.506557897319982 171540
driver 2 5.0106352940962555 171616
driving 4 4.31748811353631 171636
dro 2 5.0106352940962555 171672
drone 2 5.0106352940962555 171692
drop 10 3.4011973816621555 171712
drop-in 4 4.31748811353631 171796
dropout 6 3.912023005428146 171832
dropped 2 5.0106352940962555 171884
dropping 2 5.0106352940962555 171904
drops 6 3.912023005428146 171924
drsm 3 4.605170185988092 171976
drsy 2 5.0106352940962555 172004
drug 2 5.0106352940962555 172024
drug-like 2 5.0106352940962555 172044
drums 2 5.0106352940962555 172064
dual 4 4.31748811353631 172084
dual-path 2 5.0106352940962555 172120
duality 2 5.0106352940962555 172140
dubbed 10 3.4011973816621555 172160
duchi 2 5.0106352940962555 172244
due 31 2.2697952701710546 172264
during 68 1.4842747694800944 172516
dwivedi 2 5.0106352940962555 173064
dwork 2 5.0106352940962555 173084
dylan 2 5.0106352940962555 173104
dyna 3 4.605170185988092 173124
dyna-style 2 5.0106352940962555 173152
dynamic 32 2.2380465718564744 173172
dynamical 16 2.9311937524164198 173432
dynamically 18 2.8134107167600364 173564
dynamics 42 1.9661128563728327 173712
dynamics.we 2 5.0106352940962555 174052
dfossez 2 5.0106352940962555 174072
e 8 3.624340932976365 174092
e. 1 5.703782474656201 174160
e.3 2 5.0106352940962555 174172
e.g 60 1.6094379124341003 174192
e.g.shortest 2 5.0106352940962555 174676
each 112 0.9852836033611064 174696
early 11 3.3058872018578307 175596
early-time 2 5.0106352940962555 175688
earth 3 4.605170185988092 175708
ease 10 3.4011973816621555 175736
easier 10 3.4011973816621555 175820
easily 14 3.0647251450409425 175904
easy 20 2.70805020110221 176020
easy-to-adapt 2 5.0106352940962555 176184
easy-to-implement 2 5.0106352940962555 176204
easy-to-use 2 5.0106352940962555 176224
eb 2 5.0106352940962555 176244
ebms 2 5.0106352940962555 176264
ecgan 2 5.0106352940962555 176284
economical 4 4.31748811353631 176304
edac 2 5.0106352940962555 176340
edge 11 3.3058872018578307 176360
edge-deleted 2 5.0106352940962555 176452
edges 8 3.624340932976365 176472
edit 4 4.31748811353631 176540
edited 2 5.0106352940962555 176576
editers 2 5.0106352940962555 176596
editing 8 3.624340932976365 176616
edits 6 3.912023005428146 176684
edm 2 5.0106352940962555 176736
edm-eswe 2 5.0106352940962555 176756
edqn 2 5.0106352940962555 176776
edqn.sorry 2 5.0106352940962555 176796
effect 23 2.5682882587270512 176816
effect.this 2 5.0106352940962555 177004
effect.we 2 5.0106352940962555 177024
effective 85 1.2611312181658845 177044
effective.we 2 5.0106352940962555 177728
effectively 36 2.120263536200091 177748
effectiveness 52 1.7525387560747736 178040
effects 10 3.4011973816621555 178460
efficacy 20 2.70805020110221 178544
efficiency 34 2.17742195004004 178708
efficiency.the 2 5.0106352940962555 178984
efficiency.we 2 5.0106352940962555 179004
efficient 104 1.0593915755148284 179024
efficient.both 2 5.0106352940962555 179860
efficiently 54 1.7147984280919266 179880
effort 6 3.912023005428146 180316
effort.recurrent 2 5.0106352940962555 180368
efforts 12 3.2188758248682006 180388
efforts.automated 2 5.0106352940962555 180488
ego-networks 2 5.0106352940962555 180508
eigendirections 2 5.0106352940962555 180528
eigenfunctions 4 4.31748811353631 180548
eigenspectrum 2 5.0106352940962555 180584
eigenvalue 2 5.0106352940962555 180604
eigenvalues 6 3.912023005428146 180624
einsum 2 5.0106352940962555 180676
either 42 1.9661128563728327 180696
elaborately 2 5.0106352940962555 181036
elastic 2 5.0106352940962555 181056
elasticity-based 2 5.0106352940962555 181076
elbo 2 5.0106352940962555 181096
elbow-wrist 2 5.0106352940962555 181116
electra 2 5.0106352940962555 181136
electronic 2 5.0106352940962555 181156
elegant 2 5.0106352940962555 181176
element 2 5.0106352940962555 181196
element-wise 2 5.0106352940962555 181216
elementary 10 3.4011973816621555 181236
elements 10 3.4011973816621555 181320
eleven 2 5.0106352940962555 181404
elicit 4 4.31748811353631 181424
eliciting 2 5.0106352940962555 181460
eliminate 6 3.912023005428146 181480
eliminated 2 5.0106352940962555 181532
eliminates 6 3.912023005428146 181552
eliminating 2 5.0106352940962555 181604
elmes 2 5.0106352940962555 181624
else 2 5.0106352940962555 181644
elucidating 2 5.0106352940962555 181664
elucidation 2 5.0106352940962555 181684
elusive 4 4.31748811353631 181704
elusive.this 2 5.0106352940962555 181740
em-based 2 5.0106352940962555 181760
email 2 5.0106352940962555 181780
embed 4 4.31748811353631 181800
embedded 4 4.31748811353631 181836
embedding 30 2.302585092994046 181872
embedding.our 2 5.0106352940962555 182116
embeddingbased 2 5.0106352940962555 182136
embeddings 25 2.4849066497880004 182156
embeddings.it 2 5.0106352940962555 182360
embodied 7 3.7578723256008875 182380
embrace 2 5.0106352940962555 182440
emerald 2 5.0106352940962555 182460
emerge 4 4.31748811353631 182480
emerged 12 3.2188758248682006 182516
emergence 2 5.0106352940962555 182616
emergent 13 3.138833117194664 182636
emerges 2 5.0106352940962555 182744
emerging 6 3.912023005428146 182764
eminent 2 5.0106352940962555 182816
emission 2 5.0106352940962555 182836
emitted 2 5.0106352940962555 182856
emo 3 4.605170185988092 182876
emoji 2 5.0106352940962555 182904
emphasis 4 4.31748811353631 182924
empirical 72 1.4271163556401458 182960
empirically 56 1.6784307839210517 183540
employ 20 2.70805020110221 183992
employed 8 3.624340932976365 184156
employing 8 3.624340932976365 184224
employment 2 5.0106352940962555 184292
employs 12 3.2188758248682006 184312
empower 2 5.0106352940962555 184412
empowered 2 5.0106352940962555 184432
empowers 4 4.31748811353631 184452
empty 2 5.0106352940962555 184488
emulate 6 3.912023005428146 184508
emulates 2 5.0106352940962555 184560
emulator 2 5.0106352940962555 184580
en 2 5.0106352940962555 184600
en-de 2 5.0106352940962555 184620
enable 28 2.371577964480997 184640
enabled 6 3.912023005428146 184868
enables 50 1.791759469228055 184920
enabling 32 2.2380465718564744 185324
enciphered 2 5.0106352940962555 185584
encode 8 3.624340932976365 185604
encoded 8 3.624340932976365 185672
encoder 13 3.138833117194664 185740
encoder-decoder 4 4.31748811353631 185848
encoders 12 3.2188758248682006 185884
encodes 8 3.624340932976365 185984
encoding 17 2.870569130599985 186052
encodings 2 5.0106352940962555 186192
encompass 6 3.912023005428146 186212
encompasses 4 4.31748811353631 186264
encompassing 2 5.0106352940962555 186300
encounter 2 5.0106352940962555 186320
encountered 6 3.912023005428146 186340
encountering 2 5.0106352940962555 186392
encourage 6 3.912023005428146 186412
encourages 6 3.912023005428146 186464
encouraging 10 3.4011973816621555 186516
end 26 2.445685936634719 186600
end-effector 2 5.0106352940962555 186812
end-to-end 24 2.5257286443082556 186832
endeavors 2 5.0106352940962555 187028
endow 2 5.0106352940962555 187048
energy 12 3.2188758248682006 187068
energy-based 7 3.7578723256008875 187168
energy-efficient 2 5.0106352940962555 187228
energy-guided 2 5.0106352940962555 187248
enforce 8 3.624340932976365 187268
enforces 2 5.0106352940962555 187336
enforcing 4 4.31748811353631 187356
engaging 4 4.31748811353631 187392
engineered 4 4.31748811353631 187428
engineering 9 3.506557897319982 187464
engines 2 5.0106352940962555 187540
english 2 5.0106352940962555 187560
english-to-german 2 5.0106352940962555 187580
english.we 2 5.0106352940962555 187600
enhance 20 2.70805020110221 187620
enhanced 2 5.0106352940962555 187784
enhancements 4 4.31748811353631 187804
enhances 8 3.624340932976365 187840
enhancing 8 3.624340932976365 187908
enjoy 2 5.0106352940962555 187976
enjoyed 2 5.0106352940962555 187996
enjoys 8 3.624340932976365 188016
enlarging 2 5.0106352940962555 188084
enormous 6 3.912023005428146 188104
enough 22 2.6127400212978853 188156
enrich 2 5.0106352940962555 188336
ensemble 12 3.2188758248682006 188356
ensembles 3 4.605170185988092 188456
ensembling 2 5.0106352940962555 188484
ensuing 4 4.31748811353631 188504
ensure 12 3.2188758248682006 188540
ensures 6 3.912023005428146 188640
ensuring 2 5.0106352940962555 188692
entailment 4 4.31748811353631 188712
entanglement 2 5.0106352940962555 188748
entire 8 3.624340932976365 188768
entities 8 3.624340932976365 188836
entity 2 5.0106352940962555 188904
entitydrawbench 2 5.0106352940962555 188924
entries 2 5.0106352940962555 188944
entropy 12 3.2188758248682006 188964
entropy-minimization 2 5.0106352940962555 189064
entropy-regularized 2 5.0106352940962555 189084
entry 2 5.0106352940962555 189104
enumerate 2 5.0106352940962555 189124
environment 31 2.2697952701710546 189144
environment-conditioned 2 5.0106352940962555 189396
environment-probing 2 5.0106352940962555 189416
environments 38 2.0661963149298153 189436
environmentspecific 2 5.0106352940962555 189744
enzyme 2 5.0106352940962555 189764
enzymes 2 5.0106352940962555 189784
epi 2 5.0106352940962555 189804
epi-conditioned 2 5.0106352940962555 189824
epi-policies 2 5.0106352940962555 189844
epi-policy 2 5.0106352940962555 189864
episode 2 5.0106352940962555 189884
episodes 4 4.31748811353631 189904
episodes.this 2 5.0106352940962555 189940
episodic 5 4.0943445622221 189960
epochs 8 3.624340932976365 190004
epsilon 2 5.0106352940962555 190072
eq 3 4.605170185988092 190092
equal 54 1.7147984280919266 190120
equal-eigenvalue 2 5.0106352940962555 190556
equally 18 2.8134107167600364 190576
equally.work 2 5.0106352940962555 190724
equation 6 3.912023005428146 190744
equations 11 3.3058872018578307 190796
equilibria 4 4.31748811353631 190888
equilibrium 4 4.31748811353631 190924
equip 4 4.31748811353631 190960
equipped 6 3.912023005428146 190996
equivalence 4 4.31748811353631 191048
equivalent 24 2.5257286443082556 191084
equivariance 2 5.0106352940962555 191280
equivariant 10 3.4011973816621555 191300
er 1 5.703782474656201 191384
erm 3 4.605170185988092 191396
erm-trained 2 5.0106352940962555 191424
error 54 1.7147984280919266 191444
error-bound 2 5.0106352940962555 191880
error.we 2 5.0106352940962555 191900
errors 16 2.9311937524164198 191920
errors.we 2 5.0106352940962555 192052
esan 2 5.0106352940962555 192072
esbn 2 5.0106352940962555 192092
escape 2 5.0106352940962555 192112
escaping-saddle 2 5.0106352940962555 192132
escher 3 4.605170185988092 192152
eschewing 2 5.0106352940962555 192180
especially 38 2.0661963149298153 192200
essence 4 4.31748811353631 192508
essential 24 2.5257286443082556 192544
essentially 2 5.0106352940962555 192740
establish 20 2.70805020110221 192760
established 10 3.4011973816621555 192924
establishes 6 3.912023005428146 193008
establishing 4 4.31748811353631 193060
estimate 18 2.8134107167600364 193096
estimated 4 4.31748811353631 193244
estimates 14 3.0647251450409425 193280
estimating 6 3.912023005428146 193396
estimation 30 2.302585092994046 193448
estimations 4 4.31748811353631 193692
estimator 6 3.912023005428146 193728
estimators 2 5.0106352940962555 193780
et 43 1.9425823589626385 193800
etc 12 3.2188758248682006 194148
ethical 2 5.0106352940962555 194248
ethics 2 5.0106352940962555 194268
etp 2 5.0106352940962555 194288
euclidean 11 3.3058872018578307 194308
euler 2 5.0106352940962555 194400
evaded 2 5.0106352940962555 194420
evaluate 42 1.9661128563728327 194440
evaluated 4 4.31748811353631 194780
evaluates 4 4.31748811353631 194816
evaluating 24 2.5257286443082556 194852
evaluation 48 1.8325814637483102 195048
evaluation.we 2 5.0106352940962555 195436
evaluations 24 2.5257286443082556 195456
evaluations.in 2 5.0106352940962555 195652
evaluator 4 4.31748811353631 195672
evaluators 2 5.0106352940962555 195708
evasion 2 5.0106352940962555 195728
evci 2 5.0106352940962555 195748
even 84 1.2729656758128876 195768
event-based 2 5.0106352940962555 196444
events 6 3.912023005428146 196464
eventually 2 5.0106352940962555 196516
ever 6 3.912023005428146 196536
every 22 2.6127400212978853 196588
everyone 2 5.0106352940962555 196768
everything 2 5.0106352940962555 196788
evidence 16 2.9311937524164198 196808
evident 6 3.912023005428146 196940
evoke 2 5.0106352940962555 196992
evolution 8 3.624340932976365 197012
evolution.recent 2 5.0106352940962555 197080
evolutionary 2 5.0106352940962555 197100
evolve 6 3.912023005428146 197120
evolved 2 5.0106352940962555 197172
evolves 4 4.31748811353631 197192
evolving 4 4.31748811353631 197228
ew 1 5.703782474656201 197264
ewc 2 5.0106352940962555 197276
exacerbate 4 4.31748811353631 197296
exact 18 2.8134107167600364 197332
exactly 4 4.31748811353631 197480
exam 2 5.0106352940962555 197516
examine 8 3.624340932976365 197536
examines 2 5.0106352940962555 197604
example 42 1.9661128563728327 197624
examples 44 1.91959284073794 197964
examples.further 2 5.0106352940962555 198320
exceed 6 3.912023005428146 198340
exceeds 6 3.912023005428146 198392
excel 6 3.912023005428146 198444
excellent 6 3.912023005428146 198496
except 4 4.31748811353631 198548
exceptions 2 5.0106352940962555 198584
excessive 4 4.31748811353631 198604
exchange 2 5.0106352940962555 198640
exclusively 8 3.624340932976365 198660
executables 2 5.0106352940962555 198728
executables.after 2 5.0106352940962555 198748
executables.more 2 5.0106352940962555 198768
executed 4 4.31748811353631 198788
executes 2 5.0106352940962555 198824
executing 2 5.0106352940962555 198844
execution 7 3.7578723256008875 198864
execution-guided 2 5.0106352940962555 198924
exedec 3 4.605170185988092 198944
exemplar 2 5.0106352940962555 198972
exemplars 2 5.0106352940962555 198992
exemplified 2 5.0106352940962555 199012
exemplifying 2 5.0106352940962555 199032
exhaustive 2 5.0106352940962555 199052
exhibit 24 2.5257286443082556 199072
exhibiting 2 5.0106352940962555 199268
exhibits 12 3.2188758248682006 199288
exist 14 3.0647251450409425 199388
existence 2 5.0106352940962555 199504
existing 154 0.6668298722425718 199524
exists 8 3.624340932976365 200760
exp 1 5.703782474656201 200828
expand 6 3.912023005428146 200840
expanding 2 5.0106352940962555 200892
expands 2 5.0106352940962555 200912
expansion 7 3.7578723256008875 200932
expect 2 5.0106352940962555 200992
expectation 2 5.0106352940962555 201012
expectations 6 3.912023005428146 201032
expected 14 3.0647251450409425 201084
expense 2 5.0106352940962555 201200
expensive 30 2.302585092994046 201220
expensive-strong 2 5.0106352940962555 201464
expensive.in 2 5.0106352940962555 201484
experience 11 3.3058872018578307 201504
experiences 4 4.31748811353631 201596
experiment 12 3.2188758248682006 201632
experimental 31 2.2697952701710546 201732
experimentally 18 2.8134107167600364 201984
experiments 154 0.6668298722425718 202132
experiments.our 2 5.0106352940962555 203368
experiments.preprint 2 5.0106352940962555 203388
expert 20 2.70805020110221 203408
experts 7 3.7578723256008875 203572
explain 22 2.6127400212978853 203632
explained 2 5.0106352940962555 203812
explaining 2 5.0106352940962555 203832
explains 4 4.31748811353631 203852
explanation 8 3.624340932976365 203888
explanations 8 3.624340932976365 203956
explicit 28 2.371577964480997 204024
explicitly 24 2.5257286443082556 204252
exploding 2 5.0106352940962555 204448
exploit 16 2.9311937524164198 204468
exploiting 12 3.2188758248682006 204600
exploits 6 3.912023005428146 204700
exploration 16 2.9311937524164198 204752
exploration.for 2 5.0106352940962555 204884
exploration.in 2 5.0106352940962555 204904
explorationdoes 2 5.0106352940962555 204924
exploratory 4 4.31748811353631 204944
explore 22 2.6127400212978853 204980
explore-to-extrapolate 2 5.0106352940962555 205160
explored 10 3.4011973816621555 205180
explorers 2 5.0106352940962555 205264
exploring 8 3.624340932976365 205284
exponential 10 3.4011973816621555 205352
exponentially 8 3.624340932976365 205436
exponentially-growing 2 5.0106352940962555 205504
exponents 2 5.0106352940962555 205524
expose 2 5.0106352940962555 205544
exposes 2 5.0106352940962555 205564
exposing 4 4.31748811353631 205584
exposure 7 3.7578723256008875 205620
express 2 5.0106352940962555 205680
expressing 2 5.0106352940962555 205700
expression 6 3.912023005428146 205720
expressions 4 4.31748811353631 205772
expressive 17 2.870569130599985 205808
expressiveness 6 3.912023005428146 205948
expressivity 10 3.4011973816621555 206000
expressivity.additionally 2 5.0106352940962555 206084
extend 22 2.6127400212978853 206104
extended 6 3.912023005428146 206284
extending 6 3.912023005428146 206336
extends 12 3.2188758248682006 206388
extensible 2 5.0106352940962555 206488
extension 6 3.912023005428146 206508
extensions 8 3.624340932976365 206560
extensive 68 1.4842747694800944 206628
extensively 10 3.4011973816621555 207176
extensively-tuned 2 5.0106352940962555 207260
extent 6 3.912023005428146 207280
external 13 3.138833117194664 207332
extra 6 3.912023005428146 207440
extract 14 3.0647251450409425 207492
extracted 8 3.624340932976365 207608
extracting 8 3.624340932976365 207676
extraction 8 3.624340932976365 207744
extraction-membership 2 5.0106352940962555 207812
extractor 4 4.31748811353631 207832
extractors 2 5.0106352940962555 207868
extracts 8 3.624340932976365 207888
extraordinary 4 4.31748811353631 207956
extrapolate 4 4.31748811353631 207992
extreme 13 3.138833117194664 208028
extremely 20 2.70805020110221 208136
extrinsic 2 5.0106352940962555 208300
eye-in-hand 2 5.0106352940962555 208320
eyes 2 5.0106352940962555 208340
f 11 3.3058872018578307 208360
f1-score 2 5.0106352940962555 208452
f_i 1 5.703782474656201 208472
fa 2 5.0106352940962555 208484
face 12 3.2188758248682006 208504
facebook 4 4.31748811353631 208604
faced 4 4.31748811353631 208640
facilitate 14 3.0647251450409425 208676
facilitated 2 5.0106352940962555 208792
facilitates 6 3.912023005428146 208812
facilitating 4 4.31748811353631 208864
facing 4 4.31748811353631 208900
fact 18 2.8134107167600364 208936
facto 4 4.31748811353631 209084
factor 24 2.5257286443082556 209120
factor-level 2 5.0106352940962555 209316
factored 2 5.0106352940962555 209336
factorization 2 5.0106352940962555 209356
factorized 4 4.31748811353631 209376
factorizing 6 3.912023005428146 209412
factors 20 2.70805020110221 209464
factors.we 2 5.0106352940962555 209628
factual 8 3.624340932976365 209648
factuality 2 5.0106352940962555 209716
fady 2 5.0106352940962555 209736
fail 14 3.0647251450409425 209756
fails 10 3.4011973816621555 209872
failure 12 3.2188758248682006 209956
failures 8 3.624340932976365 210056
fair 6 3.912023005428146 210124
fairly 2 5.0106352940962555 210176
fairness 10 3.4011973816621555 210196
faithful 4 4.31748811353631 210280
faithfully 2 5.0106352940962555 210316
fall 2 5.0106352940962555 210336
fallacy 2 5.0106352940962555 210356
falling 2 5.0106352940962555 210376
falls 4 4.31748811353631 210396
false 4 4.31748811353631 210432
familiar 2 5.0106352940962555 210468
family 10 3.4011973816621555 210488
far 12 3.2188758248682006 210572
farther 2 5.0106352940962555 210672
fashion 8 3.624340932976365 210692
fast 27 2.407945608651872 210760
faster 38 2.0661963149298153 210980
fatras 2 5.0106352940962555 211288
favor 2 5.0106352940962555 211308
favorable 2 5.0106352940962555 211328
fbm 2 5.0106352940962555 211348
fc 1 5.703782474656201 211368
fcnns 2 5.0106352940962555 211380
fe 1 5.703782474656201 211400
fearnet 2 5.0106352940962555 211412
feasibility 2 5.0106352940962555 211432
feasible 14 3.0647251450409425 211452
feature 49 1.8119621765455745 211568
feature-extractor 2 5.0106352940962555 211964
feature-level 2 5.0106352940962555 211984
feature-space 2 5.0106352940962555 212004
features 62 1.5766480896111095 212024
featuring 4 4.31748811353631 212524
feb 3 4.605170185988092 212560
fed 1 5.703782474656201 212588
federated 12 3.2188758248682006 212600
fedreg 2 5.0106352940962555 212700
fedus 2 5.0106352940962555 212720
feed-forward 6 3.912023005428146 212740
feedback 24 2.5257286443082556 212792
feedbacks 2 5.0106352940962555 212988
feedforward 4 4.31748811353631 213008
feeding 4 4.31748811353631 213044
feehery 2 5.0106352940962555 213080
fellow 4 4.31748811353631 213100
femnist 2 5.0106352940962555 213136
few 17 2.870569130599985 213156
few-shot 28 2.371577964480997 213296
fewer 22 2.6127400212978853 213524
ffhq 2 5.0106352940962555 213704
fiction 2 5.0106352940962555 213724
fictitious 2 5.0106352940962555 213744
fid 8 3.624340932976365 213764
fidelity 6 3.912023005428146 213832
field 18 2.8134107167600364 213884
fields 4 4.31748811353631 214032
fifteen 2 5.0106352940962555 214068
figure 12 3.2188758248682006 214088
files 4 4.31748811353631 214188
filippo 2 5.0106352940962555 214224
fill 8 3.624340932976365 214244
filling 4 4.31748811353631 214312
film 2 5.0106352940962555 214348
filter 8 3.624340932976365 214368
filtering 8 3.624340932976365 214436
filters 2 5.0106352940962555 214504
final 12 3.2188758248682006 214524
finally 42 1.9661128563728327 214624
finance 2 5.0106352940962555 214964
finance.reasoning 2 5.0106352940962555 214984
financial 2 5.0106352940962555 215004
find 102 1.07880966137193 215024
finding 24 2.5257286443082556 215844
findings 20 2.70805020110221 216040
finds 6 3.912023005428146 216204
fine-grained 4 4.31748811353631 216256
fine-scale 2 5.0106352940962555 216292
fine-tune 4 4.31748811353631 216312
fine-tuned 10 3.4011973816621555 216348
fine-tuning 24 2.5257286443082556 216432
fine-tuning-caml 2 5.0106352940962555 216628
fine-tuning-exceeds 2 5.0106352940962555 216648
fine-tuning.through 2 5.0106352940962555 216668
finetuned 4 4.31748811353631 216688
finetuning 18 2.8134107167600364 216724
fingers 2 5.0106352940962555 216872
finite 8 3.624340932976365 216892
finite-dimensional 4 4.31748811353631 216960
finite-sum 2 5.0106352940962555 216996
finite-time 2 5.0106352940962555 217016
finn 4 4.31748811353631 217036
fire.arxiv 2 5.0106352940962555 217072
first 150 0.6931471805599453 217092
first-occupancy 2 5.0106352940962555 218296
first-order 7 3.7578723256008875 218316
first-person 6 3.912023005428146 218376
first-person-view 2 5.0106352940962555 218428
firstly 4 4.31748811353631 218448
firstoccupancy 2 5.0106352940962555 218484
firstorder 2 5.0106352940962555 218504
firth 3 4.605170185988092 218524
fist 2 5.0106352940962555 218552
fit 5 4.0943445622221 218572
fitness 2 5.0106352940962555 218616
fits 2 5.0106352940962555 218636
fitting 10 3.4011973816621555 218656
five 2 5.0106352940962555 218740
fix 2 5.0106352940962555 218760
fixed 25 2.4849066497880004 218780
fixed-length 2 5.0106352940962555 218984
fixed-size 4 4.31748811353631 219004
fixed-update 2 5.0106352940962555 219040
fixup 3 4.605170185988092 219060
fl 7 3.7578723256008875 219088
fl-specific 2 5.0106352940962555 219148
flag 2 5.0106352940962555 219168
flagging 2 5.0106352940962555 219188
flamary 2 5.0106352940962555 219208
flan 2 5.0106352940962555 219228
flan-moe 2 5.0106352940962555 219248
flan-palm 2 5.0106352940962555 219268
flan-st 2 5.0106352940962555 219288
flat 4 4.31748811353631 219308
flatness 2 5.0106352940962555 219344
flexibility 8 3.624340932976365 219364
flexible 18 2.8134107167600364 219432
flexibly 8 3.624340932976365 219580
flicker 2 5.0106352940962555 219648
flickers 2 5.0106352940962555 219668
flip 2 5.0106352940962555 219688
flipping 2 5.0106352940962555 219708
flips 2 5.0106352940962555 219728
floating 4 4.31748811353631 219748
flops 6 3.912023005428146 219784
flow 20 2.70805020110221 219836
flow-based 9 3.506557897319982 220000
flowing 2 5.0106352940962555 220076
flows 11 3.3058872018578307 220096
fluent 2 5.0106352940962555 220188
fluently 2 5.0106352940962555 220208
fly 1 5.703782474656201 220228
flying 2 5.0106352940962555 220240
fm 1 5.703782474656201 220260
focus 42 1.9661128563728327 220272
focused 14 3.0647251450409425 220612
focuses 14 3.0647251450409425 220728
focusing 10 3.4011973816621555 220844
folklore 2 5.0106352940962555 220928
follow 16 2.9311937524164198 220948
follow-the-regularized-leader 2 5.0106352940962555 221080
follow-up 2 5.0106352940962555 221100
followed 4 4.31748811353631 221120
following 25 2.4849066497880004 221156
follows 16 2.9311937524164198 221360
font 2 5.0106352940962555 221492
food 2 5.0106352940962555 221512
foods 2 5.0106352940962555 221532
fool 3 4.605170185988092 221552
fooling 2 5.0106352940962555 221580
footprint 8 3.624340932976365 221600
footprint.historically 2 5.0106352940962555 221668
for 345 -0.13976194237515874 221688
force 2 5.0106352940962555 224452
forces 2 5.0106352940962555 224472
forecasters 2 5.0106352940962555 224492
forecasting 7 3.7578723256008875 224512
foreign 4 4.31748811353631 224572
foresee 2 5.0106352940962555 224608
forever208 2 5.0106352940962555 224628
forgetting 9 3.506557897319982 224648
form 44 1.91959284073794 224724
form.specifically 2 5.0106352940962555 225080
formal 6 3.912023005428146 225100
formalism 2 5.0106352940962555 225152
formalize 8 3.624340932976365 225172
formalizes 2 5.0106352940962555 225240
formalizing 4 4.31748811353631 225260
formally 8 3.624340932976365 225296
formats 2 5.0106352940962555 225364
formatting 2 5.0106352940962555 225384
former 6 3.912023005428146 225404
forming 4 4.31748811353631 225456
forms 8 3.624340932976365 225492
formula 8 3.624340932976365 225560
formulae 2 5.0106352940962555 225628
formulas 2 5.0106352940962555 225648
formulate 18 2.8134107167600364 225668
formulating 2 5.0106352940962555 225816
formulation 20 2.70805020110221 225836
formulations 6 3.912023005428146 226000
forward 21 2.659260036932778 226052
forward-pass 2 5.0106352940962555 226224
fosr 2 5.0106352940962555 226244
foster 6 3.912023005428146 226264
found 22 2.6127400212978853 226316
foundation 9 3.506557897319982 226496
foundational 4 4.31748811353631 226572
foundations 2 5.0106352940962555 226608
four 16 2.9311937524164198 226628
four-level 2 5.0106352940962555 226760
fourier 2 5.0106352940962555 226780
fournier 2 5.0106352940962555 226800
fourth 2 5.0106352940962555 226820
foveated 2 5.0106352940962555 226840
fr 4 4.31748811353631 226860
fractal 2 5.0106352940962555 226896
fraction 8 3.624340932976365 226916
fractional 3 4.605170185988092 226984
fragile 2 5.0106352940962555 227012
frame 8 3.624340932976365 227032
frames 4 4.31748811353631 227100
frames.instead 2 5.0106352940962555 227136
framework 109 1.0124345924270572 227156
framework.our 4 4.31748811353631 228032
frameworks 4 4.31748811353631 228068
frank-wolfe 3 4.605170185988092 228104
free 2 5.0106352940962555 228132
free-position 2 5.0106352940962555 228152
freely 4 4.31748811353631 228172
freeman 2 5.0106352940962555 228208
frequencies 6 3.912023005428146 228228
frequency 14 3.0647251450409425 228280
frequency-aware 3 4.605170185988092 228396
frequency-based 2 5.0106352940962555 228424
frequency-dependent 2 5.0106352940962555 228444
frequency.while 2 5.0106352940962555 228464
frequent 4 4.31748811353631 228484
frequently.as 2 5.0106352940962555 228520
friedlander 2 5.0106352940962555 228540
frobenius 2 5.0106352940962555 228560
from 353 -0.1626855822770957 228580
fromfigure 2 5.0106352940962555 231408
front 2 5.0106352940962555 231428
frozen 11 3.3058872018578307 231448
fruitful 2 5.0106352940962555 231540
frchet 2 5.0106352940962555 231560
fs 2 5.0106352940962555 231580
fs-net 2 5.0106352940962555 231600
fsnet 2 5.0106352940962555 231620
ftrl 2 5.0106352940962555 231640
fudan 2 5.0106352940962555 231660
fujimoto 2 5.0106352940962555 231680
fulfills 2 5.0106352940962555 231700
full 24 2.5257286443082556 231720
full-hd 2 5.0106352940962555 231916
full-precision 2 5.0106352940962555 231936
full-resolution 2 5.0106352940962555 231956
full-weight 2 5.0106352940962555 231976
fully 26 2.445685936634719 231996
fully-connected 4 4.31748811353631 232208
fully-differentiable 2 5.0106352940962555 232244
funahashi 2 5.0106352940962555 232264
function 119 0.9246589815446716 232284
function-independent 2 5.0106352940962555 233240
functional 8 3.624340932976365 233260
functional.the 2 5.0106352940962555 233328
functionality 2 5.0106352940962555 233348
functionally 2 5.0106352940962555 233368
functiondependent 2 5.0106352940962555 233388
functioning 2 5.0106352940962555 233408
functions 38 2.0661963149298153 233428
functions-and 2 5.0106352940962555 233736
fundamental 30 2.302585092994046 233756
fundamentally 6 3.912023005428146 234000
further 108 1.0216512475319812 234052
furthermore 48 1.8325814637483102 234920
fuse 2 5.0106352940962555 235308
fusion 4 4.31748811353631 235328
future 32 2.2380465718564744 235364
futures 2 5.0106352940962555 235624
fvd 1 5.703782474656201 235644
fw 2 5.0106352940962555 235656
g 7 3.7578723256008875 235676
g. 1 5.703782474656201 235736
gain 6 3.912023005428146 235748
gained 4 4.31748811353631 235800
gaining 2 5.0106352940962555 235836
gains 14 3.0647251450409425 235856
gal 1 5.703782474656201 235972
gale 2 5.0106352940962555 235984
gambler 3 4.605170185988092 236004
game 10 3.4011973816621555 236032
game-play 2 5.0106352940962555 236116
games 16 2.9311937524164198 236136
gan 8 3.624340932976365 236268
gandissect 2 5.0106352940962555 236336
gans 20 2.70805020110221 236356
gansfallingshort 2 5.0106352940962555 236520
gansynth 2 5.0106352940962555 236540
gap 32 2.2380465718564744 236560
gap-dependent 2 5.0106352940962555 236820
gaps 6 3.912023005428146 236840
gate 2 5.0106352940962555 236892
gatech.edu 2 5.0106352940962555 236912
gated 2 5.0106352940962555 236932
gates 2 5.0106352940962555 236952
gating 2 5.0106352940962555 236972
gauges 2 5.0106352940962555 236992
gaussian 25 2.4849066497880004 237012
gaussians 4 4.31748811353631 237216
gave 2 5.0106352940962555 237252
gaze 2 5.0106352940962555 237272
gb 1 5.703782474656201 237292
gcn 1 5.703782474656201 237304
gda 2 5.0106352940962555 237316
gender 2 5.0106352940962555 237336
genentech 2 5.0106352940962555 237356
general 70 1.455287232606842 237376
general-purpose 10 3.4011973816621555 237940
general.when 2 5.0106352940962555 238024
generalisation 4 4.31748811353631 238044
generalise 2 5.0106352940962555 238080
generalises 2 5.0106352940962555 238100
generalist 4 4.31748811353631 238120
generality 6 3.912023005428146 238156
generalizability 2 5.0106352940962555 238208
generalizable 3 4.605170185988092 238228
generalization 72 1.4271163556401458 238256
generalization.therefore 2 5.0106352940962555 238836
generalizations 2 5.0106352940962555 238856
generalize 38 2.0661963149298153 238876
generalized 15 2.995732273553991 239184
generalizes 10 3.4011973816621555 239308
generalizing 10 3.4011973816621555 239392
generally 16 2.9311937524164198 239476
generalpurpose 2 5.0106352940962555 239608
generate 48 1.8325814637483102 239628
generated 60 1.6094379124341003 240016
generates 22 2.6127400212978853 240500
generating 30 2.302585092994046 240680
generation 81 1.3093333199837622 240924
generation.our 2 5.0106352940962555 241576
generation.the 2 5.0106352940962555 241596
generation.this 2 5.0106352940962555 241616
generations.for 2 5.0106352940962555 241636
generative 78 1.3470736479666094 241656
generator 17 2.870569130599985 242284
generators 4 4.31748811353631 242424
generic 10 3.4011973816621555 242460
genre 2 5.0106352940962555 242544
gensim 3 4.605170185988092 242564
geodesic 4 4.31748811353631 242592
geodesically 2 5.0106352940962555 242628
geodesics 4 4.31748811353631 242648
geometric 8 3.624340932976365 242684
geometrical 2 5.0106352940962555 242752
geometry 10 3.4011973816621555 242772
geometry-aware 2 5.0106352940962555 242856
geometryaware 2 5.0106352940962555 242876
george 2 5.0106352940962555 242896
german-english 2 5.0106352940962555 242916
gestalt 2 5.0106352940962555 242936
gestural 2 5.0106352940962555 242956
get 1 5.703782474656201 242976
gets 2 5.0106352940962555 242988
ghadimi 2 5.0106352940962555 243008
ghazvininejad 2 5.0106352940962555 243028
gil 2 5.0106352940962555 243048
gin 2 5.0106352940962555 243068
gio 3 4.605170185988092 243088
girl 2 5.0106352940962555 243116
git 1 5.703782474656201 243136
github 6 3.912023005428146 243148
github.com 70 1.455287232606842 243200
github.io 2 5.0106352940962555 243764
give 12 3.2188758248682006 243784
given 110 1.0033021088637848 243884
gives 6 3.912023005428146 244768
giving 6 3.912023005428146 244820
gl 1 5.703782474656201 244872
gl68 2 5.0106352940962555 244884
glette 2 5.0106352940962555 244904
glm 1 5.703782474656201 244924
glm.we 2 5.0106352940962555 244936
global 42 1.9661128563728327 244956
globally 4 4.31748811353631 245296
glove 2 5.0106352940962555 245332
glue 4 4.31748811353631 245352
glvfxvv 2 5.0106352940962555 245388
gnn 13 3.138833117194664 245408
gnn-based 4 4.31748811353631 245516
gnns 21 2.659260036932778 245552
go 3 4.605170185988092 245724
goad 2 5.0106352940962555 245752
goal 16 2.9311937524164198 245772
goal-conditioned 4 4.31748811353631 245904
goal-directed 2 5.0106352940962555 245940
goal-reaching 2 5.0106352940962555 245960
goals 10 3.4011973816621555 245980
goalstep 2 5.0106352940962555 246064
going 2 5.0106352940962555 246084
gold 2 5.0106352940962555 246104
good 30 2.302585092994046 246124
google 8 3.624340932976365 246368
google-research 4 4.31748811353631 246436
goren 2 5.0106352940962555 246472
governed 2 5.0106352940962555 246492
governing 4 4.31748811353631 246512
gpnn 2 5.0106352940962555 246548
gpnns 2 5.0106352940962555 246568
gpo 2 5.0106352940962555 246588
gpt 3 4.605170185988092 246608
gpt-2 2 5.0106352940962555 246636
gpt-3 6 3.912023005428146 246656
gpt-3.5 4 4.31748811353631 246708
gpt-4 9 3.506557897319982 246744
gpt-j 2 5.0106352940962555 246820
gpt-type 2 5.0106352940962555 246840
gpt3.5 2 5.0106352940962555 246860
gpt4 4 4.31748811353631 246880
gpt4-generated 2 5.0106352940962555 246916
gpts 2 5.0106352940962555 246936
gpu 2 5.0106352940962555 246956
gpu-hours 2 5.0106352940962555 246976
gpus 6 3.912023005428146 246996
gradient 114 0.9675840262617057 247048
gradient-based 18 2.8134107167600364 247964
gradient-guided 3 4.605170185988092 248112
gradient-space 2 5.0106352940962555 248140
gradient-td 2 5.0106352940962555 248160
gradientbased 2 5.0106352940962555 248180
gradientdescent 2 5.0106352940962555 248200
gradients 18 2.8134107167600364 248220
gradual 2 5.0106352940962555 248368
gradually 6 3.912023005428146 248388
gram 2 5.0106352940962555 248440
grammar 2 5.0106352940962555 248460
grammatical 2 5.0106352940962555 248480
granularity 2 5.0106352940962555 248500
graph 69 1.4696759700589417 248520
graph-based 6 3.912023005428146 249076
graph-structured 8 3.624340932976365 249128
graph.conducting 2 5.0106352940962555 249196
graph.figure 2 5.0106352940962555 249216
graphcnf 2 5.0106352940962555 249236
graphical 2 5.0106352940962555 249256
graphpku 2 5.0106352940962555 249276
graphs 25 2.4849066497880004 249296
graphs.fine-tuning 2 5.0106352940962555 249500
graphs.however 2 5.0106352940962555 249520
grasp 2 5.0106352940962555 249540
grasping 2 5.0106352940962555 249560
gratify 2 5.0106352940962555 249580
graves 2 5.0106352940962555 249600
gray-box 2 5.0106352940962555 249620
great 6 3.912023005428146 249640
greater 2 5.0106352940962555 249692
greatly 12 3.2188758248682006 249712
greedily 2 5.0106352940962555 249812
greedy 12 3.2188758248682006 249832
gribonval 2 5.0106352940962555 249932
grids 2 5.0106352940962555 249952
ground 16 2.9311937524164198 249972
ground-truth 10 3.4011973816621555 250104
grounding 7 3.7578723256008875 250188
group 14 3.0647251450409425 250248
group-specific 2 5.0106352940962555 250364
grouping 3 4.605170185988092 250384
groups 10 3.4011973816621555 250412
groups.existing 2 5.0106352940962555 250496
groups.extant 2 5.0106352940962555 250516
groups.we 2 5.0106352940962555 250536
growing 9 3.506557897319982 250556
grown 2 5.0106352940962555 250632
grows 8 3.624340932976365 250652
growth 2 5.0106352940962555 250720
grus 2 5.0106352940962555 250740
gsam 2 5.0106352940962555 250760
gsm8k 2 5.0106352940962555 250780
gt 2 5.0106352940962555 250800
gta 3 4.605170185988092 250820
gtd 2 5.0106352940962555 250848
gu 2 5.0106352940962555 250868
guarantee 28 2.371577964480997 250888
guaranteed 8 3.624340932976365 251116
guaranteeing 4 4.31748811353631 251184
guarantees 17 2.870569130599985 251220
guarantees.by 2 5.0106352940962555 251360
guarantees.we 2 5.0106352940962555 251380
guarded 2 5.0106352940962555 251400
guess 3 4.605170185988092 251420
guesses 2 5.0106352940962555 251448
guidance 14 3.0647251450409425 251468
guide 8 3.624340932976365 251584
guided 9 3.506557897319982 251652
guillin 2 5.0106352940962555 251728
gunasekar 2 5.0106352940962555 251748
gupta 2 5.0106352940962555 251768
gvcl 2 5.0106352940962555 251788
gwd 1 5.703782474656201 251808
h 3 4.605170185988092 251820
h-consistency 2 5.0106352940962555 251848
had 1 5.703782474656201 251868
haeffele 2 5.0106352940962555 251880
half 2 5.0106352940962555 251900
hallucination 2 5.0106352940962555 251920
hallucinations 2 5.0106352940962555 251940
halting 2 5.0106352940962555 251960
halving 4 4.31748811353631 251980
hamiltonian 2 5.0106352940962555 252016
hamming 2 5.0106352940962555 252036
hampers 2 5.0106352940962555 252056
han 3 4.605170185988092 252076
hand 16 2.9311937524164198 252104
hand-centric 2 5.0106352940962555 252236
hand-crafted 2 5.0106352940962555 252256
hand-crafting 2 5.0106352940962555 252276
hand-designed 4 4.31748811353631 252296
hand-engineered 4 4.31748811353631 252332
handcrafted 6 3.912023005428146 252368
handdesigned 2 5.0106352940962555 252420
handle 24 2.5257286443082556 252440
handled 2 5.0106352940962555 252636
handles 2 5.0106352940962555 252656
handling 7 3.7578723256008875 252676
hands 3 4.605170185988092 252736
hanns 2 5.0106352940962555 252764
happen 2 5.0106352940962555 252784
happens 2 5.0106352940962555 252804
hard 20 2.70805020110221 252824
hard-to-adapt 2 5.0106352940962555 252988
harder 4 4.31748811353631 253008
hardest 4 4.31748811353631 253044
hardly 4 4.31748811353631 253080
hardt 2 5.0106352940962555 253116
hardware 8 3.624340932976365 253136
harm 2 5.0106352940962555 253204
harmful 10 3.4011973816621555 253224
harmlessness 2 5.0106352940962555 253308
harmonics 2 5.0106352940962555 253328
harms 2 5.0106352940962555 253348
harness 4 4.31748811353631 253368
has 86 1.2494351784026934 253404
hauskrecht 2 5.0106352940962555 254096
have 210 0.3566749439387324 254116
having 6 3.912023005428146 255800
hazan 2 5.0106352940962555 255852
hazards 2 5.0106352940962555 255872
he 1 5.703782474656201 255892
head 6 3.912023005428146 255904
head-to-head 2 5.0106352940962555 255956
headless 3 4.605170185988092 255976
heads 2 5.0106352940962555 256004
health 2 5.0106352940962555 256024
healthcare 4 4.31748811353631 256044
heavily 10 3.4011973816621555 256080
heavy 2 5.0106352940962555 256164
heavy-tailed 4 4.31748811353631 256184
held 2 5.0106352940962555 256220
held-out 4 4.31748811353631 256240
help 24 2.5257286443082556 256276
helped 2 5.0106352940962555 256472
helpful 8 3.624340932976365 256492
helpfulness 2 5.0106352940962555 256560
helps 10 3.4011973816621555 256580
hence 14 3.0647251450409425 256664
hendrycks 2 5.0106352940962555 256780
here 28 2.371577964480997 256800
herein 2 5.0106352940962555 257028
hermite 2 5.0106352940962555 257048
hessian 6 3.912023005428146 257068
heterophilic 2 5.0106352940962555 257120
heteroscedastic 3 4.605170185988092 257140
heuristic 10 3.4011973816621555 257168
heuristically 2 5.0106352940962555 257252
heuristics 6 3.912023005428146 257272
hidden 22 2.6127400212978853 257324
hiding 2 5.0106352940962555 257504
hierarchical 17 2.870569130599985 257524
hierarchically 2 5.0106352940962555 257664
high 67 1.499089855265235 257684
high-capacity 2 5.0106352940962555 258224
high-dimensional 24 2.5257286443082556 258244
high-fidelity 8 3.624340932976365 258440
high-frequency 2 5.0106352940962555 258508
high-level 12 3.2188758248682006 258528
high-performance 2 5.0106352940962555 258628
high-precision 2 5.0106352940962555 258648
high-profile 2 5.0106352940962555 258668
high-quality 20 2.70805020110221 258688
high-resolution 12 3.2188758248682006 258852
high-stakes 2 5.0106352940962555 258952
high-value 3 4.605170185988092 258972
high-variance 2 5.0106352940962555 259000
high.in 2 5.0106352940962555 259020
highdimensional 2 5.0106352940962555 259040
higher 28 2.371577964480997 259060
higher-dimensional 2 5.0106352940962555 259288
higher-level 2 5.0106352940962555 259308
higher-order 7 3.7578723256008875 259328
highest 4 4.31748811353631 259388
highestscoring 2 5.0106352940962555 259424
highlevel 2 5.0106352940962555 259444
highlight 12 3.2188758248682006 259464
highlighted 2 5.0106352940962555 259564
highlighting 6 3.912023005428146 259584
highlights 14 3.0647251450409425 259636
highly 46 1.875141078167106 259752
highly-expressive 2 5.0106352940962555 260124
highway 2 5.0106352940962555 260144
hila-chefer.github.io 2 5.0106352940962555 260164
hilbert 2 5.0106352940962555 260184
hinder 2 5.0106352940962555 260204
hindered 2 5.0106352940962555 260224
hinders 4 4.31748811353631 260244
hindsight 3 4.605170185988092 260280
hinge 2 5.0106352940962555 260308
hinges 2 5.0106352940962555 260328
hints 2 5.0106352940962555 260348
historical 2 5.0106352940962555 260368
historically 2 5.0106352940962555 260388
history 8 3.624340932976365 260408
history-dependent 2 5.0106352940962555 260476
hmm-glms 2 5.0106352940962555 260496
ho 1 5.703782474656201 260516
hoc 2 5.0106352940962555 260528
hold 6 3.912023005428146 260548
holds 10 3.4011973816621555 260600
holistic 2 5.0106352940962555 260684
holistically 2 5.0106352940962555 260704
home 2 5.0106352940962555 260724
homogeneous 2 5.0106352940962555 260744
homology 2 5.0106352940962555 260764
homophilic 2 5.0106352940962555 260784
honest 2 5.0106352940962555 260804
hong 2 5.0106352940962555 260824
honghuad 2 5.0106352940962555 260844
hoogeboom 2 5.0106352940962555 260864
hope 8 3.624340932976365 260884
hops 2 5.0106352940962555 260952
horak 2 5.0106352940962555 260972
horizon 2 5.0106352940962555 260992
horizons 4 4.31748811353631 261012
horizons.we 2 5.0106352940962555 261048
horizontal 4 4.31748811353631 261068
hornet 2 5.0106352940962555 261104
hospital 2 5.0106352940962555 261124
host 2 5.0106352940962555 261144
hour-long 2 5.0106352940962555 261164
hours 6 3.912023005428146 261184
hours.assembly 2 5.0106352940962555 261236
hourslong 2 5.0106352940962555 261256
hovy 2 5.0106352940962555 261276
how 51 1.7719568419318754 261296
howard 2 5.0106352940962555 261708
however 178 0.5219989243641159 261728
hrt 2 5.0106352940962555 263156
hsw 2 5.0106352940962555 263176
http 2 5.0106352940962555 263196
https 94 1.160487692386197 263216
hu 1 5.703782474656201 263972
huang 6 3.912023005428146 263984
huge 6 3.912023005428146 264036
human 80 1.3217558399823195 264088
human-level 2 5.0106352940962555 264732
human-specified 2 5.0106352940962555 264752
humaneval 2 5.0106352940962555 264772
humanevalpack 2 5.0106352940962555 264792
humanml3d 2 5.0106352940962555 264812
humanrl_website 2 5.0106352940962555 264832
humans 15 2.995732273553991 264852
hundred 4 4.31748811353631 264976
hundreds 6 3.912023005428146 265012
hunting 2 5.0106352940962555 265064
hurst 2 5.0106352940962555 265084
hurt 2 5.0106352940962555 265104
hurts 2 5.0106352940962555 265124
hutter 2 5.0106352940962555 265144
hybrid 2 5.0106352940962555 265164
hyper-objective 2 5.0106352940962555 265184
hyper-parameter 6 3.912023005428146 265204
hyper-parameters 2 5.0106352940962555 265256
hyperbolic 5 4.0943445622221 265276
hyperboloid 2 5.0106352940962555 265320
hypercube 2 5.0106352940962555 265340
hyperdynamics 3 4.605170185988092 265360
hypernetwork 2 5.0106352940962555 265388
hypernetworks 2 5.0106352940962555 265408
hyperparameter 14 3.0647251450409425 265428
hyperparameters 8 3.624340932976365 265544
hyperplane 2 5.0106352940962555 265612
hypotheses 2 5.0106352940962555 265632
hypothesis 16 2.9311937524164198 265652
hypothesize 4 4.31748811353631 265784
hypothesized 6 3.912023005428146 265820
i 21 2.659260036932778 265872
i-th 2 5.0106352940962555 266044
i.e 54 1.7147984280919266 266064
i.we 2 5.0106352940962555 266500
iain 2 5.0106352940962555 266520
ib 2 5.0106352940962555 266540
iba 2 5.0106352940962555 266560
ibm 1 5.703782474656201 266580
ibp 2 5.0106352940962555 266592
iccv 4 4.31748811353631 266612
ice 1 5.703782474656201 266648
icl 2 5.0106352940962555 266660
iclr 50 1.791759469228055 266680
icm 2 5.0106352940962555 267084
icml 2 5.0106352940962555 267104
icrl 2 5.0106352940962555 267124
idea 16 2.9311937524164198 267144
idea.bags 2 5.0106352940962555 267276
ideal 10 3.4011973816621555 267296
ideally 2 5.0106352940962555 267380
ideas 6 3.912023005428146 267400
ideation 2 5.0106352940962555 267452
identical 12 3.2188758248682006 267472
identically 2 5.0106352940962555 267572
identification 4 4.31748811353631 267592
identified 4 4.31748811353631 267628
identifies 2 5.0106352940962555 267664
identify 34 2.17742195004004 267684
identifying 13 3.138833117194664 267960
identities 2 5.0106352940962555 268068
identity 4 4.31748811353631 268088
ieee 4 4.31748811353631 268124
if 25 2.4849066497880004 268160
ifdef 2 5.0106352940962555 268364
igmc 2 5.0106352940962555 268384
ignited 2 5.0106352940962555 268404
ignorance 6 3.912023005428146 268424
ignore 4 4.31748811353631 268476
ignored 2 5.0106352940962555 268512
ignored.needless 2 5.0106352940962555 268532
ignores 4 4.31748811353631 268552
ignoring 4 4.31748811353631 268588
ii 25 2.4849066497880004 268624
iii 10 3.4011973816621555 268828
iiw 2 5.0106352940962555 268912
iiw-based 2 5.0106352940962555 268932
ill-posed 2 5.0106352940962555 268952
ill-posed.modern 2 5.0106352940962555 268972
illuminating 2 5.0106352940962555 268992
illustrate 18 2.8134107167600364 269012
illustrated 2 5.0106352940962555 269160
illustrates 4 4.31748811353631 269180
illustrating 4 4.31748811353631 269216
illustration 2 5.0106352940962555 269252
illustrative 2 5.0106352940962555 269272
iluvw 2 5.0106352940962555 269292
image 103 1.0690534864265653 269312
image-classification 2 5.0106352940962555 270140
image-generating 2 5.0106352940962555 270160
image-level 2 5.0106352940962555 270180
image-like 2 5.0106352940962555 270200
image-text 6 3.912023005428146 270220
image-to-image 2 5.0106352940962555 270272
image.coped 2 5.0106352940962555 270292
imagebased 2 5.0106352940962555 270312
imagenet 37 2.0928645620119766 270332
imagenet-1k 2 5.0106352940962555 270632
imagenet-64 2 5.0106352940962555 270652
images 64 1.5448993912965292 270672
images.our 2 5.0106352940962555 271188
imaginationaugmented 2 5.0106352940962555 271208
imaging 3 4.605170185988092 271228
imagining 2 5.0106352940962555 271256
imbalance 2 5.0106352940962555 271276
imbalanced 4 4.31748811353631 271296
imbalancedness 2 5.0106352940962555 271332
imitate 6 3.912023005428146 271352
imitation 19 2.7593434954897607 271404
immediate 2 5.0106352940962555 271560
imp 2 5.0106352940962555 271580
impact 8 3.624340932976365 271600
impactful 2 5.0106352940962555 271668
impacts 2 5.0106352940962555 271688
impedes 2 5.0106352940962555 271708
imperative 2 5.0106352940962555 271728
imperceptible 2 5.0106352940962555 271748
imperfect 2 5.0106352940962555 271768
implement 10 3.4011973816621555 271788
implementation 20 2.70805020110221 271872
implementations 4 4.31748811353631 272036
implemented 10 3.4011973816621555 272072
implementing 6 3.912023005428146 272156
implements 2 5.0106352940962555 272208
implications 12 3.2188758248682006 272228
implicit 23 2.5682882587270512 272328
implicitly 14 3.0647251450409425 272516
implied 2 5.0106352940962555 272632
implies 6 3.912023005428146 272652
imply 2 5.0106352940962555 272704
implying 4 4.31748811353631 272724
import 2 5.0106352940962555 272760
importance 32 2.2380465718564744 272780
important 40 2.0149030205422647 273040
importantly 10 3.4011973816621555 273364
imported 2 5.0106352940962555 273448
impose 2 5.0106352940962555 273468
imposed 2 5.0106352940962555 273488
imposes 4 4.31748811353631 273508
imposing 2 5.0106352940962555 273544
impossible 4 4.31748811353631 273564
impractical 4 4.31748811353631 273600
impressionism 2 5.0106352940962555 273636
impressive 20 2.70805020110221 273656
improbable-ai 2 5.0106352940962555 273820
improper 2 5.0106352940962555 273840
improvable 2 5.0106352940962555 273860
improve 72 1.4271163556401458 273880
improved 32 2.2380465718564744 274460
improvement 46 1.875141078167106 274720
improvement.yet 2 5.0106352940962555 275092
improvements 34 2.17742195004004 275112
improves 61 1.5929086104828898 275388
improving 36 2.120263536200091 275880
impute 2 5.0106352940962555 276172
in 329 -0.09227527610917081 276192
in-and 2 5.0106352940962555 278828
in-context 16 2.9311937524164198 278848
in-depth 4 4.31748811353631 278980
in-distribution 6 3.912023005428146 279016
in-domain 2 5.0106352940962555 279068
in-file 2 5.0106352940962555 279088
in-silico 2 5.0106352940962555 279108
in-the-wild 2 5.0106352940962555 279128
inability 4 4.31748811353631 279148
inaccuracies 2 5.0106352940962555 279184
inaccurate 2 5.0106352940962555 279204
inadequate 4 4.31748811353631 279224
inadequately 2 5.0106352940962555 279260
inadvertently 2 5.0106352940962555 279280
inapplicable 2 5.0106352940962555 279300
inappropriate 2 5.0106352940962555 279320
inbai 2 5.0106352940962555 279340
incentive 2 5.0106352940962555 279360
inception 2 5.0106352940962555 279380
inception-resnet-v2 2 5.0106352940962555 279400
incite 2 5.0106352940962555 279420
include 14 3.0647251450409425 279440
included 4 4.31748811353631 279556
includes 10 3.4011973816621555 279592
including 76 1.3730491343698699 279676
incoming 2 5.0106352940962555 280288
incompatible 4 4.31748811353631 280308
incomplete 6 3.912023005428146 280344
incompleteness 4 4.31748811353631 280396
inconsistency 4 4.31748811353631 280432
inconsistent 6 3.912023005428146 280468
incontrolled 2 5.0106352940962555 280520
incorporate 22 2.6127400212978853 280540
incorporated 4 4.31748811353631 280720
incorporates 14 3.0647251450409425 280756
incorporating 10 3.4011973816621555 280872
incorrect 4 4.31748811353631 280956
incorrectly 4 4.31748811353631 280992
incorrectness 2 5.0106352940962555 281028
increase 25 2.4849066497880004 281048
increased 8 3.624340932976365 281252
increases 16 2.9311937524164198 281320
increases.for 2 5.0106352940962555 281452
increasing 24 2.5257286443082556 281472
increasingly 16 2.9311937524164198 281668
incremental 4 4.31748811353631 281800
incur 2 5.0106352940962555 281836
incurred 2 5.0106352940962555 281856
indeed 20 2.70805020110221 281876
independent 20 2.70805020110221 282040
independently 18 2.8134107167600364 282204
index 6 3.912023005428146 282352
indexing 2 5.0106352940962555 282404
indicate 16 2.9311937524164198 282424
indicated 2 5.0106352940962555 282556
indicates 12 3.2188758248682006 282576
indirection 2 5.0106352940962555 282676
indiscriminately 4 4.31748811353631 282696
indistinguishable 2 5.0106352940962555 282732
individual 24 2.5257286443082556 282752
individualize 2 5.0106352940962555 282948
individually 4 4.31748811353631 282968
individuals 2 5.0106352940962555 283004
individuating 2 5.0106352940962555 283024
indomain 2 5.0106352940962555 283044
induce 12 3.2188758248682006 283064
induced 8 3.624340932976365 283164
induces 4 4.31748811353631 283232
inducing 6 3.912023005428146 283268
induction 2 5.0106352940962555 283320
inductive 15 2.995732273553991 283340
inductively 2 5.0106352940962555 283464
industrial 2 5.0106352940962555 283484
industry 2 5.0106352940962555 283504
ineffective 8 3.624340932976365 283524
inefficient 8 3.624340932976365 283592
inefficient.in 2 5.0106352940962555 283660
inequitable 4 4.31748811353631 283680
inequitably 2 5.0106352940962555 283716
inevitably 4 4.31748811353631 283736
infeasibility 2 5.0106352940962555 283772
infeasible 2 5.0106352940962555 283792
infer 8 3.624340932976365 283812
inference 73 1.41332303350781 283880
inference-time 4 4.31748811353631 284468
inference.learning 2 5.0106352940962555 284504
inference.thereby 2 5.0106352940962555 284524
inferences 4 4.31748811353631 284544
inferior 2 5.0106352940962555 284580
inferred 4 4.31748811353631 284600
inferring 2 5.0106352940962555 284636
infigure 2 5.0106352940962555 284656
infinite 4 4.31748811353631 284676
infinite-width 2 5.0106352940962555 284712
infinitely 2 5.0106352940962555 284732
infinity 2 5.0106352940962555 284752
inflate 2 5.0106352940962555 284772
inflating 2 5.0106352940962555 284792
inflexible.latent 2 5.0106352940962555 284812
influence 4 4.31748811353631 284832
influenced 2 5.0106352940962555 284868
influencer 3 4.605170185988092 284888
influx 2 5.0106352940962555 284916
infodeletionattacks 2 5.0106352940962555 284936
infomax 2 5.0106352940962555 284956
information 119 0.9246589815446716 284976
information-theoretic 4 4.31748811353631 285932
informational 2 5.0106352940962555 285968
informative 10 3.4011973816621555 285988
informative.we 2 5.0106352940962555 286072
informativeness 2 5.0106352940962555 286092
informed 4 4.31748811353631 286112
infrastructure 2 5.0106352940962555 286148
inherent 18 2.8134107167600364 286168
inherently 8 3.624340932976365 286316
inherit 4 4.31748811353631 286384
inherited 2 5.0106352940962555 286420
initial 16 2.9311937524164198 286440
initialization 20 2.70805020110221 286572
initializations 4 4.31748811353631 286736
initialized 2 5.0106352940962555 286772
initializing 4 4.31748811353631 286792
initially 2 5.0106352940962555 286828
initiate 2 5.0106352940962555 286848
inject 2 5.0106352940962555 286868
injected 4 4.31748811353631 286888
injecting 6 3.912023005428146 286924
injection 9 3.506557897319982 286976
injectivity 4 4.31748811353631 287052
inner 6 3.912023005428146 287088
inner-loop 2 5.0106352940962555 287140
innovations 2 5.0106352940962555 287160
innovative 4 4.31748811353631 287180
inpainting 4 4.31748811353631 287216
input 104 1.0593915755148284 287252
input-output 2 5.0106352940962555 288088
inputdata 2 5.0106352940962555 288108
inputs 40 2.0149030205422647 288128
inputs.goldberger 2 5.0106352940962555 288452
inputs.our 2 5.0106352940962555 288472
insecure 2 5.0106352940962555 288492
insensitive 2 5.0106352940962555 288512
inserting 2 5.0106352940962555 288532
inside 8 3.624340932976365 288552
insight 14 3.0647251450409425 288620
insights 16 2.9311937524164198 288736
inspiration 6 3.912023005428146 288868
inspire 4 4.31748811353631 288920
inspired 22 2.6127400212978853 288956
inspires 2 5.0106352940962555 289136
inspiring 2 5.0106352940962555 289156
instabilities 2 5.0106352940962555 289176
instability 4 4.31748811353631 289196
instance 20 2.70805020110221 289232
instance-dependent 2 5.0106352940962555 289396
instance-level 2 5.0106352940962555 289416
instance-optimal 3 4.605170185988092 289436
instancedependent 2 5.0106352940962555 289464
instances 16 2.9311937524164198 289484
instantaneous 2 5.0106352940962555 289616
instantiate 4 4.31748811353631 289636
instantiated 2 5.0106352940962555 289672
instantly 2 5.0106352940962555 289692
instead 42 1.9661128563728327 289712
instead.this 2 5.0106352940962555 290052
instinct 2 5.0106352940962555 290072
institute 2 5.0106352940962555 290092
instructed 4 4.31748811353631 290112
instruction 17 2.870569130599985 290148
instruction-aligned 2 5.0106352940962555 290288
instruction-following 2 5.0106352940962555 290308
instruction-tuned 2 5.0106352940962555 290328
instruction-tuning 4 4.31748811353631 290348
instructionfollowing 2 5.0106352940962555 290384
instructions 12 3.2188758248682006 290404
instructive 2 5.0106352940962555 290504
insufficiency 2 5.0106352940962555 290524
intable 4 4.31748811353631 290544
intact 2 5.0106352940962555 290580
integral 4 4.31748811353631 290600
integrate 2 5.0106352940962555 290636
integrate-andfire 2 5.0106352940962555 290656
integrated 6 3.912023005428146 290676
integrates 2 5.0106352940962555 290728
integrating 10 3.4011973816621555 290748
integration 10 3.4011973816621555 290832
integratione 2 5.0106352940962555 290916
integrity 2 5.0106352940962555 290936
intelligence 8 3.624340932976365 290956
intelligent 4 4.31748811353631 291024
intelligible 2 5.0106352940962555 291060
intended 4 4.31748811353631 291080
intensive 4 4.31748811353631 291116
intensively 4 4.31748811353631 291152
inter-connection 2 5.0106352940962555 291188
inter-frame 2 5.0106352940962555 291208
interact 2 5.0106352940962555 291228
interaction 27 2.407945608651872 291248
interactions 21 2.659260036932778 291468
interactions.in 2 5.0106352940962555 291640
interactions.such 2 5.0106352940962555 291660
interactions.the 2 5.0106352940962555 291680
interactive 11 3.3058872018578307 291700
interactively 2 5.0106352940962555 291792
interacts 2 5.0106352940962555 291812
interconnected 2 5.0106352940962555 291832
interconvert 2 5.0106352940962555 291852
interenvironment 2 5.0106352940962555 291872
interest 14 3.0647251450409425 291892
interest.however 2 5.0106352940962555 292008
interested 4 4.31748811353631 292028
interesting 10 3.4011973816621555 292064
interestingly 6 3.912023005428146 292148
interface 4 4.31748811353631 292200
interleave 2 5.0106352940962555 292236
interleaved-frame 2 5.0106352940962555 292256
intermediary 2 5.0106352940962555 292276
intermediate 16 2.9311937524164198 292296
intern 4 4.31748811353631 292428
internal 4 4.31748811353631 292464
internalizes 2 5.0106352940962555 292500
internally 2 5.0106352940962555 292520
internet-scale 2 5.0106352940962555 292540
internship 16 2.9311937524164198 292560
interplay 8 3.624340932976365 292692
interpolate 4 4.31748811353631 292760
interpolating 2 5.0106352940962555 292796
interpolation 6 3.912023005428146 292816
interpret 2 5.0106352940962555 292868
interpretability 6 3.912023005428146 292888
interpretable 26 2.445685936634719 292940
interpretation 6 3.912023005428146 293152
interpreted 6 3.912023005428146 293204
interrelated 2 5.0106352940962555 293256
intersects 2 5.0106352940962555 293276
interval 4 4.31748811353631 293296
intervals 4 4.31748811353631 293332
intervening 2 5.0106352940962555 293368
intervention 6 3.912023005428146 293388
interventions 4 4.31748811353631 293440
interview 2 5.0106352940962555 293476
interviews 2 5.0106352940962555 293496
intl 2 5.0106352940962555 293516
into 136 0.7911275889201491 293536
intra-class 2 5.0106352940962555 294628
intractable 4 4.31748811353631 294648
intricate 4 4.31748811353631 294684
intrigued 2 5.0106352940962555 294720
intriguing 6 3.912023005428146 294740
intriguingly 2 5.0106352940962555 294792
intrinsic 8 3.624340932976365 294812
introduce 102 1.07880966137193 294880
introduced 8 3.624340932976365 295700
introduces 18 2.8134107167600364 295768
introducing 22 2.6127400212978853 295916
introduction 2 5.0106352940962555 296096
introspection 2 5.0106352940962555 296116
intuition 8 3.624340932976365 296136
intuitive 8 3.624340932976365 296204
intuitively 2 5.0106352940962555 296272
invaluable 2 5.0106352940962555 296292
invariance 8 3.624340932976365 296312
invariances 6 3.912023005428146 296380
invariant 12 3.2188758248682006 296432
inverse 22 2.6127400212978853 296532
inverse-hessian 2 5.0106352940962555 296712
inversion 6 3.912023005428146 296732
invertibility 2 5.0106352940962555 296784
invertible 6 3.912023005428146 296804
investigate 38 2.0661963149298153 296856
investigated 2 5.0106352940962555 297164
investigates 10 3.4011973816621555 297184
investigating 4 4.31748811353631 297268
investigation 14 3.0647251450409425 297304
investigation.furthermore 2 5.0106352940962555 297420
investigations 4 4.31748811353631 297440
invoke 2 5.0106352940962555 297476
invoking 2 5.0106352940962555 297496
involve 10 3.4011973816621555 297516
involves 10 3.4011973816621555 297600
involving 10 3.4011973816621555 297684
iou 2 5.0106352940962555 297768
irregular 2 5.0106352940962555 297788
irregularly 3 4.605170185988092 297808
irrelevant 4 4.31748811353631 297836
is 239 0.22731892272469034 297872
is.we 2 5.0106352940962555 299788
ising 2 5.0106352940962555 299808
isolate 4 4.31748811353631 299828
isolating 2 5.0106352940962555 299864
isomorphism 4 4.31748811353631 299884
isotropic 6 3.912023005428146 299920
issue 46 1.875141078167106 299972
issue.furthermore 2 5.0106352940962555 300344
issues 38 2.0661963149298153 300364
issues.we 2 5.0106352940962555 300672
it 121 0.90799192905946 300692
it.along 2 5.0106352940962555 301664
it.to 2 5.0106352940962555 301684
item 2 5.0106352940962555 301704
items 6 3.912023005428146 301724
iterate 2 5.0106352940962555 301776
iterates 2 5.0106352940962555 301796
iteration 14 3.0647251450409425 301816
iterations 12 3.2188758248682006 301932
iterative 14 3.0647251450409425 302032
iteratively 8 3.624340932976365 302148
iternorm 2 5.0106352940962555 302216
its 60 1.6094379124341003 302236
itself 19 2.7593434954897607 302720
itself.the 2 5.0106352940962555 302876
iurp 2 5.0106352940962555 302896
iv 2 5.0106352940962555 302916
j 8 3.624340932976365 302936
j-th 2 5.0106352940962555 303004
j. 1 5.703782474656201 303024
jaakkola 2 5.0106352940962555 303036
jack 2 5.0106352940962555 303056
jacobian 2 5.0106352940962555 303076
jacot 2 5.0106352940962555 303096
jaeger 2 5.0106352940962555 303116
jaeho-lee 2 5.0106352940962555 303136
jalaj 2 5.0106352940962555 303156
jan 2 5.0106352940962555 303176
janner 2 5.0106352940962555 303196
jason 2 5.0106352940962555 303216
jasonlee 2 5.0106352940962555 303236
java 4 4.31748811353631 303256
javascript 2 5.0106352940962555 303292
jax 1 5.703782474656201 303312
jeff 2 5.0106352940962555 303324
jhw 1 5.703782474656201 303344
jian 4 4.31748811353631 303356
jiang 2 5.0106352940962555 303392
jiaqi 2 5.0106352940962555 303412
jin 4 4.31748811353631 303432
jlyh 2 5.0106352940962555 303468
jnp 1 5.703782474656201 303488
job 2 5.0106352940962555 303500
johansen 2 5.0106352940962555 303520
john 4 4.31748811353631 303540
johnson 2 5.0106352940962555 303576
johnson-lindenstrauss 2 5.0106352940962555 303596
joint 16 2.9311937524164198 303616
jointly 14 3.0647251450409425 303748
joints 2 5.0106352940962555 303864
joints.project 2 5.0106352940962555 303884
jost 2 5.0106352940962555 303904
journal 10 3.4011973816621555 303924
jpeg 4 4.31748811353631 304008
judges 2 5.0106352940962555 304044
judgments 2 5.0106352940962555 304064
judiciously 2 5.0106352940962555 304084
jul 2 5.0106352940962555 304104
jun 2 5.0106352940962555 304124
junction 2 5.0106352940962555 304144
just 8 3.624340932976365 304164
justification 4 4.31748811353631 304232
justify 2 5.0106352940962555 304268
jy0205 2 5.0106352940962555 304288
k 7 3.7578723256008875 304308
kadavath 2 5.0106352940962555 304368
kaiser 2 5.0106352940962555 304388
kaist 2 5.0106352940962555 304408
kakade 2 5.0106352940962555 304428
kalman 2 5.0106352940962555 304448
karimireddy 2 5.0106352940962555 304468
karras 2 5.0106352940962555 304488
kasai 2 5.0106352940962555 304508
kdd 1 5.703782474656201 304528
keep 4 4.31748811353631 304540
keeping 8 3.624340932976365 304576
kefan 2 5.0106352940962555 304644
kernel 15 2.995732273553991 304664
kernels 8 3.624340932976365 304788
key 23 2.5682882587270512 304856
key-value 2 5.0106352940962555 305044
keys 4 4.31748811353631 305064
kfiou 3 4.605170185988092 305100
kg 3 4.605170185988092 305128
kg-a2c 2 5.0106352940962555 305156
kgqa 2 5.0106352940962555 305176
kgs 6 3.912023005428146 305196
kilian 2 5.0106352940962555 305248
kinds 2 5.0106352940962555 305268
kireev 2 5.0106352940962555 305288
kirsch 2 5.0106352940962555 305308
kit-ml 2 5.0106352940962555 305328
kitab 3 4.605170185988092 305348
kitchen 2 5.0106352940962555 305376
kl 5 4.0943445622221 305396
kl-divergence 2 5.0106352940962555 305440
kld 1 5.703782474656201 305460
klein 2 5.0106352940962555 305472
knapsack 3 4.605170185988092 305492
know 8 3.624340932976365 305520
knowing 2 5.0106352940962555 305588
knowledge 83 1.284941866859603 305608
knowledge-aware 2 5.0106352940962555 306276
knowledge-encompassing 2 5.0106352940962555 306296
knowledge-intensive 2 5.0106352940962555 306316
knowledge-oriented 2 5.0106352940962555 306336
knowledge-powered 2 5.0106352940962555 306356
knowledge-related 2 5.0106352940962555 306376
knowledge.to 2 5.0106352940962555 306396
knowledgespecific 2 5.0106352940962555 306416
known 46 1.875141078167106 306436
koh 2 5.0106352940962555 306808
kola 3 4.605170185988092 306828
kola.xlore.cn 2 5.0106352940962555 306856
kolouri 2 5.0106352940962555 306876
kong 4 4.31748811353631 306896
koopman 3 4.605170185988092 306932
kovalev 2 5.0106352940962555 306960
krz 1 5.703782474656201 306980
kuaishou 2 5.0106352940962555 306992
kullback-leibler 2 5.0106352940962555 307012
kumar 4 4.31748811353631 307032
kundu 2 5.0106352940962555 307068
kusner 2 5.0106352940962555 307088
kwikbucks 2 5.0106352940962555 307108
kyrre 2 5.0106352940962555 307128
l 6 3.912023005428146 307148
l2-distance 2 5.0106352940962555 307200
lab.using 2 5.0106352940962555 307220
label 32 2.2380465718564744 307240
labeled 14 3.0647251450409425 307500
labeling 8 3.624340932976365 307616
labelled 4 4.31748811353631 307684
labels 35 2.1484344131667874 307720
labels.key 2 5.0106352940962555 308004
labels.our 2 5.0106352940962555 308024
labor-intensive 6 3.912023005428146 308044
laboratory.in 2 5.0106352940962555 308096
laborious 4 4.31748811353631 308116
labyrinth 2 5.0106352940962555 308152
labyrinth.natural 2 5.0106352940962555 308172
lack 26 2.445685936634719 308192
lacking 4 4.31748811353631 308404
lacks 2 5.0106352940962555 308440
lagrangian 8 3.624340932976365 308460
lagrangian-based 2 5.0106352940962555 308528
lags 2 5.0106352940962555 308548
lai 2 5.0106352940962555 308568
laine 2 5.0106352940962555 308588
lambada 4 4.31748811353631 308608
lamo 2 5.0106352940962555 308644
lamo2023.github.io 2 5.0106352940962555 308664
lamp 2 5.0106352940962555 308684
lan 2 5.0106352940962555 308704
landmark 2 5.0106352940962555 308724
landmarks 2 5.0106352940962555 308744
landscape 8 3.624340932976365 308764
landscapes 2 5.0106352940962555 308832
language 180 0.5108256237659907 308852
language-agnostic 3 4.605170185988092 310296
language-vision 4 4.31748811353631 310324
language.even 2 5.0106352940962555 310360
language.knowledge 2 5.0106352940962555 310380
languages 16 2.9311937524164198 310400
languages.empirical 2 5.0106352940962555 310532
languages.notably 2 5.0106352940962555 310552
laplace 2 5.0106352940962555 310572
large 185 0.4834266495778763 310592
large-batch 3 4.605170185988092 312076
large-mi 2 5.0106352940962555 312104
large-scale 32 2.2380465718564744 312124
large-size 2 5.0106352940962555 312384
large-sparse 2 5.0106352940962555 312404
large.contribution 2 5.0106352940962555 312424
largecompute 2 5.0106352940962555 312444
largely 20 2.70805020110221 312464
larger 30 2.302585092994046 312628
largest 4 4.31748811353631 312872
laskin 2 5.0106352940962555 312908
last 10 3.4011973816621555 312928
last-iterate 2 5.0106352940962555 313012
lastly 2 5.0106352940962555 313032
late 2 5.0106352940962555 313052
latent 56 1.6784307839210517 313072
latent-action-based 2 5.0106352940962555 313524
latentdiffuser 2 5.0106352940962555 313544
latents 2 5.0106352940962555 313564
later 8 3.624340932976365 313584
latter 8 3.624340932976365 313652
lattimore 4 4.31748811353631 313720
launch 2 5.0106352940962555 313756
lavit 4 4.31748811353631 313776
law 6 3.912023005428146 313812
laws 6 3.912023005428146 313864
layer 36 2.120263536200091 313916
layer-adaptive 3 4.605170185988092 314208
layer-adaptive-sparsity 2 5.0106352940962555 314236
layer-norm 2 5.0106352940962555 314256
layers 36 2.120263536200091 314276
layerwise 2 5.0106352940962555 314568
layout 2 5.0106352940962555 314588
layouts 2 5.0106352940962555 314608
ldm 3 4.605170185988092 314628
le 2 5.0106352940962555 314656
lead 40 2.0149030205422647 314676
leaderboard 4 4.31748811353631 315000
leading 30 2.302585092994046 315036
leads 44 1.91959284073794 315280
leak 2 5.0106352940962555 315636
leakage 4 4.31748811353631 315656
leaking 2 5.0106352940962555 315692
learn 112 0.9852836033611064 315712
learnable 12 3.2188758248682006 316612
learnable.we 2 5.0106352940962555 316712
learned 68 1.4842747694800944 316732
learned_action_spaces 2 5.0106352940962555 317280
learner 4 4.31748811353631 317300
learner.therefore 2 5.0106352940962555 317336
learners 2 5.0106352940962555 317356
learning 392 -0.2674793651342615 317376
learning-augmented 2 5.0106352940962555 320516
learning-based 4 4.31748811353631 320536
learning.an 2 5.0106352940962555 320572
learning.drawing 2 5.0106352940962555 320592
learning.second 2 5.0106352940962555 320612
learns 34 2.17742195004004 320632
learnt 3 4.605170185988092 320908
least 16 2.9311937524164198 320936
least-square 4 4.31748811353631 321068
least-squares 2 5.0106352940962555 321104
leave 4 4.31748811353631 321124
leaves 3 4.605170185988092 321160
leaving 6 3.912023005428146 321188
led 2 5.0106352940962555 321240
lee 8 3.624340932976365 321260
lee-ny 2 5.0106352940962555 321328
left 6 3.912023005428146 321348
left-to-right 2 5.0106352940962555 321400
legacy 2 5.0106352940962555 321420
legged 2 5.0106352940962555 321440
lego-prover 3 4.605170185988092 321460
lemmas 2 5.0106352940962555 321488
lemur 2 5.0106352940962555 321508
lend 2 5.0106352940962555 321528
lending 2 5.0106352940962555 321548
length 14 3.0647251450409425 321568
length.the 2 5.0106352940962555 321684
lengths 2 5.0106352940962555 321704
lens 10 3.4011973816621555 321724
lepikhin 2 5.0106352940962555 321808
less 40 2.0149030205422647 321828
let 5 4.0943445622221 322152
lets 2 5.0106352940962555 322196
leung 2 5.0106352940962555 322216
level 14 3.0647251450409425 322236
levels 18 2.8134107167600364 322352
levels.comprehensive 2 5.0106352940962555 322500
leverage 38 2.0661963149298153 322520
leveraged 4 4.31748811353631 322828
leverages 22 2.6127400212978853 322864
leveraging 32 2.2380465718564744 323044
levine 2 5.0106352940962555 323304
lewis 2 5.0106352940962555 323324
lgi 2 5.0106352940962555 323344
li 3 4.605170185988092 323364
liang 2 5.0106352940962555 323392
liar 2 5.0106352940962555 323412
libraries 2 5.0106352940962555 323432
library 2 5.0106352940962555 323452
library.our 2 5.0106352940962555 323472
librispeech 2 5.0106352940962555 323492
lie 7 3.7578723256008875 323512
lierelated 2 5.0106352940962555 323572
lies 8 3.624340932976365 323592
lift 4 4.31748811353631 323660
lifting 2 5.0106352940962555 323696
light 16 2.9311937524164198 323716
lightweight 6 3.912023005428146 323848
like 62 1.5766480896111095 323900
likelihood 21 2.659260036932778 324400
likelihood-based 2 5.0106352940962555 324572
likelihood-tempering 2 5.0106352940962555 324592
likelihoods 2 5.0106352940962555 324612
likely 18 2.8134107167600364 324632
likerojas-carulla 2 5.0106352940962555 324780
lilnetx 3 4.605170185988092 324800
limit 18 2.8134107167600364 324828
limitation 16 2.9311937524164198 324976
limitations 24 2.5257286443082556 325108
limited 53 1.7334905611040792 325304
limited-angle 2 5.0106352940962555 325732
limited.given 2 5.0106352940962555 325752
limiting 14 3.0647251450409425 325772
limits 6 3.912023005428146 325888
lin 4 4.31748811353631 325940
linder 2 5.0106352940962555 325976
line 14 3.0647251450409425 325996
linear 66 1.5141277326297755 326112
linearization 2 5.0106352940962555 326644
linearized 2 5.0106352940962555 326664
linearly 14 3.0647251450409425 326684
lines 8 3.624340932976365 326800
linguistic 2 5.0106352940962555 326868
link 7 3.7578723256008875 326888
linked 4 4.31748811353631 326948
links 2 5.0106352940962555 326984
lins-lab 2 5.0106352940962555 327004
linucb 2 5.0106352940962555 327024
lipschitz 8 3.624340932976365 327044
lipsim 3 4.605170185988092 327112
lipton 2 5.0106352940962555 327140
list 4 4.31748811353631 327160
listed 2 5.0106352940962555 327196
listenable 2 5.0106352940962555 327216
literally 2 5.0106352940962555 327236
literature 30 2.302585092994046 327256
literatures 2 5.0106352940962555 327500
little 16 2.9311937524164198 327520
liu 6 3.912023005428146 327652
live 2 5.0106352940962555 327704
lives 4 4.31748811353631 327724
living 2 5.0106352940962555 327760
llama 9 3.506557897319982 327780
llama-2 2 5.0106352940962555 327856
llama2 2 5.0106352940962555 327876
llama2-7b 2 5.0106352940962555 327896
llm 30 2.302585092994046 327916
llm-bar 2 5.0106352940962555 328160
llm-based 2 5.0106352940962555 328180
llm.such 2 5.0106352940962555 328200
llm.we 2 5.0106352940962555 328220
llmbar 2 5.0106352940962555 328240
llms 65 1.529395204760564 328260
llms-generated 2 5.0106352940962555 328784
llms.motivated 2 5.0106352940962555 328804
llms.on 2 5.0106352940962555 328824
llms.our 2 5.0106352940962555 328844
llms.regardless 2 5.0106352940962555 328864
llms.we 4 4.31748811353631 328884
lln 2 5.0106352940962555 328920
lloyd 2 5.0106352940962555 328940
lm 7 3.7578723256008875 328960
lm-based 2 5.0106352940962555 329020
lm-emulated 2 5.0106352940962555 329040
lm1b 2 5.0106352940962555 329060
lm4visualencoding 2 5.0106352940962555 329080
lms 7 3.7578723256008875 329100
lms.through 2 5.0106352940962555 329160
ln 1 5.703782474656201 329180
loading 4 4.31748811353631 329192
loan 2 5.0106352940962555 329228
loans 2 5.0106352940962555 329248
local 40 2.0149030205422647 329268
locality 3 4.605170185988092 329592
localization 2 5.0106352940962555 329620
localize 2 5.0106352940962555 329640
localized 4 4.31748811353631 329660
locally 18 2.8134107167600364 329696
locally-coherent 2 5.0106352940962555 329844
locate 2 5.0106352940962555 329864
located 2 5.0106352940962555 329884
locating 4 4.31748811353631 329904
location 4 4.31748811353631 329940
locations 4 4.31748811353631 329976
locked 2 5.0106352940962555 330012
locomotion 14 3.0647251450409425 330032
locuslab 2 5.0106352940962555 330148
log 10 3.4011973816621555 330168
log-determinant 4 4.31748811353631 330252
log-hilbert-schmidt 2 5.0106352940962555 330288
log-likelihood 8 3.624340932976365 330308
log-prior 2 5.0106352940962555 330376
logarithm 4 4.31748811353631 330396
logarithmic 2 5.0106352940962555 330432
logarithmically 2 5.0106352940962555 330452
logdet 2 5.0106352940962555 330472
logdet-ftrl 2 5.0106352940962555 330492
logical 9 3.506557897319982 330512
logistic 6 3.912023005428146 330588
logits 2 5.0106352940962555 330640
long 34 2.17742195004004 330660
long-distance 2 5.0106352940962555 330936
long-horizon 6 3.912023005428146 330956
long-range 4 4.31748811353631 331008
long-standing 4 4.31748811353631 331044
long-tail 2 5.0106352940962555 331080
long-term 6 3.912023005428146 331100
long-text 2 5.0106352940962555 331152
longer 10 3.4011973816621555 331172
look 2 5.0106352940962555 331256
lookups 2 5.0106352940962555 331276
loop 2 5.0106352940962555 331296
loosely 2 5.0106352940962555 331316
looser 2 5.0106352940962555 331336
lora 4 4.31748811353631 331356
lose 2 5.0106352940962555 331392
loss 81 1.3093333199837622 331412
loss.using 2 5.0106352940962555 332064
losses 12 3.2188758248682006 332084
lossless 5 4.0943445622221 332184
lossy 3 4.605170185988092 332228
lost 8 3.624340932976365 332256
lot 1 5.703782474656201 332324
lottery 4 4.31748811353631 332336
low 9 3.506557897319982 332372
low-capacity 2 5.0106352940962555 332448
low-data 2 5.0106352940962555 332468
low-density 2 5.0106352940962555 332488
low-dimensional 16 2.9311937524164198 332508
low-dose 2 5.0106352940962555 332640
low-entropy 2 5.0106352940962555 332660
low-level 8 3.624340932976365 332680
low-rank 4 4.31748811353631 332748
low-resolution 4 4.31748811353631 332784
low-resource 2 5.0106352940962555 332820
low-variance 2 5.0106352940962555 332840
lower 32 2.2380465718564744 332860
lower-level 4 4.31748811353631 333120
lowest-cost 2 5.0106352940962555 333156
lowresolution 2 5.0106352940962555 333176
lp 2 5.0106352940962555 333196
lpn 2 5.0106352940962555 333216
lqvlgh 2 5.0106352940962555 333236
ls-iq 3 4.605170185988092 333256
lstm 8 3.624340932976365 333284
lstm-lm 2 5.0106352940962555 333352
lstms 4 4.31748811353631 333372
lsun-bedroom 2 5.0106352940962555 333408
lsun-church 2 5.0106352940962555 333428
lti 2 5.0106352940962555 333448
ltr 2 5.0106352940962555 333468
ludger 2 5.0106352940962555 333488
luo 2 5.0106352940962555 333508
luong 2 5.0106352940962555 333528
m 12 3.2188758248682006 333548
m3 2 5.0106352940962555 333648
ma 2 5.0106352940962555 333668
machine 70 1.455287232606842 333688
machine-learning 2 5.0106352940962555 334252
machinery 4 4.31748811353631 334272
mackey 2 5.0106352940962555 334308
made 22 2.6127400212978853 334328
madras 2 5.0106352940962555 334508
mae 2 5.0106352940962555 334528
mag 2 5.0106352940962555 334548
magnetic 2 5.0106352940962555 334568
magnitude 20 2.70805020110221 334588
magnitude-based 3 4.605170185988092 334752
magnitudebased 2 5.0106352940962555 334780
magnitudes 6 3.912023005428146 334800
mahoney 2 5.0106352940962555 334852
main 24 2.5257286443082556 334872
mainly 10 3.4011973816621555 335068
maintain 8 3.624340932976365 335152
maintained 2 5.0106352940962555 335220
maintaining 22 2.6127400212978853 335240
maintains 4 4.31748811353631 335420
maintenance 2 5.0106352940962555 335456
major 16 2.9311937524164198 335476
majority 8 3.624340932976365 335608
make 48 1.8325814637483102 335676
makers 2 5.0106352940962555 336064
makes 38 2.0661963149298153 336084
making 45 1.8971199848858813 336392
malconv 2 5.0106352940962555 336756
malicious 8 3.624340932976365 336776
malware 3 4.605170185988092 336844
malware-detection 2 5.0106352940962555 336872
maml 4 4.31748811353631 336892
manages 2 5.0106352940962555 336928
manifests 2 5.0106352940962555 336948
manifold 12 3.2188758248682006 336968
manifolds 5 4.0943445622221 337068
manifolds.we 2 5.0106352940962555 337112
manipulability 2 5.0106352940962555 337132
manipulate 10 3.4011973816621555 337152
manipulated 2 5.0106352940962555 337236
manipulating 8 3.624340932976365 337256
manipulation 17 2.870569130599985 337324
manipulations 2 5.0106352940962555 337464
manipulators 2 5.0106352940962555 337484
manner 24 2.5257286443082556 337504
manner-enjoys 2 5.0106352940962555 337700
manner.in 4 4.31748811353631 337720
manner.sparse-former 2 5.0106352940962555 337756
manning 2 5.0106352940962555 337776
mantas 2 5.0106352940962555 337796
manual 6 3.912023005428146 337816
manually 8 3.624340932976365 337868
manually-labelled 2 5.0106352940962555 337936
manuel 2 5.0106352940962555 337956
many 88 1.2264456601779945 337976
map 11 3.3058872018578307 338684
mapg 2 5.0106352940962555 338776
mapping 12 3.2188758248682006 338796
mappings 2 5.0106352940962555 338896
maps 18 2.8134107167600364 338916
marcel 2 5.0106352940962555 339064
margin 12 3.2188758248682006 339084
marginal 12 3.2188758248682006 339184
marginalizing 2 5.0106352940962555 339284
marginally 2 5.0106352940962555 339304
margins 2 5.0106352940962555 339324
mark 4 4.31748811353631 339344
markov 26 2.445685936634719 339380
markov-approximate 2 5.0106352940962555 339592
markovian 2 5.0106352940962555 339612
marlin 2 5.0106352940962555 339632
maron 2 5.0106352940962555 339652
martingale 3 4.605170185988092 339672
mask 7 3.7578723256008875 339700
masked 2 5.0106352940962555 339760
masking 2 5.0106352940962555 339780
masking.to 2 5.0106352940962555 339800
masks 4 4.31748811353631 339820
massive 4 4.31748811353631 339856
masson 2 5.0106352940962555 339892
master 2 5.0106352940962555 339912
match 18 2.8134107167600364 339932
matched 4 4.31748811353631 340080
matches 14 3.0647251450409425 340116
matching 16 2.9311937524164198 340232
material 5 4.0943445622221 340364
materials 2 5.0106352940962555 340408
math 2 5.0106352940962555 340428
mathematical 6 3.912023005428146 340448
mathematically 2 5.0106352940962555 340500
mathematicians 2 5.0106352940962555 340520
mathematics 8 3.624340932976365 340540
matplotlib.pyplot 2 5.0106352940962555 340608
matrices 12 3.2188758248682006 340628
matrix 29 2.336486644669727 340728
matter 2 5.0106352940962555 340964
matters 4 4.31748811353631 340984
maximal 4 4.31748811353631 341020
maximally 2 5.0106352940962555 341056
maximisation 2 5.0106352940962555 341076
maximise 2 5.0106352940962555 341096
maximises 2 5.0106352940962555 341116
maximising 2 5.0106352940962555 341136
maximization 4 4.31748811353631 341156
maximize 6 3.912023005428146 341192
maximized 2 5.0106352940962555 341244
maximizer 2 5.0106352940962555 341264
maximizes 4 4.31748811353631 341284
maximizing 10 3.4011973816621555 341320
maximum 23 2.5682882587270512 341404
maximum-likelihood 2 5.0106352940962555 341592
may 37 2.0928645620119766 341612
maze 2 5.0106352940962555 341912
mazeika 2 5.0106352940962555 341932
mbrl 2 5.0106352940962555 341952
mccfr 2 5.0106352940962555 341972
mcmc 2 5.0106352940962555 341992
mcmc-based 2 5.0106352940962555 342012
mcts 2 5.0106352940962555 342032
md 1 5.703782474656201 342052
mdp 9 3.506557897319982 342064
mdps 5 4.0943445622221 342140
mdps.adversarial 2 5.0106352940962555 342184
mean 16 2.9311937524164198 342204
mean-field 3 4.605170185988092 342336
meaning 10 3.4011973816621555 342364
meaningful 8 3.624340932976365 342448
meanings 2 5.0106352940962555 342516
means 8 3.624340932976365 342536
meanwhile 8 3.624340932976365 342604
measure 14 3.0647251450409425 342672
measured 6 3.912023005428146 342788
measurement 5 4.0943445622221 342840
measurements 4 4.31748811353631 342884
measures 18 2.8134107167600364 342920
measuring 14 3.0647251450409425 343068
mechanics 4 4.31748811353631 343184
mechanism 43 1.9425823589626385 343220
mechanisms 10 3.4011973816621555 343568
mechanisms.code 2 5.0106352940962555 343652
medfair 3 4.605170185988092 343672
medfair.published 2 5.0106352940962555 343700
media 4 4.31748811353631 343720
mediated 2 5.0106352940962555 343756
medical 7 3.7578723256008875 343776
medicine 4 4.31748811353631 343836
medicine.however 2 5.0106352940962555 343872
medium 2 5.0106352940962555 343892
medium-batch 2 5.0106352940962555 343912
meet 4 4.31748811353631 343932
meets 4 4.31748811353631 343968
mehrabi 2 5.0106352940962555 344004
memories 2 5.0106352940962555 344024
memorization 3 4.605170185988092 344044
memorized 2 5.0106352940962555 344072
memorizing 2 5.0106352940962555 344092
memory 43 1.9425823589626385 344112
memory-efficient 2 5.0106352940962555 344460
memory.however 2 5.0106352940962555 344480
mental 2 5.0106352940962555 344500
mention 4 4.31748811353631 344520
mentioned 4 4.31748811353631 344556
merely 10 3.4011973816621555 344592
merging 3 4.605170185988092 344676
merging-there 2 5.0106352940962555 344704
merits 2 5.0106352940962555 344724
mertyg 2 5.0106352940962555 344744
meshes 2 5.0106352940962555 344764
message 6 3.912023005428146 344784
message-passing 8 3.624340932976365 344836
messagecode 2 5.0106352940962555 344904
messages 4 4.31748811353631 344924
meta-benchmark 2 5.0106352940962555 344960
meta-evaluation 2 5.0106352940962555 344980
meta-gradients 2 5.0106352940962555 345000
meta-imitation 2 5.0106352940962555 345020
meta-learned 2 5.0106352940962555 345040
meta-learner 2 5.0106352940962555 345060
meta-learning 25 2.4849066497880004 345080
meta-optimization 2 5.0106352940962555 345284
meta-procedure 2 5.0106352940962555 345304
meta-reinforcement 4 4.31748811353631 345324
meta-trained 2 5.0106352940962555 345360
meta-training 4 4.31748811353631 345380
meta-world 2 5.0106352940962555 345416
metamer 2 5.0106352940962555 345436
metameric 2 5.0106352940962555 345456
metamerism 2 5.0106352940962555 345476
metamers 2 5.0106352940962555 345496
method 218 0.319287411867112 345516
method.our 2 5.0106352940962555 347264
method.remarkably 2 5.0106352940962555 347284
methodology 10 3.4011973816621555 347304
methods 198 0.4155154439616658 347388
methods.figure 2 5.0106352940962555 348976
methods.guess 2 5.0106352940962555 348996
methods.morec 2 5.0106352940962555 349016
methods.our 2 5.0106352940962555 349036
meticulous 2 5.0106352940962555 349056
metric 28 2.371577964480997 349076
metric.with 2 5.0106352940962555 349304
metricity 2 5.0106352940962555 349324
metrics 28 2.371577964480997 349344
metrics.research 2 5.0106352940962555 349572
metrizability 2 5.0106352940962555 349592
metrizable 2 5.0106352940962555 349612
mgno 3 4.605170185988092 349632
mi 2 5.0106352940962555 349660
micromanagement 2 5.0106352940962555 349680
mid-level 2 5.0106352940962555 349700
middle 4 4.31748811353631 349720
mige 2 5.0106352940962555 349756
might 14 3.0647251450409425 349776
migration 2 5.0106352940962555 349892
mihaela 2 5.0106352940962555 349912
mild 8 3.624340932976365 349932
miller 4 4.31748811353631 350000
million 2 5.0106352940962555 350036
millions 2 5.0106352940962555 350056
milos 2 5.0106352940962555 350076
mimic 8 3.624340932976365 350096
min 2 5.0106352940962555 350164
min-max 5 4.0943445622221 350184
mind 4 4.31748811353631 350228
minh-thang 2 5.0106352940962555 350264
mini-batch 2 5.0106352940962555 350284
minibatch 4 4.31748811353631 350304
minif2f-test 2 5.0106352940962555 350340
minif2f-valid 2 5.0106352940962555 350360
minima 5 4.0943445622221 350380
minimal 22 2.6127400212978853 350424
minimal-entropy 2 5.0106352940962555 350604
minimalist 2 5.0106352940962555 350624
minimality 2 5.0106352940962555 350644
minimally 2 5.0106352940962555 350664
minimario 2 5.0106352940962555 350684
minimax 8 3.624340932976365 350704
minimization 29 2.336486644669727 350772
minimize 14 3.0647251450409425 351008
minimized 4 4.31748811353631 351124
minimizers 2 5.0106352940962555 351160
minimizes 4 4.31748811353631 351180
minimizing 10 3.4011973816621555 351216
minimum 10 3.4011973816621555 351300
minlie 2 5.0106352940962555 351384
minor 4 4.31748811353631 351404
minutes 4 4.31748811353631 351440
miou 2 5.0106352940962555 351476
mirror 2 5.0106352940962555 351496
misclassify 2 5.0106352940962555 351516
misconception 2 5.0106352940962555 351536
misinformation.here 2 5.0106352940962555 351556
mislead 4 4.31748811353631 351576
misleads 2 5.0106352940962555 351612
mismatch 8 3.624340932976365 351632
mismatch.in 2 5.0106352940962555 351700
missed 4 4.31748811353631 351720
missing 6 3.912023005428146 351756
mission 2 5.0106352940962555 351808
mistakes 2 5.0106352940962555 351828
mistreat 2 5.0106352940962555 351848
mitigate 24 2.5257286443082556 351868
mitigates 4 4.31748811353631 352064
mitigating 12 3.2188758248682006 352100
mitigation 2 5.0106352940962555 352200
mitter 2 5.0106352940962555 352220
mix 1 5.703782474656201 352240
mixed 6 3.912023005428146 352252
mixing 2 5.0106352940962555 352304
mixture 16 2.9311937524164198 352324
mixture-of-experts 3 4.605170185988092 352456
mixtures 4 4.31748811353631 352484
mixup 5 4.0943445622221 352520
mixup-trained 2 5.0106352940962555 352564
ml 3 4.605170185988092 352584
mle 4 4.31748811353631 352612
mle.we 2 5.0106352940962555 352648
mlp 2 5.0106352940962555 352668
mmcv 2 5.0106352940962555 352688
mmd 5 4.0943445622221 352708
mmlu 2 5.0106352940962555 352752
mnih 2 5.0106352940962555 352772
mnist 14 3.0647251450409425 352792
mnp 1 5.703782474656201 352908
mobile 2 5.0106352940962555 352920
moco 4 4.31748811353631 352940
modalities 6 3.912023005428146 352976
modality 4 4.31748811353631 353028
mode 7 3.7578723256008875 353064
mode-covering 2 5.0106352940962555 353124
model 279 0.07257069283483537 353144
model-agnostic 6 3.912023005428146 355380
model-based 10 3.4011973816621555 355432
model-building 2 5.0106352940962555 355516
model-compression 2 5.0106352940962555 355536
model-editing 2 5.0106352940962555 355556
model-free 8 3.624340932976365 355576
model-level 2 5.0106352940962555 355644
model.abstractwe 2 5.0106352940962555 355664
model.in 4 4.31748811353631 355684
model.our 2 5.0106352940962555 355720
model.specifically 2 5.0106352940962555 355740
model.to 2 5.0106352940962555 355760
modelbased 2 5.0106352940962555 355780
modeled 10 3.4011973816621555 355800
modeling 69 1.4696759700589417 355884
modeling.emo 2 5.0106352940962555 356440
modeling.however 2 5.0106352940962555 356460
modellanguage 2 5.0106352940962555 356480
modelled 2 5.0106352940962555 356500
modelling 8 3.624340932976365 356520
modelling.this 2 5.0106352940962555 356588
models 328 -0.08923113372794264 356608
models.during 2 5.0106352940962555 359236
models.in 2 5.0106352940962555 359256
models.kitab 2 5.0106352940962555 359276
models.local 2 5.0106352940962555 359296
models.machine 2 5.0106352940962555 359316
models.note 2 5.0106352940962555 359336
models.our 8 3.624340932976365 359356
models.theoretically 2 5.0106352940962555 359424
models.this 2 5.0106352940962555 359444
models.we 6 3.912023005428146 359464
models.while 2 5.0106352940962555 359516
moderate 2 5.0106352940962555 359536
moderate-sized 2 5.0106352940962555 359556
moderately 2 5.0106352940962555 359576
moderating 2 5.0106352940962555 359596
moderation.related 2 5.0106352940962555 359616
modern 14 3.0647251450409425 359636
modes 12 3.2188758248682006 359752
modification 8 3.624340932976365 359852
modifications 6 3.912023005428146 359920
modified 6 3.912023005428146 359972
modify 2 5.0106352940962555 360024
modifying 8 3.624340932976365 360044
modular 9 3.506557897319982 360112
modularity 4 4.31748811353631 360188
modularly 2 5.0106352940962555 360224
modulate 3 4.605170185988092 360244
modulated 2 5.0106352940962555 360272
modulates 2 5.0106352940962555 360292
module 21 2.659260036932778 360312
modules 10 3.4011973816621555 360484
moe 4 4.31748811353631 360568
moe-base 2 5.0106352940962555 360604
moes 2 5.0106352940962555 360624
mohamed 2 5.0106352940962555 360644
mohammad 2 5.0106352940962555 360664
molchanov 2 5.0106352940962555 360684
molecular 7 3.7578723256008875 360704
molecule 6 3.912023005428146 360764
molecule-and 2 5.0106352940962555 360816
molecules 5 4.0943445622221 360836
molecules.in 2 5.0106352940962555 360880
molecules.while 2 5.0106352940962555 360900
moment-based 2 5.0106352940962555 360920
momentum 9 3.506557897319982 360940
momentum-based 2 5.0106352940962555 361016
monet 2 5.0106352940962555 361036
money 2 5.0106352940962555 361056
monitoring 2 5.0106352940962555 361076
monocular 2 5.0106352940962555 361096
monolingual 4 4.31748811353631 361116
monotone 2 5.0106352940962555 361152
monotonically 2 5.0106352940962555 361172
monte 8 3.624340932976365 361192
mopo 2 5.0106352940962555 361260
morbit 2 5.0106352940962555 361280
morcos 2 5.0106352940962555 361300
more 174 0.5447271754416719 361320
morec 2 5.0106352940962555 362716
moreover 46 1.875141078167106 362736
morerio 2 5.0106352940962555 363108
morphological 2 5.0106352940962555 363128
morphologies 2 5.0106352940962555 363148
morphology 3 4.605170185988092 363168
most 104 1.0593915755148284 363196
mostly 12 3.2188758248682006 364032
motif 2 5.0106352940962555 364132
motif-scaffolding 3 4.605170185988092 364152
motion 13 3.138833117194664 364180
motion-prompt 2 5.0106352940962555 364288
motion.both 2 5.0106352940962555 364308
motions 2 5.0106352940962555 364328
motivate 4 4.31748811353631 364348
motivated 28 2.371577964480997 364384
motivating 4 4.31748811353631 364612
motivation 6 3.912023005428146 364648
mount 2 5.0106352940962555 364700
move 2 5.0106352940962555 364720
movements 2 5.0106352940962555 364740
mover 3 4.605170185988092 364760
moves 6 3.912023005428146 364788
movie 2 5.0106352940962555 364840
movielens 2 5.0106352940962555 364860
moving 8 3.624340932976365 364880
mp 2 5.0106352940962555 364948
mpnn 4 4.31748811353631 364968
mpnns 2 5.0106352940962555 365004
mpnp 2 5.0106352940962555 365024
mre 1 5.703782474656201 365044
mrl 2 5.0106352940962555 365056
ms 2 5.0106352940962555 365076
ms-gqa 2 5.0106352940962555 365096
mse 3 4.605170185988092 365116
msgd 3 4.605170185988092 365144
msra 2 5.0106352940962555 365172
mt 2 5.0106352940962555 365192
mtan 2 5.0106352940962555 365212
much 36 2.120263536200091 365232
muffin 2 5.0106352940962555 365524
mujoco 4 4.31748811353631 365544
multi-agent 6 3.912023005428146 365580
multi-armed 2 5.0106352940962555 365632
multi-batch 2 5.0106352940962555 365652
multi-camera 2 5.0106352940962555 365672
multi-channel 2 5.0106352940962555 365692
multi-file 2 5.0106352940962555 365712
multi-frame 2 5.0106352940962555 365732
multi-head 2 5.0106352940962555 365752
multi-hop 3 4.605170185988092 365772
multi-label 2 5.0106352940962555 365800
multi-layer 2 5.0106352940962555 365820
multi-level 2 5.0106352940962555 365840
multi-modal 15 2.995732273553991 365860
multi-modalities 2 5.0106352940962555 365984
multi-modality 2 5.0106352940962555 366004
multi-object 2 5.0106352940962555 366024
multi-objective 3 4.605170185988092 366044
multi-observation 2 5.0106352940962555 366072
multi-step 4 4.31748811353631 366092
multi-task 6 3.912023005428146 366128
multi-time 3 4.605170185988092 366180
multi-view 6 3.912023005428146 366208
multi-way 2 5.0106352940962555 366260
multiagent 4 4.31748811353631 366280
multicore 2 5.0106352940962555 366316
multiframe 2 5.0106352940962555 366336
multigrid 3 4.605170185988092 366356
multilayer 3 4.605170185988092 366384
multilingual 4 4.31748811353631 366412
multimodal 10 3.4011973816621555 366448
multimodality 2 5.0106352940962555 366532
multinomial 4 4.31748811353631 366552
multiple 57 1.6607312068216509 366588
multiplication 6 3.912023005428146 367048
multiplications 2 5.0106352940962555 367100
multiplicative 4 4.31748811353631 367120
multiplied 4 4.31748811353631 367156
multiset 2 5.0106352940962555 367192
multisets 2 5.0106352940962555 367212
multistage 2 5.0106352940962555 367232
multitask 2 5.0106352940962555 367252
multitude 2 5.0106352940962555 367272
multivariate 6 3.912023005428146 367292
murino 2 5.0106352940962555 367344
murray 2 5.0106352940962555 367364
muscles 2 5.0106352940962555 367384
must 18 2.8134107167600364 367404
mutual 5 4.0943445622221 367552
mutually-teaching 2 5.0106352940962555 367596
n 19 2.7593434954897607 367616
n-gram 2 5.0106352940962555 367772
n. 2 5.0106352940962555 367792
naive 6 3.912023005428146 367812
name 14 3.0647251450409425 367864
named 16 2.9311937524164198 367980
namely 12 3.2188758248682006 368112
names 2 5.0106352940962555 368212
narang 2 5.0106352940962555 368232
narrative 2 5.0106352940962555 368252
narrow 4 4.31748811353631 368272
nash 4 4.31748811353631 368308
nati 2 5.0106352940962555 368344
natural 72 1.4271163556401458 368364
natural-language-based 2 5.0106352940962555 368944
naturally 16 2.9311937524164198 368964
nature 26 2.445685936634719 369096
naver 2 5.0106352940962555 369308
navigation 6 3.912023005428146 369328
navigational 2 5.0106352940962555 369380
navely 2 5.0106352940962555 369400
nc 1 5.703782474656201 369420
ncn 2 5.0106352940962555 369432
ncnc 2 5.0106352940962555 369452
nds 1 5.703782474656201 369472
ne 2 5.0106352940962555 369484
near 6 3.912023005428146 369504
near-linear 3 4.605170185988092 369556
near-neighbor 2 5.0106352940962555 369584
near-optimal 15 2.995732273553991 369604
near-orthogonal 2 5.0106352940962555 369728
nearest 4 4.31748811353631 369748
nearly 8 3.624340932976365 369784
necessarily 8 3.624340932976365 369852
necessary 18 2.8134107167600364 369920
necessitate 6 3.912023005428146 370068
necessitates 2 5.0106352940962555 370120
necessitating 2 5.0106352940962555 370140
necessity 4 4.31748811353631 370160
need 50 1.791759469228055 370196
needed 12 3.2188758248682006 370600
needs 8 3.624340932976365 370700
negative 18 2.8134107167600364 370768
negatively 2 5.0106352940962555 370916
neglect 2 5.0106352940962555 370936
neglecting 4 4.31748811353631 370956
negligible 4 4.31748811353631 370992
negotiate 2 5.0106352940962555 371028
negotiation 3 4.605170185988092 371048
neighbor 7 3.7578723256008875 371076
neighborhood 6 3.912023005428146 371136
neighboring 4 4.31748811353631 371188
neighbors 6 3.912023005428146 371224
neither 6 3.912023005428146 371276
neorl 2 5.0106352940962555 371328
nerf 2 5.0106352940962555 371348
nesterov 2 5.0106352940962555 371368
network 155 0.6603573577369545 371388
networks 213 0.3424903089467759 372632
networks.1 2 5.0106352940962555 374340
networks.in 2 5.0106352940962555 374360
networks.we 2 5.0106352940962555 374380
neu-vi.github.io 2 5.0106352940962555 374400
neural 257 0.15470638976098133 374420
neural-sde 2 5.0106352940962555 376480
neural-sdes.in 2 5.0106352940962555 376500
neuralcommonneighbor 2 5.0106352940962555 376520
neurally 2 5.0106352940962555 376540
neurofovea 2 5.0106352940962555 376560
neuromorphic 2 5.0106352940962555 376580
neuron 10 3.4011973816621555 376600
neurons 16 2.9311937524164198 376684
neurons.this 2 5.0106352940962555 376816
neuroscience 4 4.31748811353631 376836
neurosymbolic 2 5.0106352940962555 376872
never 6 3.912023005428146 376892
nevertheless 4 4.31748811353631 376944
new 85 1.2611312181658845 376980
newly 4 4.31748811353631 377664
next 14 3.0647251450409425 377700
next-line 2 5.0106352940962555 377816
next-token 2 5.0106352940962555 377836
next-token-prediction 2 5.0106352940962555 377856
next-word 2 5.0106352940962555 377876
nexttoken 2 5.0106352940962555 377896
nfe 2 5.0106352940962555 377916
nfe.on 2 5.0106352940962555 377936
nfnets 2 5.0106352940962555 377956
nfsp 2 5.0106352940962555 377976
nice 2 5.0106352940962555 377996
nicolas 2 5.0106352940962555 378016
ninr 2 5.0106352940962555 378036
ninr.the 2 5.0106352940962555 378056
nips 2 5.0106352940962555 378076
niv2 2 5.0106352940962555 378096
nlds 2 5.0106352940962555 378116
nlds.in 2 5.0106352940962555 378136
nlg 2 5.0106352940962555 378156
nll 4 4.31748811353631 378176
nlp 5 4.0943445622221 378212
nmst 2 5.0106352940962555 378256
nmt 2 5.0106352940962555 378276
nn 3 4.605170185988092 378296
nns 5 4.0943445622221 378324
no 26 2.445685936634719 378368
node 19 2.7593434954897607 378580
node-deleted 2 5.0106352940962555 378736
node-level 2 5.0106352940962555 378756
nodes 20 2.70805020110221 378776
noida 2 5.0106352940962555 378940
noise 26 2.445685936634719 378960
noise-shaping 2 5.0106352940962555 379172
noise.beyond 2 5.0106352940962555 379192
noiseless 2 5.0106352940962555 379212
noises 6 3.912023005428146 379232
noising 2 5.0106352940962555 379284
noisy 24 2.5257286443082556 379304
non 2 5.0106352940962555 379500
non-adaptive 2 5.0106352940962555 379520
non-additive 2 5.0106352940962555 379540
non-ar 2 5.0106352940962555 379560
non-autoregressive 2 5.0106352940962555 379580
non-bayesian 2 5.0106352940962555 379600
non-branching 2 5.0106352940962555 379620
non-concave 2 5.0106352940962555 379640
non-convergence 2 5.0106352940962555 379660
non-convex 8 3.624340932976365 379680
non-convolutional 2 5.0106352940962555 379748
non-differentiabilty 2 5.0106352940962555 379768
non-differentiable 6 3.912023005428146 379788
non-euclidean 2 5.0106352940962555 379840
non-expert 2 5.0106352940962555 379860
non-i.i.d 2 5.0106352940962555 379880
non-iid 2 5.0106352940962555 379900
non-image 2 5.0106352940962555 379920
non-invertible 2 5.0106352940962555 379940
non-linear 8 3.624340932976365 379960
non-linearities 2 5.0106352940962555 380028
non-linguistic 4 4.31748811353631 380048
non-local 2 5.0106352940962555 380084
non-maleficence 2 5.0106352940962555 380104
non-markovian 2 5.0106352940962555 380124
non-monotonic 3 4.605170185988092 380144
non-natural 2 5.0106352940962555 380172
non-overlapping 2 5.0106352940962555 380192
non-parallel 2 5.0106352940962555 380212
non-quadratic 2 5.0106352940962555 380232
non-saturating 2 5.0106352940962555 380252
non-semantic 2 5.0106352940962555 380272
non-smooth 4 4.31748811353631 380292
non-sparse 2 5.0106352940962555 380328
non-stationary 4 4.31748811353631 380348
non-terminating 2 5.0106352940962555 380384
non-termination 2 5.0106352940962555 380404
non-transparent 2 5.0106352940962555 380424
non-trivial 2 5.0106352940962555 380444
non-uniform 6 3.912023005428146 380464
non-vacuous 2 5.0106352940962555 380516
non-victim 2 5.0106352940962555 380536
non-zero 4 4.31748811353631 380556
nonconvex 5 4.0943445622221 380592
none 2 5.0106352940962555 380636
nonequal 2 5.0106352940962555 380656
nonetheless 2 5.0106352940962555 380676
nonlinear 12 3.2188758248682006 380696
nonlinearly 2 5.0106352940962555 380796
nonsmooth 5 4.0943445622221 380816
nontrivial 2 5.0106352940962555 380860
nonzero 2 5.0106352940962555 380880
nonzero-valued 2 5.0106352940962555 380900
nor 4 4.31748811353631 380920
norm 8 3.624340932976365 380956
normal 2 5.0106352940962555 381024
normality 2 5.0106352940962555 381044
normalization 15 2.995732273553991 381064
normalized 4 4.31748811353631 381188
normalizing 9 3.506557897319982 381224
normally 2 5.0106352940962555 381300
normed 2 5.0106352940962555 381320
norms 2 5.0106352940962555 381340
not 121 0.90799192905946 381360
notable 4 4.31748811353631 382332
notably 14 3.0647251450409425 382368
notations 2 5.0106352940962555 382484
note 4 4.31748811353631 382504
noteworthy 2 5.0106352940962555 382540
noticeably 2 5.0106352940962555 382560
notion 12 3.2188758248682006 382580
notoriously 4 4.31748811353631 382680
nov 2 5.0106352940962555 382716
novel 132 0.8209805520698303 382736
novelty 4 4.31748811353631 383796
now 6 3.912023005428146 383832
np 3 4.605170185988092 383884
np-hard 2 5.0106352940962555 383912
np.linspace 2 5.0106352940962555 383932
np.random.normal 2 5.0106352940962555 383952
npd 2 5.0106352940962555 383972
nps 2 5.0106352940962555 383992
nr3d 2 5.0106352940962555 384012
nsynth 2 5.0106352940962555 384032
ntk 6 3.912023005428146 384052
ntks 2 5.0106352940962555 384104
nuanced 2 5.0106352940962555 384124
nuances 2 5.0106352940962555 384144
nuclear 2 5.0106352940962555 384164
nucleus 4 4.31748811353631 384184
nuggets 2 5.0106352940962555 384220
number 102 1.07880966137193 384240
numbers 8 3.624340932976365 385060
numerical 14 3.0647251450409425 385128
numerous 6 3.912023005428146 385244
numpy 2 5.0106352940962555 385296
nuscenes 2 5.0106352940962555 385316
nvidia 2 5.0106352940962555 385336
nvs 2 5.0106352940962555 385356
ny 1 5.703782474656201 385376
nygaard 2 5.0106352940962555 385388
o 29 2.336486644669727 385408
o3 1 5.703782474656201 385644
oasis 2 5.0106352940962555 385656
oasst 2 5.0106352940962555 385676
object 33 2.207274913189721 385696
object- 2 5.0106352940962555 385964
object-based 2 5.0106352940962555 385984
object-centric 4 4.31748811353631 386004
object-pursuit 2 5.0106352940962555 386040
objective 48 1.8325814637483102 386060
objectives 22 2.6127400212978853 386448
objects 29 2.336486644669727 386628
oblivious 2 5.0106352940962555 386864
observability 6 3.912023005428146 386884
observable 6 3.912023005428146 386936
observation 36 2.120263536200091 386988
observation.we 2 5.0106352940962555 387280
observational 2 5.0106352940962555 387300
observations 53 1.7334905611040792 387320
observations-from 2 5.0106352940962555 387748
observations-sufficient 2 5.0106352940962555 387768
observe 24 2.5257286443082556 387788
observed 26 2.445685936634719 387984
observers 2 5.0106352940962555 388196
observes 2 5.0106352940962555 388216
observing 8 3.624340932976365 388236
obstacle 2 5.0106352940962555 388304
obstacles 4 4.31748811353631 388324
obstruct 2 5.0106352940962555 388360
obtain 32 2.2380465718564744 388380
obtained 24 2.5257286443082556 388640
obtaining 10 3.4011973816621555 388836
obtains 6 3.912023005428146 388920
obviates 2 5.0106352940962555 388972
obvious 2 5.0106352940962555 388992
obviously 2 5.0106352940962555 389012
occupancy 4 4.31748811353631 389032
occur 6 3.912023005428146 389068
occurence 2 5.0106352940962555 389120
occurrences 2 5.0106352940962555 389140
occurs 4 4.31748811353631 389160
ocd 2 5.0106352940962555 389196
oct 3 4.605170185988092 389216
octocoder 2 5.0106352940962555 389244
octogeex 2 5.0106352940962555 389264
octopack 3 4.605170185988092 389284
ode 3 4.605170185988092 389312
odes 2 5.0106352940962555 389340
oe_lay 2 5.0106352940962555 389360
of 348 -0.14842000511827333 389380
off 1 5.703782474656201 392168
off-policy 5 4.0943445622221 392180
off-the-shelf 4 4.31748811353631 392224
offensive 2 5.0106352940962555 392260
offer 12 3.2188758248682006 392280
offering 10 3.4011973816621555 392380
offers 20 2.70805020110221 392464
offline 26 2.445685936634719 392628
often 88 1.2264456601779945 392840
oftishby 2 5.0106352940962555 393548
of 4 4.31748811353631 393568
ogb 2 5.0106352940962555 393604
ohad 2 5.0106352940962555 393624
ohw 2 5.0106352940962555 393644
oi 2 5.0106352940962555 393664
old 1 5.703782474656201 393684
omit 2 5.0106352940962555 393696
omnia 2 5.0106352940962555 393716
omnicontrol 3 4.605170185988092 393736
omwu 2 5.0106352940962555 393764
omwuby 2 5.0106352940962555 393784
on 272 0.09798040836020382 393804
on-chip 2 5.0106352940962555 395984
on-device 3 4.605170185988092 396004
once 8 3.624340932976365 396032
one 69 1.4696759700589417 396100
one-class 3 4.605170185988092 396656
one-dimensional 2 5.0106352940962555 396684
one-hot 7 3.7578723256008875 396704
one-shot 6 3.912023005428146 396764
one-sided 2 5.0106352940962555 396816
one-to-many 2 5.0106352940962555 396836
one-to-one 2 5.0106352940962555 396856
onehas 2 5.0106352940962555 396876
ones 18 2.8134107167600364 396896
ongoing 4 4.31748811353631 397044
online 21 2.659260036932778 397080
only 149 0.6998361687107421 397252
onto 4 4.31748811353631 398448
ood 3 4.605170185988092 398484
oord 4 4.31748811353631 398512
open 16 2.9311937524164198 398548
open-ended 2 5.0106352940962555 398680
open-mmlab 2 5.0106352940962555 398700
open-participation 2 5.0106352940962555 398720
open-set 2 5.0106352940962555 398740
open-source 10 3.4011973816621555 398760
open-sourced 4 4.31748811353631 398844
open-sourceour 2 5.0106352940962555 398880
open-world 3 4.605170185988092 398900
openai 2 5.0106352940962555 398928
openbookqa 2 5.0106352940962555 398948
openllama 2 5.0106352940962555 398968
opens 4 4.31748811353631 398988
opera 2 5.0106352940962555 399024
operands 2 5.0106352940962555 399044
operate 4 4.31748811353631 399064
operates 12 3.2188758248682006 399100
operating 2 5.0106352940962555 399200
operation 4 4.31748811353631 399220
operations 20 2.70805020110221 399256
operator 12 3.2188758248682006 399420
operator-norm 2 5.0106352940962555 399520
operators 13 3.138833117194664 399540
operators.moreover 2 5.0106352940962555 399648
operators.we 2 5.0106352940962555 399668
operators.yet 2 5.0106352940962555 399688
opinion 2 5.0106352940962555 399708
oppolypdqq 2 5.0106352940962555 399728
opportunities 2 5.0106352940962555 399748
opportunity 6 3.912023005428146 399768
opposed 4 4.31748811353631 399820
opposite 2 5.0106352940962555 399856
opt 4 4.31748811353631 399876
optical 2 5.0106352940962555 399912
optima 2 5.0106352940962555 399932
optimal 72 1.4271163556401458 399952
optimality 6 3.912023005428146 400532
optimisation 2 5.0106352940962555 400584
optimises 2 5.0106352940962555 400604
optimism 4 4.31748811353631 400624
optimistic 6 3.912023005428146 400660
optimization 98 1.118814995985629 400712
optimization-based 6 3.912023005428146 401500
optimization.lastly 2 5.0106352940962555 401552
optimization.learning 2 5.0106352940962555 401572
optimization.moreover 2 5.0106352940962555 401592
optimize 32 2.2380465718564744 401612
optimized 6 3.912023005428146 401872
optimizer 4 4.31748811353631 401924
optimizers 7 3.7578723256008875 401960
optimizes 10 3.4011973816621555 402020
optimizing 24 2.5257286443082556 402104
option 2 5.0106352940962555 402300
optionally 2 5.0106352940962555 402320
or 133 0.8134333464344473 402340
oracle 4 4.31748811353631 403408
oracle-complexity 2 5.0106352940962555 403444
oracles 6 3.912023005428146 403464
orca 2 5.0106352940962555 403516
orchestrating 2 5.0106352940962555 403536
order 36 2.120263536200091 403556
order.figure 2 5.0106352940962555 403848
order.is 2 5.0106352940962555 403868
ordered 4 4.31748811353631 403888
ordering 2 5.0106352940962555 403924
orders 12 3.2188758248682006 403944
ordinary 8 3.624340932976365 404044
organize 2 5.0106352940962555 404112
organocatalysts 2 5.0106352940962555 404132
oriented 2 5.0106352940962555 404152
original 48 1.8325814637483102 404172
originally 4 4.31748811353631 404560
originating 2 5.0106352940962555 404596
oriol 2 5.0106352940962555 404616
orthogonal 5 4.0943445622221 404636
osu 1 5.703782474656201 404680
ot 2 5.0106352940962555 404692
ot-flow 2 5.0106352940962555 404712
other 104 1.0593915755148284 404732
others 4 4.31748811353631 405568
otherwise 10 3.4011973816621555 405604
otr 2 5.0106352940962555 405688
our 188 0.46734051182625186 405708
ours 2 5.0106352940962555 407216
out 20 2.70805020110221 407236
out-of-distribution 17 2.870569130599985 407400
out-of-distributions 2 5.0106352940962555 407540
out-of-the-box 2 5.0106352940962555 407560
out-performs 2 5.0106352940962555 407580
outcomes 8 3.624340932976365 407600
outcomes.to 2 5.0106352940962555 407668
outdoors 2 5.0106352940962555 407688
outer 4 4.31748811353631 407708
outer-loop 2 5.0106352940962555 407744
outlier 4 4.31748811353631 407764
outliers 2 5.0106352940962555 407800
outline 2 5.0106352940962555 407820
outlined 2 5.0106352940962555 407840
outlooks 2 5.0106352940962555 407860
outof-distribution 2 5.0106352940962555 407880
outperform 44 1.91959284073794 407900
outperformed 2 5.0106352940962555 408256
outperforming 8 3.624340932976365 408276
outperforms 84 1.2729656758128876 408344
output 42 1.9661128563728327 409020
output.in 2 5.0106352940962555 409360
output.we 2 5.0106352940962555 409380
outputs 24 2.5257286443082556 409400
outputs.3 2 5.0106352940962555 409596
outputs.finally 2 5.0106352940962555 409616
outputs.we 2 5.0106352940962555 409636
outputting 2 5.0106352940962555 409656
outset 2 5.0106352940962555 409676
outside 2 5.0106352940962555 409696
outstanding 8 3.624340932976365 409716
over 121 0.90799192905946 409784
over-fitting 2 5.0106352940962555 410756
over-parameterized 6 3.912023005428146 410776
over-parametrized 2 5.0106352940962555 410828
over-squashing 2 5.0106352940962555 410848
over-training 3 4.605170185988092 410868
over-weighs 2 5.0106352940962555 410896
overall 20 2.70805020110221 410916
overcome 10 3.4011973816621555 411080
overcomplicated 2 5.0106352940962555 411164
overconfident 2 5.0106352940962555 411184
overfit 2 5.0106352940962555 411204
overfitting 8 3.624340932976365 411224
overhead 12 3.2188758248682006 411292
overlap 2 5.0106352940962555 411392
overlap.the 2 5.0106352940962555 411412
overload 2 5.0106352940962555 411432
overlook 6 3.912023005428146 411452
overlooked 8 3.624340932976365 411504
overlooking 2 5.0106352940962555 411572
overparameterization 2 5.0106352940962555 411592
overparameterized 8 3.624340932976365 411612
overparametrized 5 4.0943445622221 411680
overpruning 2 5.0106352940962555 411724
oversmoothing 2 5.0106352940962555 411744
oversquashing 3 4.605170185988092 411764
overview 4 4.31748811353631 411792
owes 2 5.0106352940962555 411828
owing 4 4.31748811353631 411848
own 5 4.0943445622221 411884
ownership 2 5.0106352940962555 411928
p 7 3.7578723256008875 411948
p. 1 5.703782474656201 412008
pac-bayes 5 4.0943445622221 412020
pace 2 5.0106352940962555 412064
pack 2 5.0106352940962555 412084
pade 2 5.0106352940962555 412104
page 6 3.912023005428146 412124
pages 2 5.0106352940962555 412176
paint 2 5.0106352940962555 412196
painter 2 5.0106352940962555 412216
painters 2 5.0106352940962555 412236
pair 12 3.2188758248682006 412256
pair-wise 2 5.0106352940962555 412356
paired 2 5.0106352940962555 412376
pairs 18 2.8134107167600364 412396
pairs.by 2 5.0106352940962555 412544
pairwise 2 5.0106352940962555 412564
palm-e 2 5.0106352940962555 412584
panel 2 5.0106352940962555 412604
papamakarios 2 5.0106352940962555 412624
paper 216 0.32850406697203605 412644
papers 2 5.0106352940962555 414376
papineni 2 5.0106352940962555 414396
par 3 4.605170185988092 414416
paradigm 27 2.407945608651872 414444
paradigm.this 2 5.0106352940962555 414664
paradigmatic 2 5.0106352940962555 414684
paradigms 10 3.4011973816621555 414704
parallel 18 2.8134107167600364 414788
parallel-in-time 2 5.0106352940962555 414936
parallelism 6 3.912023005428146 414956
parallelizability 2 5.0106352940962555 415008
parallelization 2 5.0106352940962555 415028
parameter 34 2.17742195004004 415048
parameter-efficient 2 5.0106352940962555 415324
parameter-free 4 4.31748811353631 415344
parameter-heavy 2 5.0106352940962555 415380
parameter-sharing 4 4.31748811353631 415400
parameter-space 2 5.0106352940962555 415436
parameterization 5 4.0943445622221 415456
parameterize 6 3.912023005428146 415500
parameterized 8 3.624340932976365 415552
parameters 52 1.7525387560747736 415620
parameters-typically 2 5.0106352940962555 416040
parameters.sheared-llama 2 5.0106352940962555 416060
parametersand 2 5.0106352940962555 416080
parametric 10 3.4011973816621555 416100
parametrization 2 5.0106352940962555 416184
parametrizations 2 5.0106352940962555 416204
parametrize 4 4.31748811353631 416224
parametrized 4 4.31748811353631 416260
parametrizing 2 5.0106352940962555 416296
paramount 2 5.0106352940962555 416316
parents 2 5.0106352940962555 416336
pareto-optimal 2 5.0106352940962555 416356
paretocontinuity 2 5.0106352940962555 416376
park 2 5.0106352940962555 416396
parsed 2 5.0106352940962555 416416
parsers 2 5.0106352940962555 416436
part 26 2.445685936634719 416456
partial 18 2.8134107167600364 416668
partially 21 2.659260036932778 416816
participants 4 4.31748811353631 416988
particle 6 3.912023005428146 417024
particular 66 1.5141277326297755 417076
particularly 22 2.6127400212978853 417608
partition 3 4.605170185988092 417788
partitioning 6 3.912023005428146 417816
partly 2 5.0106352940962555 417868
parts 8 3.624340932976365 417888
party 2 5.0106352940962555 417956
pass 12 3.2188758248682006 417976
pass.we 2 5.0106352940962555 418076
passed 2 5.0106352940962555 418096
passes 2 5.0106352940962555 418116
passing 10 3.4011973816621555 418136
past 14 3.0647251450409425 418220
patch 6 3.912023005428146 418336
patch-based 2 5.0106352940962555 418388
patch-dm 2 5.0106352940962555 418408
patched 2 5.0106352940962555 418428
patches 2 5.0106352940962555 418448
path 10 3.4011973816621555 418468
pathological 2 5.0106352940962555 418552
paths 6 3.912023005428146 418572
pathway 2 5.0106352940962555 418624
pattern 14 3.0647251450409425 418644
patterns 20 2.70805020110221 418760
paul 2 5.0106352940962555 418924
paves 2 5.0106352940962555 418944
paving 2 5.0106352940962555 418964
pavlakou 2 5.0106352940962555 418984
pcbm 2 5.0106352940962555 419004
pcbms 2 5.0106352940962555 419024
pclucas14 2 5.0106352940962555 419044
pcm 2 5.0106352940962555 419064
pde 1 5.703782474656201 419084
pdes 5 4.0943445622221 419096
pdr 2 5.0106352940962555 419140
peer 3 4.605170185988092 419160
peernets 3 4.605170185988092 419188
pele 2 5.0106352940962555 419216
pelvis 2 5.0106352940962555 419236
penalization 2 5.0106352940962555 419256
penalize 2 5.0106352940962555 419276
penalized 4 4.31748811353631 419296
penalizes 4 4.31748811353631 419332
penalizing 2 5.0106352940962555 419368
penalty 9 3.506557897319982 419388
penn 2 5.0106352940962555 419464
pennington 2 5.0106352940962555 419484
people 10 3.4011973816621555 419504
pepper 2 5.0106352940962555 419588
peppers 2 5.0106352940962555 419608
per 7 3.7578723256008875 419628
per-frame 2 5.0106352940962555 419688
per-layer 2 5.0106352940962555 419708
per-output 2 5.0106352940962555 419728
per-position 2 5.0106352940962555 419748
per-token 2 5.0106352940962555 419768
perceive 2 5.0106352940962555 419788
perceived 2 5.0106352940962555 419808
percentage 2 5.0106352940962555 419828
perception 12 3.2188758248682006 419848
perceptual 10 3.4011973816621555 419948
perceptually 2 5.0106352940962555 420032
perfect 4 4.31748811353631 420052
perform 54 1.7147984280919266 420088
performance 226 0.2832474753839152 420524
performance.existing 2 5.0106352940962555 422336
performance.preprint 2 5.0106352940962555 422356
performance.this 2 5.0106352940962555 422376
performances 8 3.624340932976365 422396
performed 8 3.624340932976365 422464
performing 10 3.4011973816621555 422532
performs 28 2.371577964480997 422616
perhaps 6 3.912023005428146 422844
periodic 7 3.7578723256008875 422896
peripheral 2 5.0106352940962555 422956
permanent 2 5.0106352940962555 422976
permanently 2 5.0106352940962555 422996
permissive 2 5.0106352940962555 423016
permits 4 4.31748811353631 423036
permutation 4 4.31748811353631 423072
permutation-invariant 2 5.0106352940962555 423108
permutationinvariant 2 5.0106352940962555 423128
permutes 2 5.0106352940962555 423148
perplexity 4 4.31748811353631 423168
persistent 2 5.0106352940962555 423204
persists 4 4.31748811353631 423224
person 4 4.31748811353631 423260
personal 2 5.0106352940962555 423296
personalized 2 5.0106352940962555 423316
perspective 41 1.9902104079518932 423336
perspectives 6 3.912023005428146 423668
persuasive 2 5.0106352940962555 423720
perturbation 8 3.624340932976365 423740
perturbations 14 3.0647251450409425 423808
perturbations.to 2 5.0106352940962555 423924
perturbed 4 4.31748811353631 423944
perturbing 4 4.31748811353631 423980
pervasive 4 4.31748811353631 424016
pessimistic 2 5.0106352940962555 424052
peter 2 5.0106352940962555 424072
phase 23 2.5682882587270512 424092
phases 8 3.624340932976365 424280
phd 1 5.703782474656201 424348
phenomena 6 3.912023005428146 424360
phenomenon 10 3.4011973816621555 424412
phenomenon.a 2 5.0106352940962555 424496
phi 1 5.703782474656201 424516
photo-realistic 4 4.31748811353631 424528
physical 16 2.9311937524164198 424564
physically 2 5.0106352940962555 424696
physics 4 4.31748811353631 424716
physiological 2 5.0106352940962555 424752
piano 2 5.0106352940962555 424772
pib 2 5.0106352940962555 424792
picarones 2 5.0106352940962555 424812
picasso 2 5.0106352940962555 424832
picks 2 5.0106352940962555 424852
picle 2 5.0106352940962555 424872
pieces 2 5.0106352940962555 424892
piecewise 2 5.0106352940962555 424912
pieter 2 5.0106352940962555 424932
pilot 2 5.0106352940962555 424952
pipeline 10 3.4011973816621555 424972
pipelines 2 5.0106352940962555 425056
pitfalls 4 4.31748811353631 425076
pivotal 2 5.0106352940962555 425112
pixel 11 3.3058872018578307 425132
pixel-level 2 5.0106352940962555 425224
pixel-reconstruction 2 5.0106352940962555 425244
pixel-wise 4 4.31748811353631 425264
pixels 16 2.9311937524164198 425300
pixelwise 2 5.0106352940962555 425432
place 2 5.0106352940962555 425452
placement 2 5.0106352940962555 425472
placing 2 5.0106352940962555 425492
plain 4 4.31748811353631 425512
plan 12 3.2188758248682006 425548
planar 2 5.0106352940962555 425648
planning 22 2.6127400212978853 425668
planning.this 2 5.0106352940962555 425848
plans 4 4.31748811353631 425868
plasma 2 5.0106352940962555 425904
plasmas 2 5.0106352940962555 425924
platforms 4 4.31748811353631 425944
plausibility 2 5.0106352940962555 425980
plausible 2 5.0106352940962555 426000
play 13 3.138833117194664 426020
players 2 5.0106352940962555 426128
playing 4 4.31748811353631 426148
plays 6 3.912023005428146 426184
please 2 5.0106352940962555 426236
plm 2 5.0106352940962555 426256
plt 1 5.703782474656201 426276
plt.plot 2 5.0106352940962555 426288
plt.show 2 5.0106352940962555 426308
plug 2 5.0106352940962555 426328
plug-and-play 6 3.912023005428146 426348
plug-in 4 4.31748811353631 426400
plugins 2 5.0106352940962555 426436
plus 2 5.0106352940962555 426456
poincar 2 5.0106352940962555 426476
point 39 2.0402208285265546 426496
point-of-view 2 5.0106352940962555 426812
point-voxel 2 5.0106352940962555 426832
point_ 2 5.0106352940962555 426852
pointed 2 5.0106352940962555 426872
points 42 1.9661128563728327 426892
pointwise 2 5.0106352940962555 427232
poisoned 4 4.31748811353631 427252
poisoning 5 4.0943445622221 427288
poisson 2 5.0106352940962555 427332
policies 31 2.2697952701710546 427352
policies.we 2 5.0106352940962555 427604
policy 54 1.7147984280919266 427624
policy-based 2 5.0106352940962555 428060
policy-regularized 2 5.0106352940962555 428080
policy.the 2 5.0106352940962555 428100
poly 2 5.0106352940962555 428120
polymatrix 2 5.0106352940962555 428140
polynomial 6 3.912023005428146 428160
polynomially 4 4.31748811353631 428212
polynomially.2we 2 5.0106352940962555 428248
polynomials 2 5.0106352940962555 428268
pomdp 2 5.0106352940962555 428288
pomdps 3 4.605170185988092 428308
pomdps-a 2 5.0106352940962555 428336
pooling 2 5.0106352940962555 428356
poor 8 3.624340932976365 428376
poorer 2 5.0106352940962555 428444
poorly 14 3.0647251450409425 428464
popular 50 1.791759469228055 428580
popularity 10 3.4011973816621555 428984
population 6 3.912023005428146 429068
portability 2 5.0106352940962555 429120
portion 4 4.31748811353631 429140
portrait 2 5.0106352940962555 429176
pose 11 3.3058872018578307 429196
poses 8 3.624340932976365 429288
posing 2 5.0106352940962555 429356
posit 2 5.0106352940962555 429376
position 6 3.912023005428146 429396
positional 4 4.31748811353631 429448
positioned 2 5.0106352940962555 429484
positive 6 3.912023005428146 429504
positive-definite 2 5.0106352940962555 429556
positively 2 5.0106352940962555 429576
possess 4 4.31748811353631 429596
possibilities 2 5.0106352940962555 429632
possibility 6 3.912023005428146 429652
possible 54 1.7147984280919266 429704
possibly 4 4.31748811353631 430140
post 4 4.31748811353631 430176
post-hoc 5 4.0943445622221 430212
post-hoc-cbm 2 5.0106352940962555 430256
post-processing 4 4.31748811353631 430276
posted 2 5.0106352940962555 430312
posterior 31 2.2697952701710546 430332
posteriors 4 4.31748811353631 430584
potential 45 1.8971199848858813 430620
potential.in 2 5.0106352940962555 430984
potentially 14 3.0647251450409425 431004
power 37 2.0928645620119766 431120
power-law 2 5.0106352940962555 431420
powered 2 5.0106352940962555 431440
powerful 40 2.0149030205422647 431460
pp 1 5.703782474656201 431784
ppgn 2 5.0106352940962555 431796
pptrick 2 5.0106352940962555 431816
practical 40 2.0149030205422647 431836
practicality 4 4.31748811353631 432160
practically-relevant 2 5.0106352940962555 432196
practice 40 2.0149030205422647 432216
practice.the 2 5.0106352940962555 432540
practices 2 5.0106352940962555 432560
practitioners 6 3.912023005428146 432580
pragmatic 2 5.0106352940962555 432632
pre-collected 2 5.0106352940962555 432652
pre-computed 2 5.0106352940962555 432672
pre-specifying 2 5.0106352940962555 432692
pre-trained 44 1.91959284073794 432712
pre-training 16 2.9311937524164198 433068
pre-training.to 2 5.0106352940962555 433200
precise 8 3.624340932976365 433220
precisely 2 5.0106352940962555 433288
precision 8 3.624340932976365 433308
preclusion 2 5.0106352940962555 433376
predefined 6 3.912023005428146 433396
predict 20 2.70805020110221 433448
predictability 2 5.0106352940962555 433612
predictable 4 4.31748811353631 433632
predicted 2 5.0106352940962555 433668
predicting 15 2.995732273553991 433688
prediction 64 1.5448993912965292 433812
prediction-entropy 2 5.0106352940962555 434328
prediction.repobench 2 5.0106352940962555 434348
prediction.the 2 5.0106352940962555 434368
predictions 30 2.302585092994046 434388
predictions.our 2 5.0106352940962555 434632
predictive 26 2.445685936634719 434652
predictor 7 3.7578723256008875 434864
predictors 5 4.0943445622221 434924
predicts 8 3.624340932976365 434968
predominantly 8 3.624340932976365 435036
predominately 2 5.0106352940962555 435104
prefer 4 4.31748811353631 435124
preference 5 4.0943445622221 435160
preferences 8 3.624340932976365 435204
prefers 2 5.0106352940962555 435272
prefix 4 4.31748811353631 435292
preliminary 6 3.912023005428146 435328
premature 2 5.0106352940962555 435380
prepint 2 5.0106352940962555 435400
preprint 42 1.9661128563728327 435420
prescient 2 5.0106352940962555 435760
presence 6 3.912023005428146 435780
present 100 1.0986122886681098 435832
presented 10 3.4011973816621555 436636
presenting 2 5.0106352940962555 436720
presents 18 2.8134107167600364 436740
preservation 2 5.0106352940962555 436888
preserve 8 3.624340932976365 436908
preserves 2 5.0106352940962555 436976
preserving 8 3.624340932976365 436996
presumably 2 5.0106352940962555 437064
pretrain 2 5.0106352940962555 437084
pretrained 21 2.659260036932778 437104
pretraining 21 2.659260036932778 437276
prevailing 4 4.31748811353631 437448
prevalence 2 5.0106352940962555 437484
prevalent 6 3.912023005428146 437504
prevalently 2 5.0106352940962555 437556
prevent 2 5.0106352940962555 437576
preventing 2 5.0106352940962555 437596
prevention 2 5.0106352940962555 437616
prevents 4 4.31748811353631 437636
previous 74 1.3997173814520314 437672
previously 24 2.5257286443082556 438268
primarily 16 2.9311937524164198 438464
primary 6 3.912023005428146 438596
primitives 2 5.0106352940962555 438648
prince 2 5.0106352940962555 438668
princeton 2 5.0106352940962555 438688
princeton.edu 2 5.0106352940962555 438708
principal 5 4.0943445622221 438728
principally 4 4.31748811353631 438772
principle 10 3.4011973816621555 438808
principled 17 2.870569130599985 438892
principles 10 3.4011973816621555 439032
prior 80 1.3217558399823195 439116
prior-informed 2 5.0106352940962555 439760
prior-likelihood 2 5.0106352940962555 439780
priori 2 5.0106352940962555 439800
prioritizes 2 5.0106352940962555 439820
priors 23 2.5682882587270512 439840
privacy 11 3.3058872018578307 440028
privacy-inducing 2 5.0106352940962555 440120
privacy-preserving 4 4.31748811353631 440140
private 12 3.2188758248682006 440176
privatize 2 5.0106352940962555 440276
privatizing 2 5.0106352940962555 440296
prize 2 5.0106352940962555 440316
probabilistic 25 2.4849066497880004 440336
probabilities 8 3.624340932976365 440540
probability 36 2.120263536200091 440608
probability.however 2 5.0106352940962555 440900
probable 2 5.0106352940962555 440920
probe 2 5.0106352940962555 440940
probes 2 5.0106352940962555 440960
probing 2 5.0106352940962555 440980
problem 164 0.6039160468320027 441000
problem-dependent 2 5.0106352940962555 442316
problem-specific 2 5.0106352940962555 442336
problem.adaptivity 2 5.0106352940962555 442356
problem.in 2 5.0106352940962555 442376
problematic 2 5.0106352940962555 442396
problems 74 1.3997173814520314 442416
problems.moreover 2 5.0106352940962555 443012
procedural 2 5.0106352940962555 443032
procedure 30 2.302585092994046 443052
procedures 4 4.31748811353631 443296
process 80 1.3217558399823195 443332
process.however 2 5.0106352940962555 443976
process.introductiondeep 2 5.0106352940962555 443996
process.previous 2 5.0106352940962555 444016
process.these 2 5.0106352940962555 444036
process.unlike 2 5.0106352940962555 444056
processed 6 3.912023005428146 444076
processes 15 2.995732273553991 444128
processing 36 2.120263536200091 444252
produce 28 2.371577964480997 444544
produced 10 3.4011973816621555 444772
produces 10 3.4011973816621555 444856
producing 4 4.31748811353631 444940
product 16 2.9311937524164198 444976
productivity 2 5.0106352940962555 445108
profiling 2 5.0106352940962555 445128
program 16 2.9311937524164198 445148
programming 16 2.9311937524164198 445280
programs 8 3.624340932976365 445412
programs.probabilistic 2 5.0106352940962555 445480
progress 22 2.6127400212978853 445500
progresses 2 5.0106352940962555 445680
progressive 2 5.0106352940962555 445700
progressively 4 4.31748811353631 445720
prohibitive 4 4.31748811353631 445756
prohibitively 6 3.912023005428146 445792
prohibits 4 4.31748811353631 445844
proj 2 5.0106352940962555 445880
project 14 3.0647251450409425 445900
projected 2 5.0106352940962555 446016
projecting 4 4.31748811353631 446036
projection 8 3.624340932976365 446072
projection-free 2 5.0106352940962555 446140
projections 8 3.624340932976365 446160
projects 2 5.0106352940962555 446228
proliferate 2 5.0106352940962555 446248
prominent 2 5.0106352940962555 446268
promise 8 3.624340932976365 446288
promises 2 5.0106352940962555 446356
promising 28 2.371577964480997 446376
promoted 2 5.0106352940962555 446604
promotes 2 5.0106352940962555 446624
promoting 2 5.0106352940962555 446644
prompt 20 2.70805020110221 446664
prompt-based 5 4.0943445622221 446828
promptattack 2 5.0106352940962555 446872
prompted 6 3.912023005428146 446892
prompting 13 3.138833117194664 446944
prompts 16 2.9311937524164198 447052
prompttts 3 4.605170185988092 447184
prone 4 4.31748811353631 447212
pronounced 4 4.31748811353631 447248
proof 2 5.0106352940962555 447284
proofs 6 3.912023005428146 447304
prop-a 2 5.0106352940962555 447356
propagate 8 3.624340932976365 447376
propagating 6 3.912023005428146 447444
propagation 16 2.9311937524164198 447496
proper 10 3.4011973816621555 447628
properly 10 3.4011973816621555 447712
properties 44 1.91959284073794 447796
properties-captured 2 5.0106352940962555 448152
properties.we 2 5.0106352940962555 448172
property 16 2.9311937524164198 448192
proportion 2 5.0106352940962555 448324
proportional 2 5.0106352940962555 448344
proposal 4 4.31748811353631 448364
proposals 2 5.0106352940962555 448400
propose 256 0.15860503017663857 448420
proposed 78 1.3470736479666094 450472
proposes 18 2.8134107167600364 451100
proposing 8 3.624340932976365 451248
protect 4 4.31748811353631 451316
protected 2 5.0106352940962555 451352
protecting 2 5.0106352940962555 451372
protein 5 4.0943445622221 451392
proteinmpnn 2 5.0106352940962555 451436
protocols 2 5.0106352940962555 451456
provable 11 3.3058872018578307 451476
provably 36 2.120263536200091 451568
prove 50 1.791759469228055 451860
proved 2 5.0106352940962555 452264
proven 10 3.4011973816621555 452284
provide 100 1.0986122886681098 452368
provided 10 3.4011973816621555 453172
provides 42 1.9661128563728327 453256
providing 20 2.70805020110221 453596
proving 9 3.506557897319982 453760
proving.by 2 5.0106352940962555 453836
provision 2 5.0106352940962555 453856
prowess 2 5.0106352940962555 453876
proximal 9 3.506557897319982 453896
proximal.this 2 5.0106352940962555 453972
proxy 8 3.624340932976365 453992
prqhb 2 5.0106352940962555 454060
prune 7 3.7578723256008875 454080
pruned 8 3.624340932976365 454140
prunes 4 4.31748811353631 454208
pruning 18 2.8134107167600364 454244
pruning.more 2 5.0106352940962555 454392
pseudo 2 5.0106352940962555 454412
pseudo-label 4 4.31748811353631 454432
pseudo-labels 2 5.0106352940962555 454468
pseudo-reward 2 5.0106352940962555 454488
pseudo-rewards 2 5.0106352940962555 454508
pseudorewards 2 5.0106352940962555 454528
psychophysical 2 5.0106352940962555 454548
public 10 3.4011973816621555 454568
publicly 18 2.8134107167600364 454652
published 12 3.2188758248682006 454800
pull 2 5.0106352940962555 454900
pure 4 4.31748811353631 454920
purely 12 3.2188758248682006 454956
purifies 2 5.0106352940962555 455056
purpose 10 3.4011973816621555 455076
purposes 2 5.0106352940962555 455160
pursuit 5 4.0943445622221 455180
push 2 5.0106352940962555 455224
pushes 2 5.0106352940962555 455244
pushing 2 5.0106352940962555 455264
put 4 4.31748811353631 455284
puts 2 5.0106352940962555 455320
pythia 2 5.0106352940962555 455340
python 6 3.912023005428146 455360
q 1 5.703782474656201 455412
q-function 4 4.31748811353631 455424
q-learning 10 3.4011973816621555 455460
q-value 2 5.0106352940962555 455544
q-value.model-based 2 5.0106352940962555 455564
q-values 4 4.31748811353631 455584
qa 2 5.0106352940962555 455620
qhhg 2 5.0106352940962555 455640
qian 2 5.0106352940962555 455660
qlearning 2 5.0106352940962555 455680
qre 2 5.0106352940962555 455700
qrnns 2 5.0106352940962555 455720
qsr 2 5.0106352940962555 455740
quadratic 16 2.9311937524164198 455760
quadratic-activation 2 5.0106352940962555 455892
quadratic-time 2 5.0106352940962555 455912
quadratics 2 5.0106352940962555 455932
qualitative 4 4.31748811353631 455952
qualitatively 10 3.4011973816621555 455988
qualities 2 5.0106352940962555 456072
quality 50 1.791759469228055 456092
quality-diversity 2 5.0106352940962555 456496
quality-only 2 5.0106352940962555 456516
quant 2 5.0106352940962555 456536
quant-ph 2 5.0106352940962555 456556
quantal 2 5.0106352940962555 456576
quantification 2 5.0106352940962555 456596
quantified 2 5.0106352940962555 456616
quantifies 4 4.31748811353631 456636
quantify 10 3.4011973816621555 456672
quantifying 2 5.0106352940962555 456756
quantile 2 5.0106352940962555 456776
quantitative 8 3.624340932976365 456796
quantitatively 4 4.31748811353631 456864
quantities 6 3.912023005428146 456900
quantity 4 4.31748811353631 456952
quantization 11 3.3058872018578307 456988
quantize 2 5.0106352940962555 457080
quantized 8 3.624340932976365 457100
quantum 7 3.7578723256008875 457168
quasi-recurrent 3 4.605170185988092 457228
quasiinvariance 2 5.0106352940962555 457256
qubits 2 5.0106352940962555 457276
qubo 2 5.0106352940962555 457296
qubos 2 5.0106352940962555 457316
queried 2 5.0106352940962555 457336
queries 12 3.2188758248682006 457356
queries.our 2 5.0106352940962555 457456
query 12 3.2188758248682006 457476
querying 4 4.31748811353631 457576
quest 2 5.0106352940962555 457612
question 38 2.0661963149298153 457632
question-relation 2 5.0106352940962555 457940
question.learned 2 5.0106352940962555 457960
question.the 2 5.0106352940962555 457980
questionable.we 2 5.0106352940962555 458000
questioning 2 5.0106352940962555 458020
questionrelation 2 5.0106352940962555 458040
questions 19 2.7593434954897607 458060
questionsthe 2 5.0106352940962555 458216
quickly 10 3.4011973816621555 458236
quilting 2 5.0106352940962555 458320
quite 4 4.31748811353631 458340
quoc 2 5.0106352940962555 458376
r 6 3.912023005428146 458396
race 2 5.0106352940962555 458448
rach0012 2 5.0106352940962555 458468
rachev 2 5.0106352940962555 458488
radford 2 5.0106352940962555 458508
radiance 2 5.0106352940962555 458528
radius 2 5.0106352940962555 458548
radon 2 5.0106352940962555 458568
rae 2 5.0106352940962555 458588
raffel 4 4.31748811353631 458608
raghav 2 5.0106352940962555 458644
raghu 2 5.0106352940962555 458664
ragonesi 2 5.0106352940962555 458684
raihan 2 5.0106352940962555 458704
rain 2 5.0106352940962555 458724
raised 2 5.0106352940962555 458744
raises 8 3.624340932976365 458764
rajbhandari 2 5.0106352940962555 458832
rakhlin 2 5.0106352940962555 458852
rambo 2 5.0106352940962555 458872
ramesh 2 5.0106352940962555 458892
random 39 2.0402208285265546 458912
randomization 4 4.31748811353631 459228
randomized 4 4.31748811353631 459264
randomizing 2 5.0106352940962555 459300
randomly 8 3.624340932976365 459320
randomly-selected 2 5.0106352940962555 459388
randomness 4 4.31748811353631 459408
randomness.combining 2 5.0106352940962555 459444
randomness.our 2 5.0106352940962555 459464
range 36 2.120263536200091 459484
range.this 2 5.0106352940962555 459776
ranging 4 4.31748811353631 459796
rank 2 5.0106352940962555 459832
ranks 2 5.0106352940962555 459852
rapid 2 5.0106352940962555 459872
rapidly 6 3.912023005428146 459892
rare 2 5.0106352940962555 459944
rarely 6 3.912023005428146 459964
rate 50 1.791759469228055 460016
rates 16 2.9311937524164198 460420
rather 32 2.2380465718564744 460552
rating 2 5.0106352940962555 460812
ratings 2 5.0106352940962555 460832
ratio 10 3.4011973816621555 460852
rationality 2 5.0106352940962555 460936
ratios 5 4.0943445622221 460956
raw 5 4.0943445622221 461000
raw-waveform 2 5.0106352940962555 461044
rdm 2 5.0106352940962555 461064
re 1 5.703782474656201 461084
re-design 2 5.0106352940962555 461096
re-evaluate 2 5.0106352940962555 461116
re-expressing 2 5.0106352940962555 461136
re-identification 2 5.0106352940962555 461156
re-imagen 3 4.605170185988092 461176
re-plan 2 5.0106352940962555 461204
re-trained 4 4.31748811353631 461224
re-training 4 4.31748811353631 461260
re-weighting 2 5.0106352940962555 461296
re-writing 2 5.0106352940962555 461316
reach 14 3.0647251450409425 461336
reachability 2 5.0106352940962555 461452
reachable 3 4.605170185988092 461472
reached 4 4.31748811353631 461500
reaches 4 4.31748811353631 461536
reaching 2 5.0106352940962555 461572
reaction-level 2 5.0106352940962555 461592
reactions 2 5.0106352940962555 461612
read 4 4.31748811353631 461632
read.the 2 5.0106352940962555 461668
readily 2 5.0106352940962555 461688
reads 2 5.0106352940962555 461708
real 44 1.91959284073794 461728
real-life 2 5.0106352940962555 462084
real-scanned 2 5.0106352940962555 462104
real-time 8 3.624340932976365 462124
real-world 56 1.6784307839210517 462192
realigns 2 5.0106352940962555 462644
realism 2 5.0106352940962555 462664
realism.by 2 5.0106352940962555 462684
realistic 22 2.6127400212978853 462704
realizability 2 5.0106352940962555 462884
realization 4 4.31748811353631 462904
realize 2 5.0106352940962555 462940
realized 2 5.0106352940962555 462960
really 2 5.0106352940962555 462980
realm 2 5.0106352940962555 463000
rearrangement 2 5.0106352940962555 463020
reason 18 2.8134107167600364 463040
reasonable 2 5.0106352940962555 463188
reasonably 2 5.0106352940962555 463208
reasoners 2 5.0106352940962555 463228
reasoning 36 2.120263536200091 463248
reasoning-oriented 2 5.0106352940962555 463540
reasoning.for 2 5.0106352940962555 463560
reasoning.our 2 5.0106352940962555 463580
reasons 6 3.912023005428146 463600
reassure 2 5.0106352940962555 463652
recall 2 5.0106352940962555 463672
recall-prioritization 2 5.0106352940962555 463692
recalling 2 5.0106352940962555 463712
recasting 2 5.0106352940962555 463732
recasts 2 5.0106352940962555 463752
receive 4 4.31748811353631 463772
received 2 5.0106352940962555 463808
receives 2 5.0106352940962555 463828
recent 106 1.0403433805441338 463848
recently 64 1.5448993912965292 464700
receptive 4 4.31748811353631 465216
recipewe 2 5.0106352940962555 465252
recognition 32 2.2380465718564744 465272
recognize 4 4.31748811353631 465532
recommendation 2 5.0106352940962555 465568
recommendations 4 4.31748811353631 465588
recommender 4 4.31748811353631 465624
recommends 2 5.0106352940962555 465660
reconstruct 10 3.4011973816621555 465680
reconstructing 4 4.31748811353631 465764
reconstruction 12 3.2188758248682006 465800
reconstructions 2 5.0106352940962555 465900
record 2 5.0106352940962555 465920
records 2 5.0106352940962555 465940
recourse 5 4.0943445622221 465960
recover 24 2.5257286443082556 466004
recovered 4 4.31748811353631 466200
recovering 4 4.31748811353631 466236
recovers 8 3.624340932976365 466272
recovery 4 4.31748811353631 466340
rectification 3 4.605170185988092 466376
rectified 4 4.31748811353631 466404
recur 2 5.0106352940962555 466440
recurrent 25 2.4849066497880004 466460
recurring 2 5.0106352940962555 466664
recursive 2 5.0106352940962555 466684
recursively 2 5.0106352940962555 466704
red 7 3.7578723256008875 466724
reddi 2 5.0106352940962555 466784
redesigning 2 5.0106352940962555 466804
redness 2 5.0106352940962555 466824
reduce 30 2.302585092994046 466844
reduced 20 2.70805020110221 467088
reduces 12 3.2188758248682006 467252
reducing 32 2.2380465718564744 467352
reduction 17 2.870569130599985 467612
redundant 2 5.0106352940962555 467752
reencoding 2 5.0106352940962555 467772
reevaluate 2 5.0106352940962555 467792
reevaluation 2 5.0106352940962555 467812
refer 16 2.9311937524164198 467832
reference 9 3.506557897319982 467964
references 6 3.912023005428146 468040
referent 2 5.0106352940962555 468092
referential 3 4.605170185988092 468112
referents 2 5.0106352940962555 468140
referred 6 3.912023005428146 468160
referring 2 5.0106352940962555 468212
refers 4 4.31748811353631 468232
refine 2 5.0106352940962555 468268
refinement 6 3.912023005428146 468288
refines 2 5.0106352940962555 468340
reflect 6 3.912023005428146 468360
reflected 2 5.0106352940962555 468412
reflects 2 5.0106352940962555 468432
reformulate 2 5.0106352940962555 468452
reformulating 2 5.0106352940962555 468472
regard 6 3.912023005428146 468492
regarding 14 3.0647251450409425 468544
regardless 4 4.31748811353631 468660
regime 19 2.7593434954897607 468696
regimes 6 3.912023005428146 468852
regina 2 5.0106352940962555 468904
region 6 3.912023005428146 468924
regions 6 3.912023005428146 468976
regions.we 2 5.0106352940962555 469028
regression 12 3.2188758248682006 469048
regression.we 2 5.0106352940962555 469148
regret 14 3.0647251450409425 469168
regret.efficient 2 5.0106352940962555 469284
regrets 2 5.0106352940962555 469304
regular 4 4.31748811353631 469324
regularisation 2 5.0106352940962555 469360
regularised 2 5.0106352940962555 469380
regularization 28 2.371577964480997 469400
regularize 4 4.31748811353631 469628
regularizer 10 3.4011973816621555 469664
regularizing 6 3.912023005428146 469748
reinforce 2 5.0106352940962555 469800
reinforcement 97 1.1290714961528183 469820
reinforcement-learning 4 4.31748811353631 470600
reiterates 2 5.0106352940962555 470636
reject 3 4.605170185988092 470656
rejection 2 5.0106352940962555 470684
relate 8 3.624340932976365 470704
related 16 2.9311937524164198 470772
relatedly 2 5.0106352940962555 470904
relatedness 2 5.0106352940962555 470924
relating 4 4.31748811353631 470944
relation 6 3.912023005428146 470980
relational 6 3.912023005428146 471032
relations 6 3.912023005428146 471084
relationship 8 3.624340932976365 471136
relationships 8 3.624340932976365 471204
relative 10 3.4011973816621555 471272
relatively 10 3.4011973816621555 471356
relax 6 3.912023005428146 471440
relaxation 4 4.31748811353631 471492
relaxed 4 4.31748811353631 471528
relaxes 2 5.0106352940962555 471564
relay 3 4.605170185988092 471584
relaydiffusion 2 5.0106352940962555 471612
release 6 3.912023005428146 471632
released 12 3.2188758248682006 471684
relevance 8 3.624340932976365 471784
relevant 20 2.70805020110221 471852
reliability 2 5.0106352940962555 472016
reliable 2 5.0106352940962555 472036
reliably 6 3.912023005428146 472056
reliance 6 3.912023005428146 472108
reliant 4 4.31748811353631 472160
relied 2 5.0106352940962555 472196
relies 8 3.624340932976365 472216
relu 25 2.4849066497880004 472284
relus 2 5.0106352940962555 472488
rely 36 2.120263536200091 472508
relying 8 3.624340932976365 472800
remain 6 3.912023005428146 472868
remaining 4 4.31748811353631 472920
remains 34 2.17742195004004 472956
remarkable 16 2.9311937524164198 473232
remarkably 8 3.624340932976365 473364
remarks 2 5.0106352940962555 473432
remedy 4 4.31748811353631 473452
remember 2 5.0106352940962555 473488
remembering 2 5.0106352940962555 473508
reml-lab 2 5.0106352940962555 473528
removal 6 3.912023005428146 473548
remove 2 5.0106352940962555 473600
removed 2 5.0106352940962555 473620
removes 4 4.31748811353631 473640
removing 10 3.4011973816621555 473676
render 4 4.31748811353631 473760
rendering 2 5.0106352940962555 473796
renowned 2 5.0106352940962555 473816
ren 2 5.0106352940962555 473836
repair 5 4.0943445622221 473856
repaired 2 5.0106352940962555 473900
repairing 2 5.0106352940962555 473920
repairs 2 5.0106352940962555 473940
reparameterization 2 5.0106352940962555 473960
reparameterized 2 5.0106352940962555 473980
reparametrize 2 5.0106352940962555 474000
repeatedly 4 4.31748811353631 474020
repeating 6 3.912023005428146 474056
repeats 2 5.0106352940962555 474108
repetition 2 5.0106352940962555 474128
rephrased 2 5.0106352940962555 474148
replace 2 5.0106352940962555 474168
replaced 2 5.0106352940962555 474188
replacement 2 5.0106352940962555 474208
replacing 12 3.2188758248682006 474228
replay 5 4.0943445622221 474328
replicate 2 5.0106352940962555 474372
repobench 3 4.605170185988092 474392
repobench-c 2 5.0106352940962555 474420
repobench-p 2 5.0106352940962555 474440
repobench-r 2 5.0106352940962555 474460
report 8 3.624340932976365 474480
reported 2 5.0106352940962555 474548
reports 2 5.0106352940962555 474568
repositories 2 5.0106352940962555 474588
repository-level 3 4.605170185988092 474608
represent 30 2.302585092994046 474636
representation 82 1.297063227391948 474880
representation-building 2 5.0106352940962555 475540
representational 4 4.31748811353631 475560
representations 97 1.1290714961528183 475596
representations.consider 2 5.0106352940962555 476376
representations.ultra 2 5.0106352940962555 476396
representative 16 2.9311937524164198 476416
represented 10 3.4011973816621555 476548
representing 18 2.8134107167600364 476632
represents 18 2.8134107167600364 476780
reproduce 2 5.0106352940962555 476928
reproducible 2 5.0106352940962555 476948
repurpose 2 5.0106352940962555 476968
require 68 1.4842747694800944 476988
required 28 2.371577964480997 477536
requirement 2 5.0106352940962555 477764
requirements 14 3.0647251450409425 477784
requires 50 1.791759469228055 477900
requiring 20 2.70805020110221 478304
rescaled 4 4.31748811353631 478468
rescaling 2 5.0106352940962555 478504
rescue 2 5.0106352940962555 478524
research 60 1.6094379124341003 478544
researchers 10 3.4011973816621555 479028
resemble 2 5.0106352940962555 479112
resets 2 5.0106352940962555 479132
residency 2 5.0106352940962555 479152
resides 2 5.0106352940962555 479172
residual 11 3.3058872018578307 479192
residues 2 5.0106352940962555 479284
resilience 2 5.0106352940962555 479304
resiliency 2 5.0106352940962555 479324
resnet 4 4.31748811353631 479344
resnet-20 2 5.0106352940962555 479380
resnet-50 4 4.31748811353631 479400
resnet18 2 5.0106352940962555 479436
resnets 6 3.912023005428146 479456
resolution 12 3.2188758248682006 479508
resolution-dependent 2 5.0106352940962555 479608
resolutions 5 4.0943445622221 479628
resolve 4 4.31748811353631 479672
resolved 2 5.0106352940962555 479708
resolves 2 5.0106352940962555 479728
resort 4 4.31748811353631 479748
resorting 2 5.0106352940962555 479784
resource-constrained 4 4.31748811353631 479804
resources 8 3.624340932976365 479840
resourceshenighan 2 5.0106352940962555 479908
respect 12 3.2188758248682006 479928
respected 2 5.0106352940962555 480028
respective 8 3.624340932976365 480048
respectively 12 3.2188758248682006 480116
respectively.besides 2 5.0106352940962555 480216
respectively.this 2 5.0106352940962555 480236
respects 4 4.31748811353631 480256
respond 2 5.0106352940962555 480292
response 4 4.31748811353631 480312
responses 4 4.31748811353631 480348
rest 4 4.31748811353631 480384
restarting 2 5.0106352940962555 480420
restoration 3 4.605170185988092 480440
restoring 2 5.0106352940962555 480468
restrict 4 4.31748811353631 480488
restricted 4 4.31748811353631 480524
restricting 2 5.0106352940962555 480560
restriction 4 4.31748811353631 480580
restrictive 4 4.31748811353631 480616
result 48 1.8325814637483102 480652
resulting 40 2.0149030205422647 481040
results 180 0.5108256237659907 481364
results.in 2 5.0106352940962555 482808
retain 6 3.912023005428146 482828
retaining 4 4.31748811353631 482880
rethinking 2 5.0106352940962555 482916
retrain 2 5.0106352940962555 482936
retraining 12 3.2188758248682006 482956
retrieval 25 2.4849066497880004 483056
retrieval-and 2 5.0106352940962555 483260
retrieval-augmented 5 4.0943445622221 483280
retrieve 6 3.912023005428146 483324
retrieved 4 4.31748811353631 483376
retrieves 2 5.0106352940962555 483412
retrieving 2 5.0106352940962555 483432
retro 2 5.0106352940962555 483452
retro-fallback 3 4.605170185988092 483472
retrosynthesis 2 5.0106352940962555 483500
retrosynthetic 2 5.0106352940962555 483520
return 8 3.624340932976365 483540
returns 4 4.31748811353631 483608
reusable 2 5.0106352940962555 483644
reveal 16 2.9311937524164198 483664
revealed 2 5.0106352940962555 483796
revealing 10 3.4011973816621555 483816
reveals 12 3.2188758248682006 483900
reverse 2 5.0106352940962555 484000
review 48 1.8325814637483102 484020
review.figure 2 5.0106352940962555 484408
revisited 2 5.0106352940962555 484428
revisiting 2 5.0106352940962555 484448
reward 29 2.336486644669727 484468
reward-annotated 2 5.0106352940962555 484704
reward-consistent 3 4.605170185988092 484724
rewarded 2 5.0106352940962555 484752
rewarding 2 5.0106352940962555 484772
rewards 16 2.9311937524164198 484792
rewards.existing 2 5.0106352940962555 484924
rewards.the 2 5.0106352940962555 484944
rewind 2 5.0106352940962555 484964
rewinding 4 4.31748811353631 484984
rewire 2 5.0106352940962555 485020
rewiring 3 4.605170185988092 485040
rewound 2 5.0106352940962555 485068
rfnet 2 5.0106352940962555 485088
riccardo 2 5.0106352940962555 485108
rich 16 2.9311937524164198 485128
richer 2 5.0106352940962555 485260
ridge 4 4.31748811353631 485280
riemannian 10 3.4011973816621555 485316
right 8 3.624340932976365 485400
rigid 2 5.0106352940962555 485468
rigor 2 5.0106352940962555 485488
rigorous 6 3.912023005428146 485508
rigter 2 5.0106352940962555 485560
rise 8 3.624340932976365 485580
rising 2 5.0106352940962555 485648
risk 21 2.659260036932778 485668
risks 7 3.7578723256008875 485840
risks-such 2 5.0106352940962555 485900
risky 4 4.31748811353631 485920
rival 2 5.0106352940962555 485956
rl 26 2.445685936634719 485976
rl-based 2 5.0106352940962555 486188
rl.our 2 5.0106352940962555 486208
rl2 2 5.0106352940962555 486228
rlhf 3 4.605170185988092 486248
rmse 2 5.0106352940962555 486276
rmwggis 2 5.0106352940962555 486296
rnns 6 3.912023005428146 486316
roast 2 5.0106352940962555 486368
roberta 2 5.0106352940962555 486388
robo-adapters 2 5.0106352940962555 486408
robot 8 3.624340932976365 486428
robotic 10 3.4011973816621555 486496
robotics 8 3.624340932976365 486580
robots 6 3.912023005428146 486648
robust 51 1.7719568419318754 486700
robustfill 2 5.0106352940962555 487112
robustifying 2 5.0106352940962555 487132
robustly 4 4.31748811353631 487152
robustness 41 1.9902104079518932 487188
robustness.this 2 5.0106352940962555 487520
robustness.to 2 5.0106352940962555 487540
role 24 2.5257286443082556 487560
role.as 2 5.0106352940962555 487756
roles 2 5.0106352940962555 487776
rolled 2 5.0106352940962555 487796
rombach 2 5.0106352940962555 487816
rome 2 5.0106352940962555 487836
room 6 3.912023005428146 487856
root 6 3.912023005428146 487908
roots 2 5.0106352940962555 487960
rosca 2 5.0106352940962555 487980
rostami 2 5.0106352940962555 488000
rotary 2 5.0106352940962555 488020
rotated 2 5.0106352940962555 488040
rotation 8 3.624340932976365 488060
rotations 4 4.31748811353631 488128
rotmanmi 2 5.0106352940962555 488164
rouge 2 5.0106352940962555 488184
roughly 2 5.0106352940962555 488204
round 4 4.31748811353631 488224
rounds 4 4.31748811353631 488260
route 2 5.0106352940962555 488296
routine 2 5.0106352940962555 488316
routines 2 5.0106352940962555 488336
row 1 5.703782474656201 488356
rows 2 5.0106352940962555 488368
rps 2 5.0106352940962555 488388
rt 2 5.0106352940962555 488408
rtx 1 5.703782474656201 488428
rubin 2 5.0106352940962555 488440
ruggero 2 5.0106352940962555 488460
rule 3 4.605170185988092 488480
rule-based 2 5.0106352940962555 488508
rules 4 4.31748811353631 488528
run 2 5.0106352940962555 488564
run-time 2 5.0106352940962555 488584
runge-kutta 2 5.0106352940962555 488604
running 2 5.0106352940962555 488624
runtime 6 3.912023005428146 488644
russo 2 5.0106352940962555 488696
rust 2 5.0106352940962555 488716
rmi 2 5.0106352940962555 488736
rnyi 2 5.0106352940962555 488756
rschendorf 2 5.0106352940962555 488776
s 119 0.9246589815446716 488796
s. 1 5.703782474656201 489752
sacrificing 4 4.31748811353631 489764
saddle 2 5.0106352940962555 489800
safe 11 3.3058872018578307 489820
safedreamer 3 4.605170185988092 489912
safer 4 4.31748811353631 489940
saferl 2 5.0106352940962555 489976
safest 2 5.0106352940962555 489996
safety 16 2.9311937524164198 490016
safety-critical 2 5.0106352940962555 490148
safety-gymnasium 2 5.0106352940962555 490168
safran 2 5.0106352940962555 490188
saga 2 5.0106352940962555 490208
said 2 5.0106352940962555 490228
sales.these 2 5.0106352940962555 490248
salesforce 4 4.31748811353631 490268
salesforce.com 2 5.0106352940962555 490304
salg 2 5.0106352940962555 490324
salient 2 5.0106352940962555 490344
sam 2 5.0106352940962555 490364
same 72 1.4271163556401458 490384
sample 59 1.6262450307504817 490964
sample-efficiency 7 3.7578723256008875 491440
sample-efficient 7 3.7578723256008875 491500
sampled 21 2.659260036932778 491560
sampleefficiency.notably 2 5.0106352940962555 491732
sampler 6 3.912023005428146 491752
samples 55 1.69644928942373 491804
samples.however 2 5.0106352940962555 492248
samples.in 2 5.0106352940962555 492268
samples.our 2 5.0106352940962555 492288
sampling 61 1.5929086104828898 492308
sampling.experiments 2 5.0106352940962555 492800
samuli 2 5.0106352940962555 492820
san 4 4.31748811353631 492840
sandbox 2 5.0106352940962555 492876
sanjoy 2 5.0106352940962555 492896
sans 2 5.0106352940962555 492916
santambrogio 2 5.0106352940962555 492936
santhanam 2 5.0106352940962555 492956
santurkar 2 5.0106352940962555 492976
sapa 2 5.0106352940962555 492996
saraghazanfari 2 5.0106352940962555 493016
satisfaction 3 4.605170185988092 493036
satisfaction.motivated 2 5.0106352940962555 493064
satisfaction.we 2 5.0106352940962555 493084
satisfactory 2 5.0106352940962555 493104
satisfied 2 5.0106352940962555 493124
satisfies 4 4.31748811353631 493144
satisfy 6 3.912023005428146 493180
satisfying 8 3.624340932976365 493232
saturated 2 5.0106352940962555 493300
saturating 2 5.0106352940962555 493320
sauer 2 5.0106352940962555 493340
saurav 2 5.0106352940962555 493360
savarese 2 5.0106352940962555 493380
saved 2 5.0106352940962555 493400
savi 2 5.0106352940962555 493420
say 3 4.605170185988092 493440
scaffold 4 4.31748811353631 493468
scaffolds 2 5.0106352940962555 493504
scalability 14 3.0647251450409425 493524
scalable 24 2.5257286443082556 493640
scalar.given 2 5.0106352940962555 493836
scale 38 2.0661963149298153 493856
scale-insensitive 2 5.0106352940962555 494164
scale.modular 2 5.0106352940962555 494184
scaled 4 4.31748811353631 494204
scales 16 2.9311937524164198 494240
scaling 19 2.7593434954897607 494372
scanrefer 2 5.0106352940962555 494528
scarce 2 5.0106352940962555 494548
scarcity 2 5.0106352940962555 494568
scargle 2 5.0106352940962555 494588
scenario 14 3.0647251450409425 494608
scenarios 30 2.302585092994046 494724
scenarios.notably 2 5.0106352940962555 494968
scenarios.to 2 5.0106352940962555 494988
scene 14 3.0647251450409425 495008
scene-level 2 5.0106352940962555 495124
scenelevel 2 5.0106352940962555 495144
scenes 8 3.624340932976365 495164
schedule 4 4.31748811353631 495232
schedules 4 4.31748811353631 495268
schemata 2 5.0106352940962555 495304
scheme 20 2.70805020110221 495324
schemes 12 3.2188758248682006 495488
schemes.in 2 5.0106352940962555 495588
schmidt 2 5.0106352940962555 495608
schneider 2 5.0106352940962555 495628
school 2 5.0106352940962555 495648
science 2 5.0106352940962555 495668
sciences 2 5.0106352940962555 495688
scientific 2 5.0106352940962555 495708
scope 6 3.912023005428146 495728
score 19 2.7593434954897607 495780
score-based 2 5.0106352940962555 495936
score.our 2 5.0106352940962555 495956
scores 10 3.4011973816621555 495976
scraped 2 5.0106352940962555 496060
scratch 8 3.624340932976365 496080
scratch.this 2 5.0106352940962555 496148
scrna 2 5.0106352940962555 496168
sdes 5 4.0943445622221 496188
sds 2 5.0106352940962555 496232
seamless 4 4.31748811353631 496252
seamlessly 10 3.4011973816621555 496288
search 32 2.2380465718564744 496372
search-control 3 4.605170185988092 496632
search.furthermore 2 5.0106352940962555 496660
searchcontrol 2 5.0106352940962555 496680
searching 10 3.4011973816621555 496700
seasonal 2 5.0106352940962555 496784
sec 2 5.0106352940962555 496804
second 44 1.91959284073794 496824
second-order 5 4.0943445622221 497180
secondly 2 5.0106352940962555 497224
secret 6 3.912023005428146 497244
section 16 2.9311937524164198 497296
sections 2 5.0106352940962555 497428
secure 2 5.0106352940962555 497448
security 3 4.605170185988092 497468
see 10 3.4011973816621555 497496
seefig 2 5.0106352940962555 497580
seefigure 2 5.0106352940962555 497600
seeing 2 5.0106352940962555 497620
seek 4 4.31748811353631 497640
seeking 2 5.0106352940962555 497676
seeks 8 3.624340932976365 497696
seem 4 4.31748811353631 497764
seemingly 8 3.624340932976365 497800
seems 2 5.0106352940962555 497868
seen 24 2.5257286443082556 497888
seer 5 4.0943445622221 498084
seervideodiffusion.github.io 2 5.0106352940962555 498128
sees 2 5.0106352940962555 498148
seetable 2 5.0106352940962555 498168
segmentation 11 3.3058872018578307 498188
segmentation-based 2 5.0106352940962555 498280
select 4 4.31748811353631 498300
selected 6 3.912023005428146 498336
selecting 6 3.912023005428146 498388
selection 20 2.70805020110221 498440
selection-based 2 5.0106352940962555 498604
selection-inference 3 4.605170185988092 498624
selectively 2 5.0106352940962555 498652
selectors 2 5.0106352940962555 498672
selects 4 4.31748811353631 498692
self 4 4.31748811353631 498728
self-attention 4 4.31748811353631 498764
self-attentive 4 4.31748811353631 498800
self-concordant 2 5.0106352940962555 498836
self-consistency 3 4.605170185988092 498856
self-consistent 2 5.0106352940962555 498884
self-contrast 2 5.0106352940962555 498904
self-distillation 3 4.605170185988092 498924
self-instruct 2 5.0106352940962555 498952
self-similar 2 5.0106352940962555 498972
self-supervised 18 2.8134107167600364 498992
self-supervision 2 5.0106352940962555 499140
self-terminating 3 4.605170185988092 499160
self-training 3 4.605170185988092 499188
selfcipher 2 5.0106352940962555 499216
selfsupervised 2 5.0106352940962555 499236
semantic 25 2.4849066497880004 499256
semantically 4 4.31748811353631 499460
semantically-aware 2 5.0106352940962555 499496
semantically-identical 2 5.0106352940962555 499516
semantically-lossless 2 5.0106352940962555 499536
semantics 12 3.2188758248682006 499556
semantics-preserving 2 5.0106352940962555 499656
semi-algebraic 2 5.0106352940962555 499676
semi-parametric 2 5.0106352940962555 499696
semi-supervised 12 3.2188758248682006 499716
seminal 2 5.0106352940962555 499816
sending 2 5.0106352940962555 499836
sener 2 5.0106352940962555 499856
senior 4 4.31748811353631 499876
sense 12 3.2188758248682006 499912
sense.llms 2 5.0106352940962555 500012
sensetime 2 5.0106352940962555 500032
sensitive 15 2.995732273553991 500052
sensitivity 9 3.506557897319982 500176
sensitivity-based 2 5.0106352940962555 500252
sensor 4 4.31748811353631 500272
sensorimotor 2 5.0106352940962555 500308
sensory 2 5.0106352940962555 500328
sent 2 5.0106352940962555 500348
sentence 7 3.7578723256008875 500368
sentences 2 5.0106352940962555 500428
sentences.this 2 5.0106352940962555 500448
sentiment 4 4.31748811353631 500468
sep 4 4.31748811353631 500504
separability 2 5.0106352940962555 500540
separate 8 3.624340932976365 500560
separated 6 3.912023005428146 500628
separately 6 3.912023005428146 500680
separately.noise 2 5.0106352940962555 500732
separating 2 5.0106352940962555 500752
separation 7 3.7578723256008875 500772
separation-why 2 5.0106352940962555 500832
seq2seq 4 4.31748811353631 500852
sequence 36 2.120263536200091 500888
sequence-level 2 5.0106352940962555 501180
sequence-to-sequence 2 5.0106352940962555 501200
sequence.published 2 5.0106352940962555 501220
sequences 22 2.6127400212978853 501240
sequences.in 2 5.0106352940962555 501420
sequential 10 3.4011973816621555 501440
sequentially 6 3.912023005428146 501524
serge 2 5.0106352940962555 501576
sergey 2 5.0106352940962555 501596
serial 2 5.0106352940962555 501616
series 34 2.17742195004004 501636
seriously 2 5.0106352940962555 501912
serve 16 2.9311937524164198 501932
server 4 4.31748811353631 502064
servergrade 2 5.0106352940962555 502100
serves 4 4.31748811353631 502120
serving 4 4.31748811353631 502156
sesame 2 5.0106352940962555 502192
set 53 1.7334905611040792 502212
sets 31 2.2697952701710546 502640
setting 48 1.8325814637483102 502892
setting-prompting 2 5.0106352940962555 503280
setting.published 2 5.0106352940962555 503300
setting.related 2 5.0106352940962555 503320
setting.the 2 5.0106352940962555 503340
settings 52 1.7525387560747736 503360
settings.in 2 5.0106352940962555 503780
settings.instead 2 5.0106352940962555 503800
settings.this 4 4.31748811353631 503820
settings.when 2 5.0106352940962555 503856
settingsgoli 2 5.0106352940962555 503876
setup 10 3.4011973816621555 503896
setup.we 2 5.0106352940962555 503980
setups 4 4.31748811353631 504000
several 78 1.3470736479666094 504036
severe 8 3.624340932976365 504664
severely 6 3.912023005428146 504732
sfda 4 4.31748811353631 504784
sfid 2 5.0106352940962555 504820
sg-mcmc 2 5.0106352940962555 504840
sgd 28 2.371577964480997 504860
sgd-type 2 5.0106352940962555 505088
shafiullah 2 5.0106352940962555 505108
shahid 2 5.0106352940962555 505128
shakir 2 5.0106352940962555 505148
shall 4 4.31748811353631 505168
shallow 4 4.31748811353631 505204
shallower 4 4.31748811353631 505240
sham 2 5.0106352940962555 505276
shamir 4 4.31748811353631 505296
shannon 2 5.0106352940962555 505332
shap 3 4.605170185988092 505352
shape 13 3.138833117194664 505380
shaped 4 4.31748811353631 505488
shapes 4 4.31748811353631 505524
sharath-girish 2 5.0106352940962555 505560
share 10 3.4011973816621555 505580
shared 14 3.0647251450409425 505664
sharing 4 4.31748811353631 505780
sharp 13 3.138833117194664 505816
sharpness 4 4.31748811353631 505924
sharpness-aware 6 3.912023005428146 505960
sheared 2 5.0106352940962555 506012
sheared-llama 2 5.0106352940962555 506032
shed 6 3.912023005428146 506052
shedding 2 5.0106352940962555 506104
sheds 4 4.31748811353631 506124
shells 2 5.0106352940962555 506160
shelve 2 5.0106352940962555 506180
sherman 2 5.0106352940962555 506200
shift 23 2.5682882587270512 506220
shifted 2 5.0106352940962555 506408
shifts 17 2.870569130599985 506428
shoeybi 2 5.0106352940962555 506568
shops 2 5.0106352940962555 506588
short 10 3.4011973816621555 506608
short-term 6 3.912023005428146 506692
short-text 2 5.0106352940962555 506744
shortcoming 2 5.0106352940962555 506764
shortcomings 6 3.912023005428146 506784
shortcomings.in 2 5.0106352940962555 506836
shorter 4 4.31748811353631 506856
shot 2 5.0106352940962555 506892
should 32 2.2380465718564744 506912
show 278 0.07616136096556395 507172
showcase 8 3.624340932976365 509400
showcasing 2 5.0106352940962555 509468
showed 2 5.0106352940962555 509488
showing 28 2.371577964480997 509508
showlab 2 5.0106352940962555 509736
shown 50 1.791759469228055 509756
shows 28 2.371577964480997 510160
shrinkage 2 5.0106352940962555 510388
si 2 5.0106352940962555 510408
siam 2 5.0106352940962555 510428
siamese 2 5.0106352940962555 510448
side 10 3.4011973816621555 510468
side-channel 2 5.0106352940962555 510552
sieve 2 5.0106352940962555 510572
sight 2 5.0106352940962555 510592
sign 2 5.0106352940962555 510612
signal 10 3.4011973816621555 510632
signal-finding 2 5.0106352940962555 510716
signal-heavy 2 5.0106352940962555 510736
signal-to-noise 2 5.0106352940962555 510756
signals 14 3.0647251450409425 510776
signals.at 2 5.0106352940962555 510892
significance 2 5.0106352940962555 510912
significant 46 1.875141078167106 510932
significantly 96 1.1394342831883648 511304
sim-to-real 2 5.0106352940962555 512076
similar 42 1.9661128563728327 512096
similarities 2 5.0106352940962555 512436
similarity 20 2.70805020110221 512456
similarly 10 3.4011973816621555 512620
simoncelli 2 5.0106352940962555 512704
simple 102 1.07880966137193 512724
simpler 6 3.912023005428146 513544
simplest 2 5.0106352940962555 513596
simplicity 14 3.0647251450409425 513616
simplification 2 5.0106352940962555 513732
simplified 4 4.31748811353631 513752
simplifies 2 5.0106352940962555 513788
simplify 2 5.0106352940962555 513808
simplifying 2 5.0106352940962555 513828
simply 22 2.6127400212978853 513848
simulated 12 3.2188758248682006 514028
simulating 2 5.0106352940962555 514128
simulation 15 2.995732273553991 514148
simulations 4 4.31748811353631 514272
simulations.from 2 5.0106352940962555 514308
simulator 2 5.0106352940962555 514328
simulators.we 2 5.0106352940962555 514348
simultaneous 4 4.31748811353631 514368
simultaneously 22 2.6127400212978853 514404
simultaneously.extensive 2 5.0106352940962555 514584
sin 3 4.605170185988092 514604
since 34 2.17742195004004 514632
singal 2 5.0106352940962555 514908
singer 2 5.0106352940962555 514928
single 46 1.875141078167106 514948
single-cell 2 5.0106352940962555 515320
single-file 2 5.0106352940962555 515340
single-image 2 5.0106352940962555 515360
single-layer 2 5.0106352940962555 515380
single-loop 4 4.31748811353631 515400
single-step 2 5.0106352940962555 515436
singleinput 2 5.0106352940962555 515456
singular 3 4.605170185988092 515476
sites 2 5.0106352940962555 515504
sites.google.com 4 4.31748811353631 515524
situation 2 5.0106352940962555 515560
six 3 4.605170185988092 515580
sizable 4 4.31748811353631 515608
size 43 1.9425823589626385 515644
sizes 18 2.8134107167600364 515992
skeleton 2 5.0106352940962555 516140
skepticism 2 5.0106352940962555 516160
sketch 3 4.605170185988092 516180
skewed 2 5.0106352940962555 516208
skewiou 2 5.0106352940962555 516228
skewness 2 5.0106352940962555 516248
skews 2 5.0106352940962555 516268
skill 5 4.0943445622221 516288
skills 4 4.31748811353631 516332
skin 2 5.0106352940962555 516368
skip 3 4.605170185988092 516388
sl 2 5.0106352940962555 516416
slac 2 5.0106352940962555 516436
slice 2 5.0106352940962555 516456
sliced 5 4.0943445622221 516476
slicing 4 4.31748811353631 516520
slightly 10 3.4011973816621555 516556
slot 2 5.0106352940962555 516640
sloth 2 5.0106352940962555 516660
slots 4 4.31748811353631 516680
slow 21 2.659260036932778 516716
slower 2 5.0106352940962555 516888
slowly-learned 2 5.0106352940962555 516908
small 73 1.41332303350781 516928
small-batch 2 5.0106352940962555 517516
small-data 2 5.0106352940962555 517536
small-dense 2 5.0106352940962555 517556
small-error 2 5.0106352940962555 517576
small-sample 2 5.0106352940962555 517596
small-size 2 5.0106352940962555 517616
small-to 2 5.0106352940962555 517636
smaller 30 2.302585092994046 517656
smallest 4 4.31748811353631 517900
smart 2 5.0106352940962555 517936
smcdiff 2 5.0106352940962555 517956
smooth 20 2.70805020110221 517976
smoothed 6 3.912023005428146 518140
smoother 4 4.31748811353631 518192
smoothing 8 3.624340932976365 518228
smoothly 2 5.0106352940962555 518296
smoothness 2 5.0106352940962555 518316
smt 1 5.703782474656201 518336
snip 2 5.0106352940962555 518348
snippets 2 5.0106352940962555 518368
snli 2 5.0106352940962555 518388
so 25 2.4849066497880004 518408
so-called 6 3.912023005428146 518612
social 4 4.31748811353631 518664
soft 7 3.7578723256008875 518700
soft_m 2 5.0106352940962555 518760
softmax 10 3.4011973816621555 518780
software 4 4.31748811353631 518864
solar 3 4.605170185988092 518900
solely 14 3.0647251450409425 518928
solofusion 2 5.0106352940962555 519044
solution 38 2.0661963149298153 519064
solutions 26 2.445685936634719 519372
solve 32 2.2380465718564744 519584
solved 6 3.912023005428146 519844
solved.prior 2 5.0106352940962555 519896
solver 8 3.624340932976365 519916
solvers 7 3.7578723256008875 519984
solvers.for 2 5.0106352940962555 520044
solvers.however 2 5.0106352940962555 520064
solves 2 5.0106352940962555 520084
solving 34 2.17742195004004 520104
some 78 1.3470736479666094 520380
something 6 3.912023005428146 521008
sometimes 4 4.31748811353631 521060
somewhat 2 5.0106352940962555 521096
somewhere 2 5.0106352940962555 521116
sonata 2 5.0106352940962555 521136
song 4 4.31748811353631 521156
sophisticated 2 5.0106352940962555 521192
sophistication 2 5.0106352940962555 521212
sorting.we 2 5.0106352940962555 521232
sota 8 3.624340932976365 521252
sotoudeh 2 5.0106352940962555 521320
soudry 2 5.0106352940962555 521340
sound 11 3.3058872018578307 521360
soundness.we 2 5.0106352940962555 521452
source 31 2.2697952701710546 521472
source-domain 4 4.31748811353631 521724
source-free 6 3.912023005428146 521760
source-to-target 2 5.0106352940962555 521812
sources 10 3.4011973816621555 521832
sources.knowledge 2 5.0106352940962555 521916
space 100 1.0986122886681098 521936
space.3 2 5.0106352940962555 522740
space.their 2 5.0106352940962555 522760
space.theoretically 2 5.0106352940962555 522780
space.therefore 2 5.0106352940962555 522800
spaces 27 2.407945608651872 522820
spade 2 5.0106352940962555 523040
span 2 5.0106352940962555 523060
spanning 2 5.0106352940962555 523080
sparse 41 1.9902104079518932 523100
sparse-former 2 5.0106352940962555 523432
sparse-reward 2 5.0106352940962555 523452
sparseformer 3 4.605170185988092 523472
sparser 4 4.31748811353631 523500
sparsification 2 5.0106352940962555 523536
sparsities 4 4.31748811353631 523556
sparsities-pairs 2 5.0106352940962555 523592
sparsity 26 2.445685936634719 523612
spatial 16 2.9311937524164198 523824
spatial-temporal 2 5.0106352940962555 523956
spatially-and 2 5.0106352940962555 523976
spatially-varying 2 5.0106352940962555 523996
special 18 2.8134107167600364 524016
specialized 5 4.0943445622221 524164
species 2 5.0106352940962555 524208
specific 40 2.0149030205422647 524228
specifically 48 1.8325814637483102 524552
specification 2 5.0106352940962555 524940
specifications 2 5.0106352940962555 524960
specifications.we 2 5.0106352940962555 524980
specifics 2 5.0106352940962555 525000
specified 18 2.8134107167600364 525020
specify 2 5.0106352940962555 525168
specifying 4 4.31748811353631 525188
spectra 2 5.0106352940962555 525224
spectral 14 3.0647251450409425 525244
spectral-type 2 5.0106352940962555 525360
spectrum 10 3.4011973816621555 525380
speech 8 3.624340932976365 525464
speechcommands 2 5.0106352940962555 525532
speed 14 3.0647251450409425 525552
speed-up 4 4.31748811353631 525668
speed.preprint 2 5.0106352940962555 525704
speeding 2 5.0106352940962555 525724
speedup 4 4.31748811353631 525744
spelling 2 5.0106352940962555 525780
spga 2 5.0106352940962555 525800
sphere 4 4.31748811353631 525820
spherical 2 5.0106352940962555 525856
spike 2 5.0106352940962555 525876
spike-based 2 5.0106352940962555 525896
spikegrad 3 4.605170185988092 525916
spikes 3 4.605170185988092 525944
spiking 2 5.0106352940962555 525972
spite 4 4.31748811353631 525992
split 2 5.0106352940962555 526028
splitting 2 5.0106352940962555 526048
spns 2 5.0106352940962555 526068
spotlight 2 5.0106352940962555 526088
sppm 2 5.0106352940962555 526108
spreading 2 5.0106352940962555 526128
spring 2 5.0106352940962555 526148
spurious 8 3.624340932976365 526168
spuriously 2 5.0106352940962555 526236
square 4 4.31748811353631 526256
square-root 2 5.0106352940962555 526292
squared 10 3.4011973816621555 526312
squares 4 4.31748811353631 526396
sr 2 5.0106352940962555 526432
sr3d 2 5.0106352940962555 526452
srebro 2 5.0106352940962555 526472
ssl 4 4.31748811353631 526492
ssv2 2 5.0106352940962555 526528
st 3 4.605170185988092 526548
stability 18 2.8134107167600364 526576
stabilize 4 4.31748811353631 526724
stable 21 2.659260036932778 526760
stable-rps.preprint.under 2 5.0106352940962555 526932
stack 2 5.0106352940962555 526952
stacked 6 3.912023005428146 526972
stacks.we 2 5.0106352940962555 527024
stage 10 3.4011973816621555 527044
stages 4 4.31748811353631 527128
staggering 2 5.0106352940962555 527164
stakes 2 5.0106352940962555 527184
stan 2 5.0106352940962555 527204
standard 78 1.3470736479666094 527224
standardly 2 5.0106352940962555 527852
standards 2 5.0106352940962555 527872
staple 2 5.0106352940962555 527892
star 2 5.0106352940962555 527912
starcoder 2 5.0106352940962555 527932
starcraft 2 5.0106352940962555 527952
stark 2 5.0106352940962555 527972
start 4 4.31748811353631 527992
starting 12 3.2188758248682006 528028
starts 2 5.0106352940962555 528128
stat.ml 4 4.31748811353631 528148
state 51 1.7719568419318754 528184
state-action 8 3.624340932976365 528596
state-and 2 5.0106352940962555 528664
state-constant 2 5.0106352940962555 528684
state-dependent 4 4.31748811353631 528704
state-of-art 2 5.0106352940962555 528740
state-of-the 2 5.0106352940962555 528760
state-of-the-art 124 0.8835009090511642 528780
state-of-the-art.overall 2 5.0106352940962555 529776
state-of-the-arts 2 5.0106352940962555 529796
state-of-theart 6 3.912023005428146 529816
state-ofthe-art 6 3.912023005428146 529868
state-switching 2 5.0106352940962555 529920
state.the 2 5.0106352940962555 529940
stateful 2 5.0106352940962555 529960
stateless 2 5.0106352940962555 529980
statements 2 5.0106352940962555 530000
stateof-the-art 6 3.912023005428146 530020
states 28 2.371577964480997 530072
states.our 2 5.0106352940962555 530300
static 10 3.4011973816621555 530320
stationary 4 4.31748811353631 530404
statistical 24 2.5257286443082556 530440
statistically 4 4.31748811353631 530636
statistics 12 3.2188758248682006 530672
stays 2 5.0106352940962555 530772
steady 2 5.0106352940962555 530792
steal 2 5.0106352940962555 530812
stealing 3 4.605170185988092 530832
stealthily 3 4.605170185988092 530860
stealthy 2 5.0106352940962555 530888
steering 2 5.0106352940962555 530908
steers 2 5.0106352940962555 530928
step 50 1.791759469228055 530948
step-by-step 6 3.912023005428146 531352
step-size 2 5.0106352940962555 531404
step-unrolled 3 4.605170185988092 531424
steps 26 2.445685936634719 531452
steps.lego-prover 2 5.0106352940962555 531664
stereo 2 5.0106352940962555 531684
steven 2 5.0106352940962555 531704
stiefel 2 5.0106352940962555 531724
still 48 1.8325814637483102 531744
stimuli 6 3.912023005428146 532132
stochastic 67 1.499089855265235 532184
stopping 4 4.31748811353631 532724
stops 2 5.0106352940962555 532760
storage 2 5.0106352940962555 532780
store 4 4.31748811353631 532800
stored 2 5.0106352940962555 532836
storing 4 4.31748811353631 532856
straightforward 12 3.2188758248682006 532892
strains 2 5.0106352940962555 532992
strand 2 5.0106352940962555 533012
strategic 2 5.0106352940962555 533032
strategically 2 5.0106352940962555 533052
strategies 46 1.875141078167106 533072
strategy 36 2.120263536200091 533444
strategy-graph 2 5.0106352940962555 533736
strategy-graphs 2 5.0106352940962555 533756
strategy.to 2 5.0106352940962555 533776
strategyqa 2 5.0106352940962555 533796
stream 8 3.624340932976365 533816
streamed 2 5.0106352940962555 533884
streaming 2 5.0106352940962555 533904
street 4 4.31748811353631 533924
strength 4 4.31748811353631 533960
strengths 4 4.31748811353631 533996
stress 2 5.0106352940962555 534032
strict 4 4.31748811353631 534052
strictly 4 4.31748811353631 534088
striking 4 4.31748811353631 534124
string 2 5.0106352940962555 534160
strings 2 5.0106352940962555 534180
striving 2 5.0106352940962555 534200
strong 49 1.8119621765455745 534220
stronger 8 3.624340932976365 534616
strongest 4 4.31748811353631 534684
strongly 12 3.2188758248682006 534720
structural 8 3.624340932976365 534820
structurally 2 5.0106352940962555 534888
structure 64 1.5448993912965292 534908
structure.based 2 5.0106352940962555 535424
structured 20 2.70805020110221 535444
structures 18 2.8134107167600364 535608
structures.however 2 5.0106352940962555 535756
structuring 2 5.0106352940962555 535776
struggle 16 2.9311937524164198 535796
stuart 2 5.0106352940962555 535928
stubborn 2 5.0106352940962555 535948
studied 18 2.8134107167600364 535968
studies 48 1.8325814637483102 536116
study 103 1.0690534864265653 536504
study.preprint 2 5.0106352940962555 537332
studying 8 3.624340932976365 537352
style 9 3.506557897319982 537420
style-based 2 5.0106352940962555 537496
stylegan-xl 2 5.0106352940962555 537516
styles 2 5.0106352940962555 537536
su 1 5.703782474656201 537556
sub-gaussian 2 5.0106352940962555 537568
sub-linear 2 5.0106352940962555 537588
sub-optimal 2 5.0106352940962555 537608
sub-optimality 2 5.0106352940962555 537628
sub-pixel 2 5.0106352940962555 537648
subclass 2 5.0106352940962555 537668
subclasses 2 5.0106352940962555 537688
subdifferential 2 5.0106352940962555 537708
subdomain 2 5.0106352940962555 537728
subdomains 2 5.0106352940962555 537748
subfield 2 5.0106352940962555 537768
subgoals 4 4.31748811353631 537788
subgradient 4 4.31748811353631 537824
subgradients 2 5.0106352940962555 537860
subgraph 5 4.0943445622221 537880
subgraphs 6 3.912023005428146 537924
subgroups 6 3.912023005428146 537976
subject 2 5.0106352940962555 538028
subjective 2 5.0106352940962555 538048
subjects 2 5.0106352940962555 538068
sublevel 2 5.0106352940962555 538088
sublinear 2 5.0106352940962555 538108
submanifolds 2 5.0106352940962555 538128
submissions 2 5.0106352940962555 538148
submodular 3 4.605170185988092 538168
subnetworks 2 5.0106352940962555 538196
suboptimal 19 2.7593434954897607 538216
suboptimality 2 5.0106352940962555 538372
subpo 2 5.0106352940962555 538392
subpopulation 2 5.0106352940962555 538412
subpopulations 4 4.31748811353631 538432
subrl 2 5.0106352940962555 538468
subroutines 2 5.0106352940962555 538488
subsection 4 4.31748811353631 538508
subsequence 2 5.0106352940962555 538544
subsequent 4 4.31748811353631 538564
subsequently 10 3.4011973816621555 538600
subset 10 3.4011973816621555 538684
subsets 2 5.0106352940962555 538768
subspace 4 4.31748811353631 538788
substantial 12 3.2188758248682006 538824
substantially 24 2.5257286443082556 538924
substructures 2 5.0106352940962555 539120
subtask 2 5.0106352940962555 539140
subtasks 4 4.31748811353631 539160
subtle 2 5.0106352940962555 539196
succeed 6 3.912023005428146 539216
succeeded 2 5.0106352940962555 539268
succeeds 2 5.0106352940962555 539288
success 44 1.91959284073794 539308
successes 10 3.4011973816621555 539664
successful 22 2.6127400212978853 539748
successfully 16 2.9311937524164198 539928
successive 2 5.0106352940962555 540060
successively 2 5.0106352940962555 540080
successor 4 4.31748811353631 540100
succinctness 2 5.0106352940962555 540136
such 222 0.3011050927839216 540156
suffer 24 2.5257286443082556 541936
suffering 4 4.31748811353631 542132
suffers 12 3.2188758248682006 542168
sufficient 24 2.5257286443082556 542268
sufficiently 10 3.4011973816621555 542464
suffixes 2 5.0106352940962555 542548
suggest 10 3.4011973816621555 542568
suggested 4 4.31748811353631 542652
suggesting 12 3.2188758248682006 542688
suggests 8 3.624340932976365 542788
suitability 2 5.0106352940962555 542856
suitable 10 3.4011973816621555 542876
suite 6 3.912023005428146 542960
suited 6 3.912023005428146 543012
suites 2 5.0106352940962555 543064
sum 3 4.605170185988092 543084
sum-product 3 4.605170185988092 543112
summarization 2 5.0106352940962555 543140
summarize 2 5.0106352940962555 543160
summarized 4 4.31748811353631 543180
summarizes 2 5.0106352940962555 543216
summarizing 2 5.0106352940962555 543236
summary 6 3.912023005428146 543256
summed 2 5.0106352940962555 543308
summer 2 5.0106352940962555 543328
sun 3 4.605170185988092 543348
sundae 2 5.0106352940962555 543376
super-features 3 4.605170185988092 543396
super-quadratic 2 5.0106352940962555 543424
super-resolution 2 5.0106352940962555 543444
superfluous 2 5.0106352940962555 543464
superior 34 2.17742195004004 543484
superiority 10 3.4011973816621555 543760
superresolution 2 5.0106352940962555 543844
superscripts 2 5.0106352940962555 543864
supervised 29 2.336486644669727 543884
supervised-pretrained 2 5.0106352940962555 544120
supervision 5 4.0943445622221 544140
supervisions 2 5.0106352940962555 544184
supplemented 2 5.0106352940962555 544204
support 24 2.5257286443082556 544224
supported 2 5.0106352940962555 544420
supporting 4 4.31748811353631 544440
supports 12 3.2188758248682006 544476
suppose 2 5.0106352940962555 544576
suppressed 2 5.0106352940962555 544596
suppressing 2 5.0106352940962555 544616
surface 8 3.624340932976365 544636
surfaces 4 4.31748811353631 544704
surged 2 5.0106352940962555 544740
surjective 2 5.0106352940962555 544760
surpasses 4 4.31748811353631 544780
surpassing 2 5.0106352940962555 544816
surprising 8 3.624340932976365 544836
surprisingly 22 2.6127400212978853 544904
surrogate 7 3.7578723256008875 545084
surrounding 4 4.31748811353631 545144
surroundings 2 5.0106352940962555 545180
susceptibility 2 5.0106352940962555 545200
susceptible 2 5.0106352940962555 545220
suspected 2 5.0106352940962555 545240
sutskever 2 5.0106352940962555 545260
sutton 2 5.0106352940962555 545280
svamp 2 5.0106352940962555 545300
svetlozar 2 5.0106352940962555 545320
svm 2 5.0106352940962555 545340
svrg 2 5.0106352940962555 545360
svrp 2 5.0106352940962555 545380
sw 2 5.0106352940962555 545400
sweet 2 5.0106352940962555 545420
switching 2 5.0106352940962555 545440
sycophantic 2 5.0106352940962555 545460
symbol 4 4.31748811353631 545480
symbol-like 2 5.0106352940962555 545516
symbol-processing 2 5.0106352940962555 545536
symbolic 9 3.506557897319982 545556
symbols 4 4.31748811353631 545632
symbols.such 2 5.0106352940962555 545668
symmetric 6 3.912023005428146 545688
symmetries 2 5.0106352940962555 545740
symmetry 6 3.912023005428146 545760
synchronization 3 4.605170185988092 545812
synchronizing 2 5.0106352940962555 545840
synflow 2 5.0106352940962555 545860
synonymy 2 5.0106352940962555 545880
syntactic 2 5.0106352940962555 545900
syntax 4 4.31748811353631 545920
synthesis 39 2.0402208285265546 545956
synthesize 12 3.2188758248682006 546272
synthesized 6 3.912023005428146 546372
synthesizes 2 5.0106352940962555 546424
synthesizing 6 3.912023005428146 546444
synthetic 46 1.875141078167106 546496
sysbinder 2 5.0106352940962555 546868
system 26 2.445685936634719 546888
system.on 2 5.0106352940962555 547100
systematic 9 3.506557897319982 547120
systematical 2 5.0106352940962555 547196
systematically 20 2.70805020110221 547216
systematicity 2 5.0106352940962555 547380
systems 53 1.7334905611040792 547400
systems.repobench 2 5.0106352940962555 547828
szepesvari 2 5.0106352940962555 547848
sbastien 2 5.0106352940962555 547868
t 23 2.5682882587270512 547888
t0 1 5.703782474656201 548076
t0-sf 2 5.0106352940962555 548088
t1 1 5.703782474656201 548108
t2 1 5.703782474656201 548120
t2i 2 5.0106352940962555 548132
t5 3 4.605170185988092 548152
table 4 4.31748811353631 548180
tabletop 2 5.0106352940962555 548216
tabular 10 3.4011973816621555 548236
tackle 20 2.70805020110221 548320
tackles 2 5.0106352940962555 548484
tackling 4 4.31748811353631 548504
tail 2 5.0106352940962555 548540
tailored 16 2.9311937524164198 548560
take 12 3.2188758248682006 548692
taken 6 3.912023005428146 548792
takes 12 3.2188758248682006 548844
taking 12 3.2188758248682006 548944
talk 2 5.0106352940962555 549044
talvitie 2 5.0106352940962555 549064
tamas 2 5.0106352940962555 549084
tame 2 5.0106352940962555 549104
tangent 10 3.4011973816621555 549124
tangible 2 5.0106352940962555 549208
tao 1 5.703782474656201 549228
target 54 1.7147984280919266 549240
target-agnostic 3 4.605170185988092 549676
target-domain 2 5.0106352940962555 549704
targeted 4 4.31748811353631 549724
targeting 2 5.0106352940962555 549760
targets 4 4.31748811353631 549780
targetspecific 2 5.0106352940962555 549816
task 128 0.8517522107365839 549836
task-agnostic 11 3.3058872018578307 550864
task-and 2 5.0106352940962555 550956
task-dependent 2 5.0106352940962555 550976
task-irrelevant 2 5.0106352940962555 550996
task-level 4 4.31748811353631 551016
task-relevant 2 5.0106352940962555 551052
task-specific 14 3.0647251450409425 551072
task.heuristic 2 5.0106352940962555 551188
task.however 2 5.0106352940962555 551208
task.over 2 5.0106352940962555 551228
tasks 207 0.37106368139083196 551248
tasks.an 2 5.0106352940962555 552908
tasks.evaluations 2 5.0106352940962555 552928
tasks.further 2 5.0106352940962555 552948
tasks.however 4 4.31748811353631 552968
tasks.in 2 5.0106352940962555 553004
tasks.notably 2 5.0106352940962555 553024
tasks.our 2 5.0106352940962555 553044
tasks.preprint 2 5.0106352940962555 553064
tasks.these 4 4.31748811353631 553084
tasks.this 2 5.0106352940962555 553120
tasks.we 2 5.0106352940962555 553140
taskspecific 2 5.0106352940962555 553160
taxonomy 4 4.31748811353631 553180
taylor 2 5.0106352940962555 553216
td 5 4.0943445622221 553236
td-learning 2 5.0106352940962555 553280
td-vae 2 5.0106352940962555 553300
td3 2 5.0106352940962555 553320
tdc 2 5.0106352940962555 553340
tdl 2 5.0106352940962555 553360
te 1 5.703782474656201 553380
teach 2 5.0106352940962555 553392
teaching 4 4.31748811353631 553412
teaching_arithmetic 2 5.0106352940962555 553448
teaming 2 5.0106352940962555 553468
technical 6 3.912023005428146 553488
technically 4 4.31748811353631 553540
technique 42 1.9661128563728327 553576
techniques 56 1.6784307839210517 553916
techniques.we 2 5.0106352940962555 554368
technology 4 4.31748811353631 554388
tell 2 5.0106352940962555 554424
temperature 2 5.0106352940962555 554444
template-based 2 5.0106352940962555 554464
template.autoregressive 2 5.0106352940962555 554484
tempo 3 4.605170185988092 554504
tempoover 2 5.0106352940962555 554532
temporal 34 2.17742195004004 554552
temporal-difference 4 4.31748811353631 554828
temporally 6 3.912023005428146 554864
ten 3 4.605170185988092 554916
tend 14 3.0647251450409425 554944
tends 6 3.912023005428146 555060
tengyu 4 4.31748811353631 555112
tens 2 5.0106352940962555 555148
tension 4 4.31748811353631 555168
tensor 9 3.506557897319982 555204
tensor-train 2 5.0106352940962555 555280
tensors 5 4.0943445622221 555300
ter 2 5.0106352940962555 555344
terabytes 2 5.0106352940962555 555364
term 30 2.302585092994046 555384
termed 6 3.912023005428146 555628
terminal 2 5.0106352940962555 555680
terminate 2 5.0106352940962555 555700
termination 2 5.0106352940962555 555720
terminology 4 4.31748811353631 555740
terms 58 1.6433394641097818 555776
tesauro 2 5.0106352940962555 556244
test 68 1.4842747694800944 556264
test-time 4 4.31748811353631 556812
tested 6 3.912023005428146 556848
testing 22 2.6127400212978853 556900
tests 4 4.31748811353631 557080
text 45 1.8971199848858813 557116
text-based 4 4.31748811353631 557480
text-conditioned 4 4.31748811353631 557516
text-driven 2 5.0106352940962555 557552
text-modeling 2 5.0106352940962555 557572
text-to-3d 3 4.605170185988092 557592
text-to-image 7 3.7578723256008875 557620
text-to-speech 2 5.0106352940962555 557680
text-to-video 5 4.0943445622221 557700
text.they 2 5.0106352940962555 557744
text.using 2 5.0106352940962555 557764
text8 2 5.0106352940962555 557784
textbook 2 5.0106352940962555 557804
texts 2 5.0106352940962555 557824
texttovec 2 5.0106352940962555 557844
textual 14 3.0647251450409425 557864
texture 8 3.624340932976365 557980
thakur 2 5.0106352940962555 558048
than 148 0.7065702008920861 558068
thanks 10 3.4011973816621555 559256
that 542 -0.5914835267834451 559340
that-at 2 5.0106352940962555 563680
that.encipher 2 5.0106352940962555 563700
the 336 -0.11332868530700312 563720
their 172 0.556287997842748 566412
them 72 1.4271163556401458 567792
thematic 2 5.0106352940962555 568372
themselves 6 3.912023005428146 568392
then 82 1.297063227391948 568444
theo 2 5.0106352940962555 569104
theorem 9 3.506557897319982 569124
theorems 2 5.0106352940962555 569200
theorems.one 2 5.0106352940962555 569220
theoretical 56 1.6784307839210517 569240
theoretically 34 2.17742195004004 569692
theories 2 5.0106352940962555 569968
theory 29 2.336486644669727 569988
theory-grounded 2 5.0106352940962555 570224
theory.towards 2 5.0106352940962555 570244
there 60 1.6094379124341003 570264
thereby 16 2.9311937524164198 570748
therefore 28 2.371577964480997 570880
therein 4 4.31748811353631 571108
these 202 0.3955147772549963 571144
thesis 2 5.0106352940962555 572764
they 116 0.9501922835498364 572784
thieves 2 5.0106352940962555 573716
think 2 5.0106352940962555 573736
thinking 4 4.31748811353631 573756
thinning 2 5.0106352940962555 573792
third 10 3.4011973816621555 573812
third-person 2 5.0106352940962555 573896
thirdperson 2 5.0106352940962555 573916
this 496 -0.5027934520687264 573936
thomas 2 5.0106352940962555 577908
thompson 2 5.0106352940962555 577928
thorough 12 3.2188758248682006 577948
thoroughly 4 4.31748811353631 578048
those 46 1.875141078167106 578084
though 12 3.2188758248682006 578456
thought 6 3.912023005428146 578556
thoughtful 2 5.0106352940962555 578608
thoughts 2 5.0106352940962555 578628
thousands 4 4.31748811353631 578648
threat 6 3.912023005428146 578684
threatening 2 5.0106352940962555 578736
threats 2 5.0106352940962555 578756
three 46 1.875141078167106 578776
three-dimensional 2 5.0106352940962555 579148
three-round 2 5.0106352940962555 579168
threefold 2 5.0106352940962555 579188
threshold 4 4.31748811353631 579208
threshold-based 4 4.31748811353631 579244
threshold.based 2 5.0106352940962555 579280
thresholding 4 4.31748811353631 579300
thresholds 2 5.0106352940962555 579336
through 113 0.9763946559438605 579356
throughout 2 5.0106352940962555 580264
thudm 2 5.0106352940962555 580284
thus 72 1.4271163556401458 580304
tian 4 4.31748811353631 580884
ticket 4 4.31748811353631 580920
tickets 2 5.0106352940962555 580956
tied 2 5.0106352940962555 580976
tight 14 3.0647251450409425 580996
tighter 2 5.0106352940962555 581112
tightly 4 4.31748811353631 581132
time 106 1.0403433805441338 581168
time-homogeneous 2 5.0106352940962555 582020
time-step 2 5.0106352940962555 582040
time.arxiv 2 5.0106352940962555 582060
time.as 2 5.0106352940962555 582080
time.extensive 2 5.0106352940962555 582100
time.in 2 5.0106352940962555 582120
time.learning 2 5.0106352940962555 582140
time.these 2 5.0106352940962555 582160
time.to 2 5.0106352940962555 582180
times 24 2.5257286443082556 582200
times.related 2 5.0106352940962555 582396
timestep 2 5.0106352940962555 582416
timesteps 4 4.31748811353631 582436
timo 2 5.0106352940962555 582472
tiny 2 5.0106352940962555 582492
tishby 2 5.0106352940962555 582512
tk 1 5.703782474656201 582532
tm 2 5.0106352940962555 582544
tms 1 5.703782474656201 582564
to 305 -0.016529301951210582 582576
today 2 5.0106352940962555 585020
together 24 2.5257286443082556 585040
togl 2 5.0106352940962555 585236
tokamak 2 5.0106352940962555 585256
token 12 3.2188758248682006 585276
token-dependent 2 5.0106352940962555 585376
token-level 2 5.0106352940962555 585396
token-like 2 5.0106352940962555 585416
tokenization 5 4.0943445622221 585436
tokenized 2 5.0106352940962555 585480
tokenizer 4 4.31748811353631 585500
tokens 33 2.207274913189721 585536
tokens.our 2 5.0106352940962555 585804
tolerable 2 5.0106352940962555 585824
tolerance 2 5.0106352940962555 585844
tolsma 2 5.0106352940962555 585864
tommi 2 5.0106352940962555 585884
tomography 2 5.0106352940962555 585904
tone 2 5.0106352940962555 585924
tonnes 2 5.0106352940962555 585944
too 5 4.0943445622221 585964
tool 22 2.6127400212978853 586008
tool-and 2 5.0106352940962555 586188
toolbox 2 5.0106352940962555 586208
toolemu 2 5.0106352940962555 586228
toolkit 2 5.0106352940962555 586248
tools 20 2.70805020110221 586268
tools.we 2 5.0106352940962555 586432
top 2 5.0106352940962555 586452
top-1 6 3.912023005428146 586472
top-down 2 5.0106352940962555 586524
top-k 4 4.31748811353631 586544
top-ranking 2 5.0106352940962555 586580
topic 9 3.506557897319982 586600
topological 10 3.4011973816621555 586676
topologies 4 4.31748811353631 586760
topology 5 4.0943445622221 586796
topped 2 5.0106352940962555 586840
torus 3 4.605170185988092 586860
toss 2 5.0106352940962555 586888
total 18 2.8134107167600364 586908
totality 2 5.0106352940962555 587056
touch 2 5.0106352940962555 587076
touch-line 3 4.605170185988092 587096
tours 2 5.0106352940962555 587124
tourzhao 2 5.0106352940962555 587144
touvron 2 5.0106352940962555 587164
toward 10 3.4011973816621555 587184
towards 45 1.8971199848858813 587268
toxic 2 5.0106352940962555 587632
trace 4 4.31748811353631 587652
trace-driven 2 5.0106352940962555 587688
traceability 2 5.0106352940962555 587708
traced 2 5.0106352940962555 587728
traces 2 5.0106352940962555 587748
track 4 4.31748811353631 587768
tracking 2 5.0106352940962555 587804
tracks 6 3.912023005428146 587824
tractability 2 5.0106352940962555 587876
tractable 12 3.2188758248682006 587896
traction 2 5.0106352940962555 587996
trade 2 5.0106352940962555 588016
trade-off 18 2.8134107167600364 588036
tradeoff 8 3.624340932976365 588184
tradeoff.moreover 2 5.0106352940962555 588252
trades 2 5.0106352940962555 588272
traditional 14 3.0647251450409425 588292
traditionally 6 3.912023005428146 588408
trail 3 4.605170185988092 588460
train 68 1.4842747694800944 588488
train-test 2 5.0106352940962555 589036
trainable 6 3.912023005428146 589056
trained 120 0.9162907318741551 589108
training 240 0.22314355131420976 590072
training-free 5 4.0943445622221 591996
training.as 2 5.0106352940962555 592040
training.in 2 5.0106352940962555 592060
training.the 2 5.0106352940962555 592080
training.to 2 5.0106352940962555 592100
training.upon 2 5.0106352940962555 592120
trains 14 3.0647251450409425 592140
traits 2 5.0106352940962555 592256
trajectories 8 3.624340932976365 592276
trajectory 22 2.6127400212978853 592344
traning 2 5.0106352940962555 592524
transactions 4 4.31748811353631 592544
transductive 2 5.0106352940962555 592580
transfer 41 1.9902104079518932 592600
transferability 4 4.31748811353631 592932
transferable 2 5.0106352940962555 592968
transfers 4 4.31748811353631 592988
transform 8 3.624340932976365 593024
transformation 18 2.8134107167600364 593092
transformation-based 2 5.0106352940962555 593240
transformations 17 2.870569130599985 593260
transformed 4 4.31748811353631 593400
transformer 32 2.2380465718564744 593436
transformer-based 2 5.0106352940962555 593696
transformers 33 2.207274913189721 593716
transforming 2 5.0106352940962555 593984
transition 17 2.870569130599985 594004
transition-metal 2 5.0106352940962555 594144
transition-reparametrized 2 5.0106352940962555 594164
transitions 14 3.0647251450409425 594184
transitions.in 2 5.0106352940962555 594300
transitions.on 2 5.0106352940962555 594320
transitions.they 2 5.0106352940962555 594340
translate 4 4.31748811353631 594360
translated 8 3.624340932976365 594396
translating 2 5.0106352940962555 594464
translation 25 2.4849066497880004 594484
translations 2 5.0106352940962555 594688
translators 2 5.0106352940962555 594708
transmitted 2 5.0106352940962555 594728
transpilation 3 4.605170185988092 594748
transpiler.we 2 5.0106352940962555 594776
transpiles 2 5.0106352940962555 594796
transport 10 3.4011973816621555 594816
transport-based 2 5.0106352940962555 594900
transportation 2 5.0106352940962555 594920
traversal 2 5.0106352940962555 594940
traverse 2 5.0106352940962555 594960
traversing 4 4.31748811353631 594980
treat 4 4.31748811353631 595016
treated 2 5.0106352940962555 595052
treating 8 3.624340932976365 595072
treatment 9 3.506557897319982 595140
treatments 2 5.0106352940962555 595216
tree 11 3.3058872018578307 595236
tree-like 2 5.0106352940962555 595328
tree-structured 2 5.0106352940962555 595348
treebank 2 5.0106352940962555 595368
treelike 2 5.0106352940962555 595388
trees 2 5.0106352940962555 595408
tremendous 2 5.0106352940962555 595428
trend 6 3.912023005428146 595448
trend-level 2 5.0106352940962555 595500
trends 2 5.0106352940962555 595520
trends.despite 2 5.0106352940962555 595540
trial 2 5.0106352940962555 595560
trial-anderror 2 5.0106352940962555 595580
trials 2 5.0106352940962555 595600
triangle 4 4.31748811353631 595620
triangular 2 5.0106352940962555 595656
trick 6 3.912023005428146 595676
trick.we 2 5.0106352940962555 595728
tries 4 4.31748811353631 595748
trigger 7 3.7578723256008875 595784
triggers 2 5.0106352940962555 595844
trillions 2 5.0106352940962555 595864
triples 2 5.0106352940962555 595884
trivial 2 5.0106352940962555 595904
trivially 2 5.0106352940962555 595924
trojai 2 5.0106352940962555 595944
trojan 3 4.605170185988092 595964
trojaned 2 5.0106352940962555 595992
true 16 2.9311937524164198 596012
truly 2 5.0106352940962555 596144
truncation 2 5.0106352940962555 596164
trustworthiness 2 5.0106352940962555 596184
truth 14 3.0647251450409425 596204
try 5 4.0943445622221 596320
trying 2 5.0106352940962555 596364
tt 2 5.0106352940962555 596384
tt-cores 2 5.0106352940962555 596404
tt-ranks 2 5.0106352940962555 596424
tt-representation 2 5.0106352940962555 596444
tts 2 5.0106352940962555 596464
tunes 2 5.0106352940962555 596484
tuning 26 2.445685936634719 596504
tuples 2 5.0106352940962555 596716
turing 3 4.605170185988092 596736
turing-complete 2 5.0106352940962555 596764
turn 8 3.624340932976365 596784
turns 2 5.0106352940962555 596852
tutorial 2 5.0106352940962555 596872
tvp 2 5.0106352940962555 596892
tweak 2 5.0106352940962555 596912
tweet 2 5.0106352940962555 596932
tweets 2 5.0106352940962555 596952
twin 2 5.0106352940962555 596972
twitter 5 4.0943445622221 596992
twitter.more 2 5.0106352940962555 597036
two 68 1.4842747694800944 597056
two-branch 2 5.0106352940962555 597604
two-fold 4 4.31748811353631 597624
two-layer 9 3.506557897319982 597660
two-layer-neural 2 5.0106352940962555 597736
two-line 2 5.0106352940962555 597756
two-player 2 5.0106352940962555 597776
two-stage 4 4.31748811353631 597796
two-step 2 5.0106352940962555 597832
two-timescale 2 5.0106352940962555 597852
twofold 2 5.0106352940962555 597872
tying 3 4.605170185988092 597892
type 14 3.0647251450409425 597920
types 34 2.17742195004004 598036
typical 10 3.4011973816621555 598312
typically 46 1.875141078167106 598396
tze 1 5.703782474656201 598768
u 2 5.0106352940962555 598780
u-net 2 5.0106352940962555 598800
u-shaped 2 5.0106352940962555 598820
u-statistics 2 5.0106352940962555 598840
ubiquitous 8 3.624340932976365 598860
ubiquity 2 5.0106352940962555 598928
ucb 2 5.0106352940962555 598948
ucb-vi 2 5.0106352940962555 598968
uci 1 5.703782474656201 598988
ultimately 2 5.0106352940962555 599000
ultra 2 5.0106352940962555 599020
ultra-sparse 2 5.0106352940962555 599040
unable 8 3.624340932976365 599060
unadapted 2 5.0106352940962555 599128
unaltered 2 5.0106352940962555 599148
unavailable 2 5.0106352940962555 599168
unbiased 8 3.624340932976365 599188
unbounded 2 5.0106352940962555 599256
uncertain 4 4.31748811353631 599276
uncertainty 19 2.7593434954897607 599312
uncertainty.we 2 5.0106352940962555 599468
uncharted 2 5.0106352940962555 599488
unclear 12 3.2188758248682006 599508
uncommon 4 4.31748811353631 599608
unconditional 8 3.624340932976365 599644
unconstrained 2 5.0106352940962555 599712
uncontrolled 2 5.0106352940962555 599732
uncorrupted 2 5.0106352940962555 599752
uncurated 2 5.0106352940962555 599772
under 137 0.7838015488280762 599792
under-explored 2 5.0106352940962555 600892
under-or 2 5.0106352940962555 600912
under-represents 2 5.0106352940962555 600932
under-studied 2 5.0106352940962555 600952
underexplored 2 5.0106352940962555 600972
underlie 4 4.31748811353631 600992
underline 2 5.0106352940962555 601028
underlying 34 2.17742195004004 601048
undermine 2 5.0106352940962555 601324
underparameterization 2 5.0106352940962555 601344
underparameterized 5 4.0943445622221 601364
underperform 2 5.0106352940962555 601408
underpinning 2 5.0106352940962555 601428
underscores 2 5.0106352940962555 601448
underscoring 4 4.31748811353631 601468
understand 22 2.6127400212978853 601504
understanding 46 1.875141078167106 601684
understood 10 3.4011973816621555 602056
understudied 2 5.0106352940962555 602140
undesirable 6 3.912023005428146 602160
undesired 2 5.0106352940962555 602212
undetected 4 4.31748811353631 602232
unet 2 5.0106352940962555 602268
unfair 2 5.0106352940962555 602288
unfortunately 20 2.70805020110221 602308
unfriendly 2 5.0106352940962555 602472
unification 4 4.31748811353631 602492
unified 17 2.870569130599985 602528
unifies 2 5.0106352940962555 602668
uniform 10 3.4011973816621555 602688
uniformity 2 5.0106352940962555 602772
uniformly 14 3.0647251450409425 602792
uniformly.however 2 5.0106352940962555 602908
unifying 14 3.0647251450409425 602928
unigram 2 5.0106352940962555 603044
unikgqa 3 4.605170185988092 603064
unintended 2 5.0106352940962555 603092
uninteresting 2 5.0106352940962555 603112
uninterrupted 2 5.0106352940962555 603132
unique 18 2.8134107167600364 603152
unit 8 3.624340932976365 603300
unit- 2 5.0106352940962555 603368
unit-hypersphere 2 5.0106352940962555 603388
unite 2 5.0106352940962555 603408
units 8 3.624340932976365 603428
univariate 2 5.0106352940962555 603496
universal 17 2.870569130599985 603516
universality 3 4.605170185988092 603656
universally 2 5.0106352940962555 603684
university 10 3.4011973816621555 603704
unknown 14 3.0647251450409425 603788
unlabeled 18 2.8134107167600364 603904
unlabelled 4 4.31748811353631 604052
unlawful 2 5.0106352940962555 604088
unlearnable 2 5.0106352940962555 604108
unleashing 2 5.0106352940962555 604128
unlike 22 2.6127400212978853 604148
unlimited 4 4.31748811353631 604328
unlock 2 5.0106352940962555 604364
unlocked 4 4.31748811353631 604384
unmasked 2 5.0106352940962555 604420
unmasking 2 5.0106352940962555 604440
unmixed 2 5.0106352940962555 604460
unnecessary 4 4.31748811353631 604480
unnecessary.but 2 5.0106352940962555 604516
unobserved 6 3.912023005428146 604536
unpack 2 5.0106352940962555 604588
unprecedented 4 4.31748811353631 604608
unquantized 2 5.0106352940962555 604644
unrealistic 2 5.0106352940962555 604664
unrealistically 2 5.0106352940962555 604684
unrelated 3 4.605170185988092 604704
unresolved 2 5.0106352940962555 604732
unrestricted 2 5.0106352940962555 604752
unrolling 2 5.0106352940962555 604772
unsafe 2 5.0106352940962555 604792
unseen 26 2.445685936634719 604812
unstable 4 4.31748811353631 605024
unstructured 10 3.4011973816621555 605060
unsupervised 31 2.2697952701710546 605144
unsupervised-disentanglement-torus 2 5.0106352940962555 605396
unsupervisedly 2 5.0106352940962555 605416
until 12 3.2188758248682006 605436
untrained 3 4.605170185988092 605536
unwieldy 2 5.0106352940962555 605564
up 29 2.336486644669727 605584
up-to-date 2 5.0106352940962555 605820
update 16 2.9311937524164198 605840
update.code 2 5.0106352940962555 605972
updated 10 3.4011973816621555 605992
updates 22 2.6127400212978853 606076
updating 4 4.31748811353631 606256
upgrade 2 5.0106352940962555 606292
upon 14 3.0647251450409425 606312
upper 12 3.2188758248682006 606428
upper-and 2 5.0106352940962555 606528
upper-bounds 2 5.0106352940962555 606548
upsampling 2 5.0106352940962555 606568
ur 1 5.703782474656201 606588
url 1 5.703782474656201 606600
us 27 2.407945608651872 606612
usage 10 3.4011973816621555 606832
use 68 1.4842747694800944 606916
used 112 0.9852836033611064 607464
useful 18 2.8134107167600364 608364
useless 2 5.0106352940962555 608512
user 15 2.995732273553991 608532
user-friendly 2 5.0106352940962555 608656
user-specified 2 5.0106352940962555 608676
user-written 2 5.0106352940962555 608696
users 9 3.506557897319982 608716
users.our 2 5.0106352940962555 608792
uses 40 2.0149030205422647 608812
using 209 0.36144822269139 609136
usual 2 5.0106352940962555 610812
usually 26 2.445685936634719 610832
ut 2 5.0106352940962555 611044
utility 6 3.912023005428146 611064
utility-privacy 2 5.0106352940962555 611116
utilization 2 5.0106352940962555 611136
utilize 12 3.2188758248682006 611156
utilized 8 3.624340932976365 611256
utilizes 14 3.0647251450409425 611324
utilizing 14 3.0647251450409425 611440
utmost 2 5.0106352940962555 611556
uts 1 5.703782474656201 611576
utterances 2 5.0106352940962555 611588
uttered 2 5.0106352940962555 611608
v 5 4.0943445622221 611628
v1 2 5.0106352940962555 611672
v2 3 4.605170185988092 611692
v3 1 5.703782474656201 611720
vaccines 2 5.0106352940962555 611732
vae 1 5.703782474656201 611752
vaes 2 5.0106352940962555 611764
vague 2 5.0106352940962555 611784
vaidehi99 2 5.0106352940962555 611804
valid 8 3.624340932976365 611824
validate 26 2.445685936634719 611892
validated 8 3.624340932976365 612104
validating 2 5.0106352940962555 612172
validation 10 3.4011973816621555 612192
validity 2 5.0106352940962555 612276
valuable 2 5.0106352940962555 612296
value 42 1.9661128563728327 612316
value-aligned 2 5.0106352940962555 612656
value-based 4 4.31748811353631 612676
value-function 2 5.0106352940962555 612712
value.on 2 5.0106352940962555 612732
valued 2 5.0106352940962555 612752
values 25 2.4849066497880004 612772
van 1 5.703782474656201 612976
vanilla 6 3.912023005428146 612988
vanishing 4 4.31748811353631 613040
vapnik-chervonenkis 2 5.0106352940962555 613076
variability 4 4.31748811353631 613096
variable 14 3.0647251450409425 613132
variable-binding 2 5.0106352940962555 613248
variables 14 3.0647251450409425 613268
variance 20 2.70805020110221 613384
variance-reduced 2 5.0106352940962555 613548
variance-reduction 2 5.0106352940962555 613568
variant 6 3.912023005428146 613588
variants 26 2.445685936634719 613640
variation 6 3.912023005428146 613852
variational 36 2.120263536200091 613904
variations 10 3.4011973816621555 614196
varied 2 5.0106352940962555 614280
varies 4 4.31748811353631 614300
variety 52 1.7525387560747736 614336
various 74 1.3997173814520314 614756
vary 4 4.31748811353631 615352
varying 16 2.9311937524164198 615388
varying-size 2 5.0106352940962555 615520
vast 6 3.912023005428146 615540
vaswani 2 5.0106352940962555 615592
vc 3 4.605170185988092 615612
vcl 2 5.0106352940962555 615640
vector 22 2.6127400212978853 615660
vector-quantized 2 5.0106352940962555 615840
vector-space 2 5.0106352940962555 615860
vectored 2 5.0106352940962555 615880
vectors 14 3.0647251450409425 615900
vehicle 2 5.0106352940962555 616016
vein 2 5.0106352940962555 616036
vendors 2 5.0106352940962555 616056
verbal 2 5.0106352940962555 616076
verifiable 2 5.0106352940962555 616096
verifiably 3 4.605170185988092 616116
verification 12 3.2188758248682006 616144
verification-based 2 5.0106352940962555 616244
verification.we 2 5.0106352940962555 616264
verified 8 3.624340932976365 616284
verify 20 2.70805020110221 616352
verma 2 5.0106352940962555 616516
versatile 2 5.0106352940962555 616536
versatility 4 4.31748811353631 616556
version 12 3.2188758248682006 616592
versions 4 4.31748811353631 616692
versus 4 4.31748811353631 616728
vertex 2 5.0106352940962555 616764
vertical 2 5.0106352940962555 616784
very 38 2.0661963149298153 616804
vgg 1 5.703782474656201 617112
vgg-based 2 5.0106352940962555 617124
vgg-net 2 5.0106352940962555 617144
vi 2 5.0106352940962555 617164
via 56 1.6784307839210517 617184
viability 2 5.0106352940962555 617636
viable 4 4.31748811353631 617656
vib 2 5.0106352940962555 617692
vibrational 2 5.0106352940962555 617712
victim 6 3.912023005428146 617732
victims 2 5.0106352940962555 617784
vidal 2 5.0106352940962555 617804
video 29 2.336486644669727 617824
videoflow 2 5.0106352940962555 618060
videos 24 2.5257286443082556 618080
videos.existing 2 5.0106352940962555 618276
view 20 2.70805020110221 618296
viewed 8 3.624340932976365 618460
viewing 2 5.0106352940962555 618528
viewpoint 2 5.0106352940962555 618548
views 4 4.31748811353631 618568
vigorous 2 5.0106352940962555 618604
vinyals 2 5.0106352940962555 618624
violate 2 5.0106352940962555 618644
violations 2 5.0106352940962555 618664
virtual 6 3.912023005428146 618684
virtually 2 5.0106352940962555 618736
visible 2 5.0106352940962555 618756
vision 35 2.1484344131667874 618776
vision-based 5 4.0943445622221 619060
vision-language 10 3.4011973816621555 619104
vision-only 4 4.31748811353631 619188
visit 4 4.31748811353631 619224
visited 2 5.0106352940962555 619260
visual 66 1.5141277326297755 619280
visual-only 2 5.0106352940962555 619812
visualizations 2 5.0106352940962555 619832
visualize 4 4.31748811353631 619852
visualized 2 5.0106352940962555 619888
visualizing 4 4.31748811353631 619908
visually 4 4.31748811353631 619944
vit 2 5.0106352940962555 619980
vit-b 4 4.31748811353631 620000
vit-based 2 5.0106352940962555 620036
vit-huge 2 5.0106352940962555 620056
vits 4 4.31748811353631 620076
vittorio 2 5.0106352940962555 620112
vizdoom 2 5.0106352940962555 620132
vlp 2 5.0106352940962555 620152
vmoe 2 5.0106352940962555 620172
vocabularies 2 5.0106352940962555 620192
vocabularies.in 4 4.31748811353631 620212
vocabulary 4 4.31748811353631 620248
vocalizations 2 5.0106352940962555 620284
voice 2 5.0106352940962555 620304
voices 3 4.605170185988092 620324
volpi 2 5.0106352940962555 620352
volume 2 5.0106352940962555 620372
vq 2 5.0106352940962555 620392
vs 1 5.703782474656201 620412
vs. 3 4.605170185988092 620424
vulnerabilities 2 5.0106352940962555 620452
vulnerability 12 3.2188758248682006 620472
vulnerable 10 3.4011973816621555 620572
vwhdo 2 5.0106352940962555 620656
w 2 5.0106352940962555 620676
w.r.t 6 3.912023005428146 620696
wake 2 5.0106352940962555 620748
wake-sleep 2 5.0106352940962555 620768
walkable 2 5.0106352940962555 620788
walking 2 5.0106352940962555 620808
wall 2 5.0106352940962555 620828
wall-clock 2 5.0106352940962555 620848
wanda 2 5.0106352940962555 620868
want 2 5.0106352940962555 620888
ward 2 5.0106352940962555 620908
warning 2 5.0106352940962555 620928
warp 2 5.0106352940962555 620948
was 13 3.138833117194664 620968
wasserstein 12 3.2188758248682006 621076
wassertein 2 5.0106352940962555 621176
watch 2 5.0106352940962555 621196
watermarking-which 2 5.0106352940962555 621216
watson 2 5.0106352940962555 621236
waveform 2 5.0106352940962555 621256
waveforms 2 5.0106352940962555 621276
wavegan 2 5.0106352940962555 621296
wavelet 4 4.31748811353631 621316
wavelet-based 2 5.0106352940962555 621352
wavelets 2 5.0106352940962555 621372
wavenet 2 5.0106352940962555 621392
way 24 2.5257286443082556 621412
ways 10 3.4011973816621555 621608
we 292 0.02702867238791942 621692
weak 6 3.912023005428146 624032
weaker 2 5.0106352940962555 624084
weakly 2 5.0106352940962555 624104
weaknesses 2 5.0106352940962555 624124
wealth 4 4.31748811353631 624144
weather 4 4.31748811353631 624180
web-search 2 5.0106352940962555 624216
website 8 3.624340932976365 624236
weed 2 5.0106352940962555 624304
weight 33 2.207274913189721 624324
weight-assignment 2 5.0106352940962555 624592
weight-rewinding 2 5.0106352940962555 624612
weight-space 2 5.0106352940962555 624632
weighted 10 3.4011973816621555 624652
weighting 2 5.0106352940962555 624736
weights 40 2.0149030205422647 624756
weights.in 2 5.0106352940962555 625080
weights.preprint 2 5.0106352940962555 625100
weijie 2 5.0106352940962555 625120
weisfeiler-lehman 2 5.0106352940962555 625140
weisfeiler-leman 2 5.0106352940962555 625160
weizmann 2 5.0106352940962555 625180
well 96 1.1394342831883648 625200
well-calibrated 2 5.0106352940962555 625972
well-designed 6 3.912023005428146 625992
well-developed 2 5.0106352940962555 626044
well-established 6 3.912023005428146 626064
well-known 10 3.4011973816621555 626116
well-separated 4 4.31748811353631 626200
well-spread 2 5.0106352940962555 626236
well-suited 2 5.0106352940962555 626256
well-understood 2 5.0106352940962555 626276
well.categorical 2 5.0106352940962555 626296
well.existing 2 5.0106352940962555 626316
well.in 2 5.0106352940962555 626336
welleck 2 5.0106352940962555 626356
wengong 2 5.0106352940962555 626376
wenzek 2 5.0106352940962555 626396
wer 1 5.703782474656201 626416
were 22 2.6127400212978853 626428
werman 2 5.0106352940962555 626608
what 34 2.17742195004004 626628
when 164 0.6039160468320027 626904
where 114 0.9675840262617057 628220
whereas 16 2.9311937524164198 629136
whereby 2 5.0106352940962555 629268
wherein 8 3.624340932976365 629288
whether 30 2.302585092994046 629356
which 310 -0.03278982282299084 629600
while 160 0.6286086594223741 632084
whilst 2 5.0106352940962555 633368
white-and 2 5.0106352940962555 633388
whitebox 2 5.0106352940962555 633408
whitened 2 5.0106352940962555 633428
whitening 2 5.0106352940962555 633448
who 1 5.703782474656201 633468
whole 10 3.4011973816621555 633480
whom 2 5.0106352940962555 633564
whose 14 3.0647251450409425 633584
why 10 3.4011973816621555 633700
wide 28 2.371577964480997 633784
wide-baseline 2 5.0106352940962555 634012
wide-ranging 2 5.0106352940962555 634032
wide-spread 2 5.0106352940962555 634052
widely 22 2.6127400212978853 634072
wider 6 3.912023005428146 634252
widespread 4 4.31748811353631 634304
width 2 5.0106352940962555 634340
wikiimage 2 5.0106352940962555 634360
wikipedia 2 5.0106352940962555 634380
wikitext-2 2 5.0106352940962555 634400
will 51 1.7719568419318754 634420
win-win 2 5.0106352940962555 634832
win-win.to 2 5.0106352940962555 634852
winci-ai 2 5.0106352940962555 634872
window 6 3.912023005428146 634892
windows 3 4.605170185988092 634944
windows.as 2 5.0106352940962555 634972
windows.this 2 5.0106352940962555 634992
winning 7 3.7578723256008875 635012
wisdom 4 4.31748811353631 635072
wish 2 5.0106352940962555 635108
with 541 -0.5896368041902804 635128
within 58 1.6433394641097818 639460
without 104 1.0593915755148284 639928
witness 2 5.0106352940962555 640764
witnessed 4 4.31748811353631 640784
wkh 1 5.703782474656201 640820
wklv 2 5.0106352940962555 640832
wl 1 5.703782474656201 640852
wmt 4 4.31748811353631 640864
wmt14 2 5.0106352940962555 640900
word 14 3.0647251450409425 640920
word-topic 2 5.0106352940962555 641036
wordnet 2 5.0106352940962555 641056
words 12 3.2188758248682006 641076
work 232 0.25704510298989114 641176
work.published 2 5.0106352940962555 643036
workan 2 5.0106352940962555 643056
workdistributed 2 5.0106352940962555 643076
worked 2 5.0106352940962555 643096
workers 2 5.0106352940962555 643116
workin 2 5.0106352940962555 643136
working 2 5.0106352940962555 643156
workloads 2 5.0106352940962555 643176
works 56 1.6784307839210517 643196
works.preprint 2 5.0106352940962555 643648
world 40 2.0149030205422647 643668
worlds 4 4.31748811353631 643992
worse 10 3.4011973816621555 644028
worst 2 5.0106352940962555 644112
worst-case 10 3.4011973816621555 644132
worth 6 3.912023005428146 644216
worthy 4 4.31748811353631 644268
would 22 2.6127400212978853 644304
wr 1 5.703782474656201 644484
write 4 4.31748811353631 644496
writers 2 5.0106352940962555 644532
writing 6 3.912023005428146 644552
written 4 4.31748811353631 644604
wrong 2 5.0106352940962555 644640
x 6 3.912023005428146 644660
x-ray 2 5.0106352940962555 644712
x. 2 5.0106352940962555 644732
x_data 2 5.0106352940962555 644752
x_data.size 2 5.0106352940962555 644772
xie 5 4.0943445622221 644792
xinyu-andy.github.io 2 5.0106352940962555 644836
xixj 2 5.0106352940962555 644856
xor 3 4.605170185988092 644876
xp3x 2 5.0106352940962555 644904
xs 1 5.703782474656201 644924
xu 3 4.605170185988092 644936
xun 1 5.703782474656201 644964
y 3 4.605170185988092 644976
y. 1 5.703782474656201 645004
y_data 2 5.0106352940962555 645016
yang 4 4.31748811353631 645036
yao 1 5.703782474656201 645072
yazhe 2 5.0106352940962555 645084
ybybzhang 2 5.0106352940962555 645104
years 12 3.2188758248682006 645124
yehudai 2 5.0106352940962555 645224
yes 4 4.31748811353631 645244
yet 27 2.407945608651872 645280
yield 4 4.31748811353631 645500
yielding 8 3.624340932976365 645536
yields 18 2.8134107167600364 645604
yjruan 2 5.0106352940962555 645752
yli939 2 5.0106352940962555 645772
york 2 5.0106352940962555 645792
you 5 4.0943445622221 645812
younes 2 5.0106352940962555 645856
your 4 4.31748811353631 645876
yourefit 2 5.0106352940962555 645912
ys 1 5.703782474656201 645932
ys-zong 2 5.0106352940962555 645944
yu 2 5.0106352940962555 645964
yu.bai 2 5.0106352940962555 645984
yuan 2 5.0106352940962555 646004
z 2 5.0106352940962555 646024
zaslavsky 2 5.0106352940962555 646044
zero 14 3.0647251450409425 646064
zero-cost 2 5.0106352940962555 646180
zero-day 2 5.0106352940962555 646200
zero-shot 6 3.912023005428146 646220
zero-sum 3 4.605170185988092 646272
zerofl 3 4.605170185988092 646300
zhang 4 4.31748811353631 646328
zhao 2 5.0106352940962555 646364
zhaoyanglyu 2 5.0106352940962555 646384
zhou 2 5.0106352940962555 646404
zhu 1 5.703782474656201 646424
zietlow 2 5.0106352940962555 646436
zimmert 2 5.0106352940962555 646456
zine 2 5.0106352940962555 646476
zip 2 5.0106352940962555 646496
zipit 3 4.605170185988092 646516
zipping 2 5.0106352940962555 646544
ziqipang 2 5.0106352940962555 646564
zloo 2 5.0106352940962555 646584
zoesgithub 2 5.0106352940962555 646604
 3 4.605170185988092 646624
 1 5.703782474656201 646652
-greedy 2 5.0106352940962555 646664
 3 4.605170185988092 646684
-exponentiated 2 5.0106352940962555 646712
i 2 5.0106352940962555 646732
 2 5.0106352940962555 646752
 1 5.703782474656201 646772
 1 5.703782474656201 646784
 3 4.605170185988092 646796
 1 5.703782474656201 646824
min 2 5.0106352940962555 646836
 2 5.0106352940962555 646856
 3 4.605170185988092 646876
 1 5.703782474656201 646904
 1 5.703782474656201 646916
 1 5.703782474656201 646928
plog 2 5.0106352940962555 646940
 1 5.703782474656201 646960
 2 5.0106352940962555 646972
-stationary 2 5.0106352940962555 646992
 1 5.703782474656201 647012
